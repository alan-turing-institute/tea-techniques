{
  "tag": {
    "name": "lifecycle-stage/data-collection",
    "slug": "lifecycle-stage-data-collection",
    "count": 6,
    "category": "lifecycle-stage"
  },
  "techniques": [
    {
      "slug": "fairness-gan",
      "name": "Fairness GAN",
      "description": "A data generation technique that employs Generative Adversarial Networks (GANs) to create fair synthetic datasets by learning to generate data representations that preserve utility whilst obfuscating protected attributes. Unlike traditional GANs, Fairness GANs incorporate fairness constraints into the training objective, ensuring that the generated data maintains statistical parity across demographic groups. The technique can be used for data augmentation to balance underrepresented groups or to create privacy-preserving synthetic datasets that remove demographic bias from training data.",
      "assurance_goals": [
        "Fairness",
        "Privacy",
        "Reliability"
      ],
      "tags": [
        "applicable-models/architecture/neural-networks/generative/gan",
        "applicable-models/paradigm/generative",
        "applicable-models/paradigm/unsupervised",
        "applicable-models/requirements/gradient-access",
        "applicable-models/requirements/training-data",
        "applicable-models/requirements/white-box",
        "assurance-goal-category/fairness",
        "assurance-goal-category/fairness/group",
        "assurance-goal-category/privacy",
        "assurance-goal-category/reliability",
        "data-requirements/labelled-data",
        "data-requirements/sensitive-attributes",
        "data-type/any",
        "evidence-type/fairness-metric",
        "evidence-type/quantitative-metric",
        "evidence-type/synthetic-data",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistics",
        "fairness-approach/group",
        "lifecycle-stage/data-collection",
        "lifecycle-stage/data-collection/data-augmentation",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Generating balanced synthetic datasets for medical research by creating additional samples from underrepresented demographic groups, ensuring equal representation across ethnicity and gender whilst maintaining the statistical properties needed for robust model training.",
          "goal": "Fairness"
        },
        {
          "description": "Creating privacy-preserving synthetic datasets for financial services that remove demographic identifiers whilst preserving the underlying patterns needed for credit risk assessment, allowing secure data sharing between institutions without exposing sensitive customer information.",
          "goal": "Privacy"
        },
        {
          "description": "Augmenting recruitment datasets by generating synthetic candidate profiles that balance gender and ethnicity representation, ensuring reliable model performance across all demographic groups when real-world data exhibits significant imbalances.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "GAN training is notoriously difficult to stabilise, with potential for mode collapse or failure to converge, especially when additional fairness constraints are imposed."
        },
        {
          "description": "Ensuring fairness in generated data may come at the cost of data utility, potentially reducing the quality or realism of synthetic samples."
        },
        {
          "description": "Requires large datasets to train both generator and discriminator networks effectively, limiting applicability in data-scarce domains."
        },
        {
          "description": "Evaluation complexity is high, as it requires assessing both the quality of generated data and the preservation of fairness properties across demographic groups."
        },
        {
          "description": "May inadvertently introduce new biases if the fairness constraints are not properly specified or if the training data itself contains subtle biases."
        }
      ],
      "resources": [
        {
          "title": "Fairness GAN",
          "url": "http://arxiv.org/pdf/1805.09910v1",
          "source_type": "technical_paper",
          "authors": [
            "Prasanna Sattigeri",
            "Samuel C. Hoffman",
            "Vijil Chenthamarakshan",
            "Kush R. Varshney"
          ],
          "publication_date": "2018-05-24"
        },
        {
          "title": "Fair GANs through model rebalancing for extremely imbalanced class distributions",
          "url": "http://arxiv.org/pdf/2308.08638v2",
          "source_type": "technical_paper",
          "authors": [
            "Anubhav Jain",
            "Nasir Memon",
            "Julian Togelius"
          ],
          "publication_date": "2023-08-16"
        },
        {
          "title": "Inclusive GAN: Improving Data and Minority Coverage in Generative Models",
          "url": "http://arxiv.org/abs/2004.03355",
          "source_type": "technical_paper",
          "authors": [
            "Ning Yu",
            "Ke Li",
            "Peng Zhou",
            "Jitendra Malik",
            "Larry Davis",
            "Mario Fritz"
          ],
          "publication_date": "2020-04-07"
        }
      ],
      "complexity_rating": 5,
      "computational_cost_rating": 5,
      "related_techniques": [
        "bayesian-fairness-regularization",
        "sensitivity-analysis-for-fairness",
        "attribute-removal-fairness-through-unawareness",
        "fair-adversarial-networks"
      ]
    },
    {
      "slug": "relabelling",
      "name": "Relabelling",
      "description": "A preprocessing fairness technique that modifies class labels in training data to achieve equal positive outcome rates across protected groups. Also known as 'data massaging', this method identifies instances that contribute to discriminatory patterns and flips their labels (from positive to negative or vice versa) to balance the proportion of positive outcomes between demographic groups. The technique aims to remove historical bias from training data whilst preserving the overall class distribution, enabling standard classifiers to learn from discrimination-free datasets.",
      "assurance_goals": [
        "Fairness",
        "Transparency",
        "Reliability"
      ],
      "tags": [
        "applicable-models/architecture/model-agnostic",
        "applicable-models/paradigm/supervised",
        "applicable-models/requirements/black-box",
        "applicable-models/requirements/training-data",
        "assurance-goal-category/fairness",
        "assurance-goal-category/fairness/group",
        "assurance-goal-category/reliability",
        "assurance-goal-category/transparency",
        "data-requirements/access-to-training-data",
        "data-requirements/labelled-data",
        "data-requirements/sensitive-attributes",
        "data-type/any",
        "evidence-type/dataset-analysis",
        "evidence-type/quantitative-metric",
        "expertise-needed/domain-knowledge",
        "expertise-needed/statistics",
        "fairness-approach/group",
        "lifecycle-stage/data-collection",
        "lifecycle-stage/data-collection/data-preprocessing",
        "lifecycle-stage/model-development",
        "technique-type/procedural"
      ],
      "example_use_cases": [
        {
          "description": "Preprocessing historical hiring datasets by relabelling borderline cases to ensure equal hiring rates across gender and ethnicity groups, correcting for past discriminatory practices whilst maintaining overall qualification standards for fair recruitment model training.",
          "goal": "Fairness"
        },
        {
          "description": "Creating transparent credit scoring datasets by documenting which loan applications had labels modified to address historical lending discrimination, providing clear audit trails showing how training data bias was systematically corrected before model development.",
          "goal": "Transparency"
        },
        {
          "description": "Improving reliability of medical diagnosis training data by relabelling cases where demographic bias may have influenced historical diagnoses, ensuring models learn from corrected labels that reflect true medical conditions rather than biased historical treatment patterns.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Altering training labels risks introducing new biases or artificial patterns that may not reflect genuine relationships in the data."
        },
        {
          "description": "Deciding which instances to relabel requires careful selection criteria and domain expertise to avoid inappropriate label changes."
        },
        {
          "description": "May reduce prediction accuracy if too many labels are changed, particularly when the modifications conflict with genuine patterns in the data."
        },
        {
          "description": "Requires access to ground truth or expert knowledge to determine whether original labels reflect genuine outcomes or discriminatory bias."
        },
        {
          "description": "Effectiveness depends on accurate identification of discriminatory instances, which can be challenging when bias patterns are subtle or complex."
        }
      ],
      "resources": [
        {
          "title": "Data preprocessing techniques for classification without discrimination",
          "url": "https://link.springer.com/article/10.1007/s10115-011-0463-8",
          "source_type": "technical_paper",
          "authors": [
            "Faisal Kamiran",
            "Toon Calders"
          ],
          "publication_date": "2012-06-01"
        },
        {
          "title": "Classifying without discriminating",
          "url": "https://www.researchgate.net/publication/224440330_Classifying_without_discriminating",
          "source_type": "technical_paper",
          "authors": [
            "Toon Calders",
            "Sicco Verwer"
          ],
          "publication_date": "2010-02-01"
        },
        {
          "title": "Data Pre-Processing for Discrimination Prevention",
          "url": "https://krvarshney.github.io/pubs/CalmonWVRV_jstsp2018.pdf",
          "source_type": "technical_paper",
          "authors": [
            "Flavio Calmon",
            "Dennis Wei",
            "Bhanukiran Vinzamuri",
            "Karthikeyan Natesan Ramamurthy",
            "Kush R. Varshney"
          ],
          "publication_date": "2018-01-01"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "related_techniques": [
        "reweighing",
        "disparate-impact-remover",
        "preferential-sampling",
        "multi-accuracy-boosting"
      ]
    },
    {
      "slug": "preferential-sampling",
      "name": "Preferential Sampling",
      "description": "A preprocessing fairness technique developed by Kamiran and Calders that addresses dataset imbalances by re-sampling training data with preference for underrepresented groups to achieve discrimination-free classification. This method modifies the training distribution by prioritising borderline objects (instances near decision boundaries) from underrepresented groups for duplication whilst potentially removing instances from overrepresented groups. Unlike relabelling approaches, preferential sampling maintains original class labels whilst creating a more balanced dataset that prevents models from learning biased patterns due to skewed group representation.",
      "assurance_goals": [
        "Fairness",
        "Reliability",
        "Transparency"
      ],
      "tags": [
        "applicable-models/architecture/model-agnostic",
        "applicable-models/paradigm/supervised",
        "applicable-models/requirements/black-box",
        "applicable-models/requirements/training-data",
        "assurance-goal-category/fairness",
        "assurance-goal-category/fairness/group",
        "assurance-goal-category/reliability",
        "assurance-goal-category/transparency",
        "data-requirements/labelled-data",
        "data-requirements/sensitive-attributes",
        "data-type/any",
        "evidence-type/dataset-analysis",
        "evidence-type/quantitative-metric",
        "expertise-needed/low",
        "fairness-approach/group",
        "lifecycle-stage/data-collection",
        "lifecycle-stage/data-collection/data-preprocessing",
        "lifecycle-stage/model-development",
        "technique-type/procedural"
      ],
      "example_use_cases": [
        {
          "description": "Preprocessing hiring datasets by preferentially sampling candidates from underrepresented gender and ethnic groups, particularly focusing on borderline cases near decision boundaries, to ensure fair representation whilst maintaining original qualifications and labels for unbiased recruitment model training.",
          "goal": "Fairness"
        },
        {
          "description": "Balancing medical training datasets by oversampling patients from underrepresented demographic groups to ensure reliable diagnostic performance across all populations, preventing models from exhibiting reduced accuracy for minority patient groups due to insufficient training examples.",
          "goal": "Reliability"
        },
        {
          "description": "Creating transparent credit scoring datasets by documenting and adjusting the sampling process to ensure equal representation across demographic groups, providing clear evidence to regulators that training data imbalances have been addressed without altering original creditworthiness labels.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Oversampling minority groups can cause overfitting to duplicated examples, particularly when borderline instances are repeatedly sampled, potentially reducing model generalisation."
        },
        {
          "description": "Undersampling majority groups may remove important examples that contain valuable information, potentially degrading overall model performance."
        },
        {
          "description": "Does not address inherent algorithmic bias in the learning process itself, only correcting for representation imbalances in the training data."
        },
        {
          "description": "Selection of borderline objects requires careful threshold tuning and may be sensitive to the choice of distance metrics or similarity measures used."
        },
        {
          "description": "May not address intersectional fairness issues when multiple protected attributes create complex group combinations that require nuanced sampling strategies."
        }
      ],
      "resources": [
        {
          "title": "Data preprocessing techniques for classification without discrimination",
          "url": "https://link.springer.com/article/10.1007/s10115-011-0463-8",
          "source_type": "technical_paper",
          "authors": [
            "Faisal Kamiran",
            "Toon Calders"
          ],
          "publication_date": "2012-06-01"
        },
        {
          "title": "Classification with no discrimination by preferential sampling",
          "url": "https://research.tue.nl/en/publications/classification-with-no-discrimination-by-preferential-sampling",
          "source_type": "technical_paper",
          "authors": [
            "Faisal Kamiran",
            "Toon Calders"
          ],
          "publication_date": "2010-05-27"
        },
        {
          "title": "A Survey on Bias and Fairness in Machine Learning",
          "url": "https://arxiv.org/abs/1908.09635",
          "source_type": "documentation",
          "authors": [
            "Ninareh Mehrabi",
            "Fred Morstatter",
            "Nripsuta Saxena",
            "Kristina Lerman",
            "Aram Galstyan"
          ],
          "publication_date": "2019-08-25"
        }
      ],
      "complexity_rating": 2,
      "computational_cost_rating": 2,
      "related_techniques": [
        "disparate-impact-remover",
        "reweighing",
        "relabelling",
        "out-of-domain-detection"
      ]
    },
    {
      "slug": "model-development-audit-trails",
      "name": "Model Development Audit Trails",
      "description": "Model development audit trails create comprehensive, immutable records of all decisions, experiments, and changes throughout the ML lifecycle. This technique captures dataset versions, training configurations, hyperparameter searches, model architecture changes, evaluation results, and deployment decisions in tamper-evident logs. Audit trails enable reproducibility, accountability, and regulatory compliance by documenting who made what changes when and why, supporting post-hoc investigation of model failures or unexpected behaviours.",
      "assurance_goals": [
        "Transparency",
        "Reliability",
        "Safety"
      ],
      "tags": [
        "applicable-models/architecture/model-agnostic",
        "applicable-models/requirements/black-box",
        "assurance-goal-category/transparency",
        "assurance-goal-category/reliability",
        "assurance-goal-category/safety",
        "data-type/any",
        "lifecycle-stage/project-planning",
        "lifecycle-stage/data-collection",
        "lifecycle-stage/model-development",
        "lifecycle-stage/model-evaluation",
        "lifecycle-stage/deployment",
        "lifecycle-stage/post-deployment",
        "technique-type/procedural",
        "technique-type/governance-framework",
        "evidence-type/documentation",
        "expertise-needed/software-engineering",
        "expertise-needed/ml-engineering",
        "data-requirements/no-special-requirements"
      ],
      "example_use_cases": [
        {
          "description": "Maintaining detailed audit trails for medical AI development enabling investigators to trace how training data, model architecture, and evaluation decisions led to specific diagnostic behaviors during regulatory review.",
          "goal": "Transparency"
        },
        {
          "description": "Recording all model updates and performance changes over time to support root cause analysis when deployed systems exhibit unexpected behavior or reliability degradation.",
          "goal": "Reliability"
        },
        {
          "description": "Documenting safety-critical decisions like dataset filtering, bias testing, and red teaming results to demonstrate due diligence in preventing harmful deployments.",
          "goal": "Safety"
        },
        {
          "description": "Documenting credit scoring model development for regulatory compliance, maintaining detailed records of data sources, feature engineering decisions, fairness testing, and validation results to demonstrate adherence to fair lending requirements during audits.",
          "goal": "Transparency"
        },
        {
          "description": "Creating comprehensive audit trails for criminal justice risk assessment tools to enable external review of training data selection, bias mitigation techniques, and validation methodologies when legal challenges question algorithmic fairness.",
          "goal": "Fairness"
        },
        {
          "description": "Maintaining development logs for autonomous vehicle perception systems to support accident investigations, enabling forensic analysis of which model version was deployed, what training data informed its behavior, and what testing validated its safety.",
          "goal": "Safety"
        }
      ],
      "limitations": [
        {
          "description": "Comprehensive logging generates large volumes of data requiring significant storage infrastructure and data management."
        },
        {
          "description": "Audit trails may contain sensitive information about proprietary techniques, requiring careful access control and redaction procedures."
        },
        {
          "description": "Creating meaningful audit trails requires discipline and tooling integration that may slow development velocity."
        },
        {
          "description": "Retrospective analysis of audit trails can be time-consuming and requires expertise to extract actionable insights from complex logs."
        },
        {
          "description": "Implementing comprehensive audit trail systems requires integrating with diverse development tools (version control, experiment tracking, data pipelines), which can be complex and may require custom development for organisation-specific workflows."
        },
        {
          "description": "Storage costs can be substantial, with comprehensive model development projects generating terabytes of logs, experimental artifacts, and dataset versions requiring long-term retention for compliance purposes."
        }
      ],
      "resources": [
        {
          "title": "mlflow/mlflow",
          "url": "https://github.com/mlflow/mlflow",
          "source_type": "software_package"
        },
        {
          "title": "iterative/dvc",
          "url": "https://github.com/iterative/dvc",
          "source_type": "software_package"
        },
        {
          "title": "Advances in Data Lineage, Auditing, and Governance in Distributed Cloud Data Ecosystems",
          "url": "https://www.researchgate.net/publication/392917516_Advances_in_Data_Lineage_Auditing_and_Governance_in_Distributed_Cloud_Data_Ecosystems",
          "source_type": "technical_paper"
        },
        {
          "title": "Logging requirement for continuous auditing of responsible machine learning-based applications",
          "url": "https://link.springer.com/article/10.1007/s10664-025-10656-8",
          "source_type": "technical_paper"
        }
      ],
      "related_techniques": [
        "api-usage-pattern-monitoring",
        "out-of-domain-detection",
        "preferential-sampling",
        "relabelling"
      ]
    },
    {
      "slug": "data-poisoning-detection",
      "name": "Data Poisoning Detection",
      "description": "Data poisoning detection identifies malicious training data designed to compromise model behaviour. This technique detects various poisoning attacks including backdoor injection (triggers causing specific behaviours), availability attacks (degrading overall performance), and targeted attacks (causing errors on specific inputs). Detection methods include statistical outlier analysis, influence function analysis to identify high-impact training points, and validation-based approaches that test for suspicious model behaviours indicative of poisoning.",
      "assurance_goals": [
        "Security",
        "Reliability",
        "Safety"
      ],
      "tags": [
        "applicable-models/architecture/model-agnostic",
        "applicable-models/requirements/white-box",
        "applicable-models/requirements/gradient-access",
        "assurance-goal-category/security",
        "assurance-goal-category/reliability",
        "assurance-goal-category/safety",
        "data-requirements/training-data-required",
        "data-type/any",
        "lifecycle-stage/data-collection",
        "lifecycle-stage/model-development",
        "lifecycle-stage/model-evaluation",
        "technique-type/analytical",
        "technique-type/testing",
        "evidence-type/quantitative-metric",
        "evidence-type/test-results",
        "expertise-needed/ml-engineering",
        "expertise-needed/domain-expertise"
      ],
      "example_use_cases": [
        {
          "description": "Scanning training data for an autonomous driving system to detect images that might contain backdoor triggers designed to cause unsafe behavior when specific objects appear in the environment.",
          "goal": "Safety"
        },
        {
          "description": "Protecting a federated learning system for hospital patient diagnosis models from malicious participants who might inject poisoned medical records designed to create backdoors that misclassify specific patient profiles or degrade overall diagnostic reliability.",
          "goal": "Security"
        },
        {
          "description": "Ensuring a content moderation model hasn't been poisoned with data designed to make it fail on specific types of harmful content, maintaining reliable safety filtering.",
          "goal": "Reliability"
        },
        {
          "description": "Detecting poisoned training data in recidivism prediction models used for sentencing recommendations, where malicious actors might inject manipulated records to systematically bias predictions for specific demographic groups.",
          "goal": "Safety"
        },
        {
          "description": "Scanning training data for algorithmic trading models to identify poisoned market data designed to create exploitable patterns, ensuring reliable trading decisions and preventing market manipulation vulnerabilities.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Sophisticated poisoning attacks can be designed to evade statistical detection by mimicking benign data distributions closely."
        },
        {
          "description": "High false positive rates may lead to incorrectly filtering legitimate but unusual training examples, reducing training data quality and diversity."
        },
        {
          "description": "Computationally expensive to apply influence function analysis or deep inspection to very large training datasets, potentially requiring hours to days for thorough analysis of datasets with millions of samples, particularly when using gradient-based detection methods."
        },
        {
          "description": "Difficult to detect poisoning in scenarios where attackers have knowledge of detection methods and can adapt attacks accordingly."
        },
        {
          "description": "Establishing ground truth for what constitutes 'poisoned' versus legitimate but unusual data is challenging, particularly when dealing with naturally occurring outliers or edge cases in the data distribution."
        }
      ],
      "resources": [
        {
          "title": "art.defences.detector.poison — Adversarial Robustness Toolbox ...",
          "url": "https://adversarial-robustness-toolbox.readthedocs.io/en/latest/modules/defences/detector_poisoning.html",
          "source_type": "documentation"
        },
        {
          "title": "Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT Systems using Large Language Models",
          "url": "http://arxiv.org/pdf/2511.02894v1",
          "source_type": "technical_paper",
          "authors": [
            "W. K. M Mithsara",
            "Ning Yang",
            "Ahmed Imteaj",
            "Hussein Zangoti",
            "Abdur R. Shahid"
          ],
          "publication_date": "2025-11-04"
        },
        {
          "title": "SAFELOC: Overcoming Data Poisoning Attacks in Heterogeneous Federated Machine Learning for Indoor Localization",
          "url": "http://arxiv.org/pdf/2411.09055v1",
          "source_type": "technical_paper",
          "authors": [
            "Akhil Singampalli",
            "Danish Gufran",
            "Sudeep Pasricha"
          ],
          "publication_date": "2024-11-13"
        },
        {
          "title": "Introduction — trojai 0.2.22 documentation",
          "url": "https://trojai.readthedocs.io/en/latest/intro.html",
          "source_type": "documentation"
        },
        {
          "title": "Understanding Influence Functions and Datamodels via Harmonic Analysis",
          "url": "http://arxiv.org/pdf/2210.01072v1",
          "source_type": "technical_paper",
          "authors": [
            "Nikunj Saunshi",
            "Arushi Gupta",
            "Mark Braverman",
            "Sanjeev Arora"
          ],
          "publication_date": "2022-10-03"
        }
      ],
      "related_techniques": [
        "anomaly-detection",
        "adversarial-robustness-testing",
        "influence-functions",
        "cross-validation"
      ]
    },
    {
      "slug": "synthetic-data-evaluation",
      "name": "Synthetic Data Evaluation",
      "description": "Synthetic data evaluation assesses whether synthetic datasets protect individual privacy while maintaining statistical utility and fidelity to real data. This technique evaluates three key dimensions: privacy (through disclosure risk metrics and re-identification attack success rates), utility (by comparing statistical properties and model performance), and fidelity (measuring distributional similarity to real data). It produces evaluation reports quantifying the privacy-utility-fidelity trade-offs.",
      "assurance_goals": [
        "Privacy",
        "Transparency",
        "Reliability"
      ],
      "tags": [
        "applicable-models/architecture/model-agnostic",
        "applicable-models/requirements/black-box",
        "assurance-goal-category/privacy",
        "assurance-goal-category/transparency",
        "assurance-goal-category/reliability",
        "data-requirements/training-data-required",
        "data-type/tabular",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/documentation",
        "evidence-type/test-results",
        "expertise-needed/statistical-knowledge",
        "expertise-needed/domain-expertise",
        "expertise-needed/ml-engineering",
        "lifecycle-stage/data-collection",
        "lifecycle-stage/model-evaluation",
        "technique-type/analytical",
        "technique-type/testing"
      ],
      "example_use_cases": [
        {
          "description": "Validating synthetic patient data generated for medical research to ensure individual patients cannot be re-identified while maintaining statistical relationships needed for valid clinical studies.",
          "goal": "Privacy"
        },
        {
          "description": "Validating that machine learning models for predicting student outcomes trained on synthetic educational data maintain reliable performance comparable to models trained on real student records, while enabling researchers to share datasets without FERPA violations.",
          "goal": "Reliability"
        },
        {
          "description": "Ensuring fraud detection models trained on synthetic credit card transactions maintain reliable performance comparable to models trained on sensitive real transaction data.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Trade-off between privacy and utility means strong privacy guarantees often significantly degrade data quality and analytical value."
        },
        {
          "description": "Difficult to validate that synthetic data protects against all possible privacy attacks, especially sophisticated adversaries with auxiliary information."
        },
        {
          "description": "Utility metrics may not capture subtle distributional differences that matter for specific downstream tasks or edge case analyses."
        },
        {
          "description": "Synthetic data may introduce artificial patterns or miss rare but important real-world phenomena, limiting use for certain applications."
        },
        {
          "description": "Requires significant domain expertise to properly validate fidelity and utility for specific use cases, as generic statistical metrics may not capture domain-specific requirements or failure modes."
        },
        {
          "description": "Synthetic data may not preserve fairness properties or bias patterns from original data in predictable ways, requiring careful fairness testing when synthetic data is used to train decision-making models."
        }
      ],
      "resources": [
        {
          "title": "SCU-TrustworthyAI/SynEval",
          "url": "https://github.com/SCU-TrustworthyAI/SynEval",
          "source_type": "software_package"
        },
        {
          "title": "Evaluating synthetic data | Towards Data Science",
          "url": "https://towardsdatascience.com/evaluating-synthetic-data-c5833f6b2f15/",
          "source_type": "tutorial"
        },
        {
          "title": "Privacy Mechanisms and Evaluation Metrics for Synthetic Data Generation: A Systematic Review",
          "url": "https://www.semanticscholar.org/paper/e76d1dde9340ed1bfef28808297df51791ce4506",
          "source_type": "review_paper",
          "authors": [
            "Pablo A. Osorio-Marulanda",
            "Gorka Epelde",
            "Mikel Hernandez",
            "Imanol Isasa",
            "Nicolas Moreno Reyes",
            "A. B. Iraola"
          ]
        },
        {
          "title": "Can We Trust Synthetic Data in Medicine? A Scoping Review of Privacy and Utility Metrics",
          "url": "https://www.semanticscholar.org/paper/5e1eb0df4db1316cff416aee9e7676517778a780",
          "source_type": "review_paper",
          "authors": [
            "B. Kaabachi",
            "J. Despraz",
            "T. Meurers",
            "K. Otte",
            "M. Halilovic",
            "F. Prasser",
            "J. Raisaro"
          ]
        },
        {
          "title": "Welcome to TAPAS's documentation! — tapas 0.1 documentation",
          "url": "https://tapas-privacy.readthedocs.io/",
          "source_type": "documentation"
        }
      ],
      "related_techniques": [
        "membership-inference-attack-testing",
        "synthetic-data-generation",
        "differential-privacy",
        "machine-unlearning"
      ]
    }
  ],
  "count": 6
}