{
  "tag": {
    "name": "data-type/image",
    "slug": "data-type-image",
    "count": 7,
    "category": "data-type"
  },
  "techniques": [
    {
      "slug": "saliency-maps",
      "name": "Saliency Maps",
      "description": "Saliency maps are visual explanations for image classification models that highlight which pixels in an image most strongly influence the model's prediction. Computed by calculating gradients of the model's output with respect to input pixels, saliency maps produce heatmaps where brighter regions indicate pixels that, when changed, would most significantly affect the prediction. This technique helps users understand which parts of an image the model is 'looking at' when making decisions.",
      "assurance_goals": [
        "Explainability",
        "Fairness"
      ],
      "tags": [
        "applicable-models/architecture/neural-networks",
        "applicable-models/requirements/differentiable",
        "applicable-models/requirements/gradient-access",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/attribution-methods/gradient-based",
        "assurance-goal-category/explainability/explains/feature-importance",
        "assurance-goal-category/explainability/property/comprehensibility",
        "assurance-goal-category/explainability/property/efficiency",
        "assurance-goal-category/explainability/visualization-methods/activation-maps",
        "assurance-goal-category/fairness",
        "data-requirements/access-to-model-internals",
        "data-type/image",
        "evidence-type/visualization",
        "expertise-needed/ml-engineering",
        "explanatory-scope/local",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Analysing X-ray images in a pneumonia detection model to verify that the algorithm focuses on lung regions showing inflammatory patterns rather than irrelevant areas like medical equipment or patient positioning markers.",
          "goal": "Explainability"
        },
        {
          "description": "Examining skin lesion classification models to ensure the algorithm identifies diagnostic features (irregular borders, colour variation) rather than artifacts like rulers, hair, or skin markings that shouldn't influence medical decisions.",
          "goal": "Explainability"
        },
        {
          "description": "Auditing a dermatology AI system to verify it focuses on medical symptoms rather than skin colour when diagnosing conditions, ensuring equitable treatment across racial groups by revealing inappropriate attention to demographic features.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Saliency maps are often noisy and can change dramatically with small input perturbations, making them unstable."
        },
        {
          "description": "Highlighted regions may not correspond to semantically meaningful or human-understandable features."
        },
        {
          "description": "Only indicates local gradient information, not causal importance or actual decision-making logic."
        },
        {
          "description": "May highlight irrelevant pixels that happen to have high gradients due to model artifacts rather than meaningful patterns."
        }
      ],
      "resources": [
        {
          "title": "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps",
          "url": "https://arxiv.org/abs/1312.6034",
          "source_type": "technical_paper",
          "authors": [
            "Karen Simonyan",
            "Andrea Vedaldi",
            "Andrew Zisserman"
          ],
          "publication_date": "2013-12-20"
        },
        {
          "title": "utkuozbulak/pytorch-cnn-visualizations",
          "url": "https://github.com/utkuozbulak/pytorch-cnn-visualizations",
          "source_type": "software_package"
        },
        {
          "title": "Concepts of Saliency and Explainability in AI",
          "url": "https://xaitk-saliency.readthedocs.io/en/latest/xaitk_explanation.html",
          "source_type": "documentation"
        },
        {
          "title": "Occlusion Saliency Example",
          "url": "https://xaitk-saliency.readthedocs.io/en/latest/examples/OcclusionSaliency.html",
          "source_type": "tutorial"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "related_techniques": [
        "gradient-weighted-class-activation-mapping",
        "integrated-gradients",
        "occlusion-sensitivity",
        "layer-wise-relevance-propagation"
      ]
    },
    {
      "slug": "gradient-weighted-class-activation-mapping",
      "name": "Gradient-weighted Class Activation Mapping",
      "description": "Grad-CAM creates visual heatmaps showing which regions of an image a convolutional neural network focuses on when making a specific classification. Unlike pixel-level techniques, Grad-CAM produces coarser region-based explanations by using gradients from the predicted class to weight the CNN's final feature maps, then projecting these weighted activations back to create an overlay on the original image. This provides intuitive visual explanations of where the model is 'looking' for evidence of different classes.",
      "assurance_goals": [
        "Explainability",
        "Fairness"
      ],
      "tags": [
        "applicable-models/architecture/neural-networks/convolutional",
        "applicable-models/requirements/architecture-specific",
        "applicable-models/requirements/gradient-access",
        "applicable-models/requirements/white-box",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/attribution-methods/gradient-based",
        "assurance-goal-category/explainability/explains/feature-importance",
        "assurance-goal-category/explainability/explains/internal-mechanisms",
        "assurance-goal-category/explainability/property/comprehensibility",
        "assurance-goal-category/explainability/property/efficiency",
        "assurance-goal-category/explainability/property/sparsity",
        "assurance-goal-category/explainability/visualization-methods/activation-maps",
        "assurance-goal-category/fairness",
        "data-requirements/access-to-model-internals",
        "data-type/image",
        "evidence-type/visualization",
        "expertise-needed/ml-engineering",
        "explanatory-scope/local",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Validating that a melanoma detection model focuses on the actual skin lesion rather than surrounding healthy skin, medical equipment, or artifacts when making cancer/benign classifications.",
          "goal": "Explainability"
        },
        {
          "description": "Debugging an autonomous vehicle's traffic sign recognition system by visualising whether the model correctly focuses on the sign itself rather than background objects, shadows, or irrelevant visual elements.",
          "goal": "Explainability"
        },
        {
          "description": "Auditing a medical imaging system for racial bias by examining whether diagnostic predictions inappropriately focus on skin tone regions rather than actual pathological features, ensuring equitable healthcare AI deployment.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Requires access to the CNN's internal feature maps and gradients, limiting use to white-box scenarios."
        },
        {
          "description": "Resolution is constrained by the final convolutional layer's feature map size, producing coarser localisation than pixel-level methods."
        },
        {
          "description": "Only applicable to CNN architectures with clearly defined convolutional layers, not suitable for other neural network types."
        },
        {
          "description": "May highlight regions that correlate with the class but aren't causally important for the model's decision-making process."
        }
      ],
      "resources": [
        {
          "title": "Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization",
          "url": "https://arxiv.org/abs/1610.02391",
          "source_type": "technical_paper",
          "authors": [
            "Ramprasaath R. Selvaraju",
            "Michael Cogswell",
            "Abhishek Das",
            "Ramakrishna Vedantam",
            "Devi Parikh",
            "Dhruv Batra"
          ],
          "publication_date": "2016-10-07"
        },
        {
          "title": "jacobgil/pytorch-grad-cam",
          "url": "https://github.com/jacobgil/pytorch-grad-cam",
          "source_type": "software_package"
        },
        {
          "title": "Grad-CAM: Visualize class activation maps with Keras, TensorFlow ...",
          "url": "https://pyimagesearch.com/2020/03/09/grad-cam-visualize-class-activation-maps-with-keras-tensorflow-and-deep-learning/",
          "source_type": "tutorial"
        },
        {
          "title": "kazuto1011/grad-cam-pytorch",
          "url": "https://github.com/kazuto1011/grad-cam-pytorch",
          "source_type": "software_package"
        },
        {
          "title": "A Tutorial on Explainable Image Classification for Dementia Stages Using Convolutional Neural Network and Gradient-weighted Class Activation Mapping",
          "url": "https://www.semanticscholar.org/paper/8b1139cb06cfe5ba69e2fd05e1450b43df031a02",
          "source_type": "technical_paper",
          "authors": [
            "Kevin Kam Fung Yuen"
          ],
          "publication_date": "2024-08-20"
        },
        {
          "title": "A Guide to Grad-CAM in Deep Learning - Analytics Vidhya",
          "url": "https://www.analyticsvidhya.com/blog/2023/12/grad-cam-in-deep-learning/",
          "source_type": "tutorial"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "acronym": "Grad-CAM",
      "related_techniques": [
        "integrated-gradients",
        "saliency-maps",
        "occlusion-sensitivity",
        "local-interpretable-model-agnostic-explanations"
      ]
    },
    {
      "slug": "occlusion-sensitivity",
      "name": "Occlusion Sensitivity",
      "description": "Occlusion sensitivity tests which parts of the input are important by occluding (masking or removing) them and seeing how the model's prediction changes. For example, portions of an image can be covered up in a sliding window fashion; if the model's confidence drops significantly when a certain region is occluded, that region was important for the prediction. This technique is model-agnostic and provides highly interpretable results by directly showing which spatial regions drive model decisions.",
      "assurance_goals": [
        "Explainability"
      ],
      "tags": [
        "applicable-models/architecture/model-agnostic",
        "applicable-models/requirements/black-box",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/attribution-methods/perturbation-based",
        "assurance-goal-category/explainability/explains/feature-importance",
        "assurance-goal-category/explainability/property/sparsity",
        "data-requirements/no-special-requirements",
        "data-type/image",
        "evidence-type/quantitative-metric",
        "expertise-needed/ml-engineering",
        "explanatory-scope/local",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Testing which regions of a chest X-ray are critical for pneumonia detection by systematically covering different areas with grey patches and measuring how much the model's confidence drops for each occluded region.",
          "goal": "Explainability"
        },
        {
          "description": "Evaluating whether a facial recognition system relies on specific facial features by masking eyes, nose, mouth, or other regions to identify which areas cause the biggest drop in recognition accuracy.",
          "goal": "Explainability"
        },
        {
          "description": "Analysing agricultural crop disease classification models to verify that predictions focus on visible disease symptoms (discolouration, lesions, wilting) rather than background elements like soil or irrigation equipment, ensuring reliable deployment across different farm environments.",
          "goal": "Explainability"
        }
      ],
      "limitations": [
        {
          "description": "Computationally expensive as it requires running inference multiple times for each region tested, scaling poorly with input size."
        },
        {
          "description": "Choice of occlusion size and shape can significantly bias results - too small may miss important features, too large may occlude multiple relevant regions simultaneously."
        },
        {
          "description": "Cannot capture interactions between multiple regions that jointly contribute to the prediction but are individually less important."
        },
        {
          "description": "Results may be misleading if the model adapts to occlusion patterns or if occluded regions are filled with unrealistic pixel values."
        }
      ],
      "resources": [
        {
          "title": "kazuto1011/grad-cam-pytorch",
          "url": "https://github.com/kazuto1011/grad-cam-pytorch",
          "source_type": "software_package"
        },
        {
          "title": "Occlusion Sensitivity Analysis with Augmentation Subspace Perturbation in Deep Feature Space",
          "url": "http://arxiv.org/pdf/2311.15022v1",
          "source_type": "technical_paper",
          "authors": [
            "Pedro Valois",
            "Koichiro Niinuma",
            "Kazuhiro Fukui"
          ],
          "publication_date": "2023-11-25"
        },
        {
          "title": "Occlusion Sensitivity — tf-explain documentation",
          "url": "https://tf-explain.readthedocs.io/en/latest/methods.html#occlusion-sensitivity",
          "source_type": "documentation"
        },
        {
          "title": "Adaptive occlusion sensitivity analysis for visually explaining video recognition networks",
          "url": "http://arxiv.org/pdf/2207.12859v2",
          "source_type": "technical_paper",
          "authors": [
            "Tomoki Uchiyama",
            "Naoya Sogi",
            "Satoshi Iizuka",
            "Koichiro Niinuma",
            "Kazuhiro Fukui"
          ],
          "publication_date": "2022-07-26"
        },
        {
          "title": "sicara/tf-explain",
          "url": "https://github.com/sicara/tf-explain",
          "source_type": "software_package"
        }
      ],
      "complexity_rating": 2,
      "computational_cost_rating": 4,
      "related_techniques": [
        "saliency-maps",
        "gradient-weighted-class-activation-mapping",
        "integrated-gradients",
        "local-interpretable-model-agnostic-explanations"
      ]
    },
    {
      "slug": "attention-visualisation-in-transformers",
      "name": "Attention Visualisation in Transformers",
      "description": "Attention Visualisation in Transformers analyses the multi-head self-attention mechanisms that enable transformers to process sequences by attending to different positions simultaneously. The technique visualises attention weights as heatmaps showing how strongly each token attends to every other token across different heads and layers. By examining these attention patterns, practitioners can understand how models like BERT, GPT, and T5 build contextual representations, identify which tokens influence predictions most strongly, and detect potential biases in how the model processes different types of input. This provides insights into positional encoding effects, head specialisation patterns, and the evolution of attention from local to global context across layers.",
      "assurance_goals": [
        "Explainability",
        "Fairness",
        "Transparency"
      ],
      "tags": [
        "applicable-models/architecture/neural-networks/transformer",
        "applicable-models/requirements/architecture-specific",
        "applicable-models/requirements/model-internals",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/explains/data-patterns",
        "assurance-goal-category/explainability/explains/internal-mechanisms",
        "assurance-goal-category/explainability/feature-analysis",
        "assurance-goal-category/explainability/feature-analysis/importance-and-attribution",
        "assurance-goal-category/explainability/property/comprehensibility",
        "assurance-goal-category/explainability/property/efficiency",
        "assurance-goal-category/explainability/visualization-methods/attention-patterns",
        "assurance-goal-category/fairness",
        "assurance-goal-category/transparency",
        "data-requirements/access-to-model-internals",
        "data-type/image",
        "data-type/text",
        "evidence-type/visualization",
        "expertise-needed/ml-engineering",
        "explanatory-scope/global",
        "explanatory-scope/local",
        "lifecycle-stage/model-development",
        "lifecycle-stage/testing",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Examining attention patterns in a medical language model processing clinical notes to verify it focuses on relevant symptoms and conditions rather than irrelevant demographic identifiers, revealing that certain attention heads specialise in medical terminology whilst others track syntactic relationships between diagnoses and treatments.",
          "goal": "Explainability"
        },
        {
          "description": "Auditing a sentiment analysis model for customer reviews by visualising how attention weights differ when processing reviews from different demographic groups, discovering that the model pays disproportionate attention to certain cultural expressions or colloquialisms that could lead to biased sentiment predictions.",
          "goal": "Fairness"
        },
        {
          "description": "Creating visual explanations for regulatory compliance in a financial document classification system, showing which specific words and phrases in loan applications or contracts triggered particular risk assessments, enabling auditors to verify that decisions are based on legitimate financial factors rather than discriminatory language patterns.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "High attention weights do not necessarily indicate causal importance for predictions, as models may attend strongly to tokens that serve structural rather than semantic purposes."
        },
        {
          "description": "The sheer number of attention heads and layers in modern transformers creates visualisation overload, making it difficult to identify meaningful patterns without systematic analysis tools."
        },
        {
          "description": "Attention patterns can be misleading when models use residual connections and layer normalisation, as the final representation incorporates information beyond what attention weights suggest."
        },
        {
          "description": "Different transformer architectures (encoder-only, decoder-only, encoder-decoder) exhibit fundamentally different attention patterns, limiting the generalisability of insights across model types."
        },
        {
          "description": "The technique cannot explain the reasoning process within feed-forward layers or how attention patterns translate into specific predictions, providing only a partial view of model behaviour."
        }
      ],
      "resources": [
        {
          "title": "jessevig/bertviz",
          "url": "https://github.com/jessevig/bertviz",
          "source_type": "software_package"
        },
        {
          "title": "Attention is All You Need",
          "url": "https://arxiv.org/abs/1706.03762",
          "source_type": "technical_paper",
          "authors": [
            "Ashish Vaswani",
            "Noam Shazeer",
            "Niki Parmar",
            "Jakob Uszkoreit",
            "Llion Jones",
            "Aidan N. Gomez",
            "Lukasz Kaiser",
            "Illia Polosukhin"
          ],
          "publication_date": "2017-06-12"
        },
        {
          "title": "Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned",
          "url": "https://arxiv.org/abs/1905.09418",
          "source_type": "technical_paper",
          "authors": [
            "Elena Voita",
            "David Talbot",
            "Fedor Moiseev",
            "Rico Sennrich",
            "Ivan Titov"
          ],
          "publication_date": "2019-05-23"
        },
        {
          "title": "What Does BERT Look At? An Analysis of BERT's Attention",
          "url": "https://arxiv.org/abs/1906.04341",
          "source_type": "technical_paper",
          "authors": [
            "Kevin Clark",
            "Urvashi Khandelwal",
            "Omer Levy",
            "Christopher D. Manning"
          ],
          "publication_date": "2019-06-11"
        },
        {
          "title": "Transformer Explainability Beyond Attention Visualization",
          "url": "https://arxiv.org/abs/2012.09838",
          "source_type": "technical_paper",
          "authors": [
            "Hila Chefer",
            "Shir Gur",
            "Lior Wolf"
          ],
          "publication_date": "2020-12-17"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "related_techniques": [
        "feature-attribution-with-integrated-gradients-in-nlp",
        "causal-mediation-analysis-in-language-models",
        "integrated-gradients",
        "classical-attention-analysis-in-neural-networks"
      ]
    },
    {
      "slug": "adversarial-robustness-testing",
      "name": "Adversarial Robustness Testing",
      "description": "Adversarial robustness testing evaluates model resilience against intentionally crafted inputs designed to cause misclassification or malfunction. This technique generates adversarial examples through methods like FGSM, PGD, Carlini & Wagner (C&W) attacks, and AutoAttack to measure model vulnerability. Testing assesses both white-box scenarios (where attackers have full model access) and black-box scenarios (where attackers only observe inputs and outputs), measuring metrics like robust accuracy, attack success rates, and perturbation budgets required for successful attacks.",
      "assurance_goals": [
        "Security",
        "Reliability",
        "Safety"
      ],
      "tags": [
        "applicable-models/architecture/model-agnostic",
        "applicable-models/architecture/neural-networks",
        "applicable-models/requirements/black-box",
        "applicable-models/requirements/white-box",
        "assurance-goal-category/security",
        "assurance-goal-category/reliability",
        "assurance-goal-category/safety",
        "data-requirements/no-special-requirements",
        "data-type/image",
        "data-type/text",
        "data-type/tabular",
        "evidence-type/quantitative-metric",
        "evidence-type/test-results",
        "expertise-needed/ml-engineering",
        "expertise-needed/domain-expertise",
        "lifecycle-stage/model-evaluation",
        "lifecycle-stage/post-deployment",
        "technique-type/testing"
      ],
      "example_use_cases": [
        {
          "description": "Testing an autonomous vehicle's perception system against adversarial perturbations to road signs or traffic signals, ensuring the vehicle cannot be fooled by maliciously modified visual inputs.",
          "goal": "Safety"
        },
        {
          "description": "Evaluating a spam filter's robustness to adversarial text manipulations such as character substitutions, synonym replacements, and formatting tricks that attackers use to craft phishing emails that evade detection whilst remaining readable to humans.",
          "goal": "Security"
        },
        {
          "description": "Assessing whether a medical imaging classifier maintains reliable performance when images are slightly corrupted or manipulated, preventing misdiagnosis from low-quality or tampered inputs.",
          "goal": "Reliability"
        },
        {
          "description": "Testing a credit risk assessment model against adversarial perturbations in loan application data, ensuring attackers cannot manipulate minor input features like stated income or employment history to obtain fraudulent credit approvals.",
          "goal": "Security"
        }
      ],
      "limitations": [
        {
          "description": "Adversarial examples often transfer poorly between different models or architectures, making it difficult to comprehensively test against all possible attacks."
        },
        {
          "description": "Focus on worst-case perturbations may not reflect realistic threat models, as some adversarial examples require unrealistic levels of control over inputs."
        },
        {
          "description": "Computationally expensive to generate strong adversarial examples, often requiring 10-100x more computation than standard inference, especially for large models or high-dimensional inputs."
        },
        {
          "description": "Trade-off between robustness and accuracy means improving adversarial robustness often reduces performance on clean, unperturbed data."
        },
        {
          "description": "Requires representative test data covering expected deployment scenarios, as adversarial testing on narrow datasets may miss vulnerabilities that emerge in real-world conditions."
        },
        {
          "description": "Requires expertise in both attack methodology and domain knowledge to design meaningful adversarial perturbations that reflect genuine threats rather than artificial edge cases."
        }
      ],
      "resources": [
        {
          "title": "Welcome to the Adversarial Robustness Toolbox — Adversarial ...",
          "url": "https://adversarial-robustness-toolbox.readthedocs.io/",
          "source_type": "documentation"
        },
        {
          "title": "SecML-Torch: A Library for Robustness Evaluation of Deep ...",
          "url": "https://secml-torch.readthedocs.io/",
          "source_type": "software_package"
        },
        {
          "title": "A Survey of Neural Network Robustness Assessment in Image Recognition",
          "url": "https://www.semanticscholar.org/paper/4ff905df12fe2e5d3ef9801f22cdf301124cb0cb",
          "source_type": "technical_paper",
          "authors": [
            "Jie Wang",
            "Jun Ai",
            "Minyan Lu",
            "Haoran Su",
            "Dan Yu",
            "Yutao Zhang",
            "Junda Zhu",
            "Jingyu Liu"
          ]
        },
        {
          "title": "Adversarial Training: A Survey",
          "url": "https://www.semanticscholar.org/paper/3181007ef16737020b51d5f77ec2855bf2b82b0e",
          "source_type": "technical_paper",
          "authors": [
            "Mengnan Zhao",
            "Lihe Zhang",
            "Jingwen Ye",
            "Huchuan Lu",
            "Baocai Yin",
            "Xinchao Wang"
          ]
        },
        {
          "title": "What is an adversarial attack in NLP? — TextAttack 0.3.10 ...",
          "url": "https://textattack.readthedocs.io/en/latest/1start/what_is_an_adversarial_attack.html",
          "source_type": "tutorial"
        }
      ],
      "related_techniques": [
        "data-poisoning-detection",
        "model-watermarking-and-theft-detection",
        "multi-agent-system-testing",
        "prompt-injection-testing"
      ]
    },
    {
      "slug": "embedding-bias-analysis",
      "name": "Embedding Bias Analysis",
      "description": "Embedding bias analysis examines learned representations to identify biases, spurious correlations, and problematic clustering patterns in vector embeddings. This technique uses dimensionality reduction, clustering analysis, and geometric fairness metrics to examine whether embeddings encode sensitive attributes, place certain groups in disadvantaged regions of representation space, or learn stereotypical associations. Analysis reveals whether embeddings used for retrieval, recommendation, or downstream tasks contain fairness issues that will propagate to applications.",
      "assurance_goals": [
        "Fairness",
        "Explainability",
        "Transparency"
      ],
      "tags": [
        "applicable-models/architecture/neural-networks",
        "applicable-models/architecture/neural-networks/transformer",
        "applicable-models/architecture/neural-networks/transformer/llm",
        "applicable-models/architecture/neural-networks/convolutional",
        "applicable-models/requirements/model-internals",
        "assurance-goal-category/fairness",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/explains/internal-mechanisms",
        "assurance-goal-category/explainability/visualization-methods/activation-maps",
        "assurance-goal-category/transparency",
        "data-type/text",
        "data-type/image",
        "data-requirements/training-data-required",
        "lifecycle-stage/model-evaluation",
        "lifecycle-stage/post-deployment",
        "technique-type/analytical",
        "technique-type/testing",
        "evidence-type/quantitative-metric",
        "evidence-type/visual-artifact",
        "evidence-type/qualitative-assessment",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistical-knowledge",
        "explanatory-scope/global",
        "fairness-approach/fairness-metrics"
      ],
      "example_use_cases": [
        {
          "description": "Auditing word embeddings for biased gender associations like 'doctor' being closer to 'male' than 'female', identifying problematic stereotypes that could affect downstream NLP applications.",
          "goal": "Fairness"
        },
        {
          "description": "Analyzing face recognition embeddings to understand whether certain demographic groups are clustered more tightly (enabling easier discrimination) or more dispersed (enabling easier misidentification).",
          "goal": "Explainability"
        },
        {
          "description": "Transparently documenting embedding space properties including which attributes are encoded and what associations exist, enabling informed decisions about appropriate use cases.",
          "goal": "Transparency"
        },
        {
          "description": "Analyzing clinical note embeddings in a medical diagnosis support system to detect whether disease associations cluster along demographic lines (e.g., certain conditions being systematically closer to particular age or ethnic groups in the embedding space), which could lead to biased treatment recommendations.",
          "goal": "Fairness"
        },
        {
          "description": "Examining embeddings in a legal document analysis system to identify whether risk assessment features encode problematic associations between demographic attributes and recidivism concepts, revealing bias that could affect sentencing or parole decisions.",
          "goal": "Explainability"
        },
        {
          "description": "Auditing loan application embeddings to detect whether protected characteristics (ethnicity, gender, age) cluster near creditworthiness concepts in ways that could enable indirect discrimination in lending decisions.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Defining ground truth for 'fair' or 'unbiased' embedding geometries is challenging and may depend on application-specific requirements."
        },
        {
          "description": "Debiasing embeddings can reduce performance on downstream tasks or remove useful information along with unwanted biases."
        },
        {
          "description": "High-dimensional embedding spaces are difficult to visualize and interpret comprehensively, potentially missing subtle bias patterns."
        },
        {
          "description": "Embeddings may encode bias in complex, non-linear ways that simple geometric metrics fail to capture."
        },
        {
          "description": "Requires access to model internals and training data to extract and analyze embeddings, making this technique infeasible for black-box commercial models or systems where proprietary data cannot be shared."
        },
        {
          "description": "Analyzing large-scale embedding spaces can be computationally expensive and requires sophisticated understanding of both vector space geometry and the specific domain to interpret results meaningfully."
        },
        {
          "description": "Effective bias analysis requires representative test datasets covering relevant demographic groups and attributes, which may be difficult to obtain for sensitive domains or protected characteristics."
        }
      ],
      "resources": [
        {
          "title": "lmcinnes/umap",
          "url": "https://github.com/lmcinnes/umap",
          "source_type": "software_package"
        },
        {
          "title": "harveyslash/TSNE-UMAP-Embedding-Visualisation",
          "url": "https://github.com/harveyslash/TSNE-UMAP-Embedding-Visualisation",
          "source_type": "software_package"
        },
        {
          "title": "A new embedding quality assessment method for manifold learning",
          "url": "https://www.sciencedirect.com/science/article/pii/S092523121200389X",
          "source_type": "technical_paper"
        },
        {
          "title": "Unsupervised embedding quality evaluation",
          "url": "https://proceedings.mlr.press/v221/tsitsulin23a.html",
          "source_type": "technical_paper"
        },
        {
          "title": "Semantic Certainty Assessment in Vector Retrieval Systems: A Novel Framework for Embedding Quality Evaluation",
          "url": "https://arxiv.org/abs/2507.05933",
          "source_type": "technical_paper"
        }
      ],
      "related_techniques": [
        "attention-visualisation-in-transformers",
        "concept-activation-vectors",
        "chain-of-thought-faithfulness-evaluation",
        "few-shot-fairness-evaluation"
      ]
    },
    {
      "slug": "multimodal-alignment-evaluation",
      "name": "Multimodal Alignment Evaluation",
      "description": "Multimodal alignment evaluation assesses whether different modalities (vision, language, audio) are synchronised and consistent in multimodal AI systems. This technique tests whether descriptions match content, sounds associate with correct sources, and cross-modal reasoning maintains accuracy. It produces alignment scores, grounding quality metrics, and reports on modality-specific failure patterns.",
      "assurance_goals": [
        "Reliability",
        "Explainability",
        "Safety"
      ],
      "tags": [
        "applicable-models/architecture/neural-networks",
        "applicable-models/architecture/neural-networks/transformer",
        "applicable-models/requirements/black-box",
        "assurance-goal-category/reliability",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/explains/internal-mechanisms",
        "assurance-goal-category/safety",
        "data-type/image",
        "data-type/text",
        "data-requirements/labeled-data-required",
        "lifecycle-stage/model-evaluation",
        "lifecycle-stage/post-deployment",
        "technique-type/testing",
        "technique-type/analytical",
        "evidence-type/quantitative-metric",
        "evidence-type/qualitative-assessment",
        "evidence-type/test-results",
        "expertise-needed/ml-engineering",
        "expertise-needed/domain-expertise"
      ],
      "example_use_cases": [
        {
          "description": "Verifying that a vision-language model for medical imaging correctly associates diagnostic findings in radiology reports with corresponding visual features in scans, preventing misdiagnosis from misaligned interpretations.",
          "goal": "Safety"
        },
        {
          "description": "Testing accessibility tools that generate image descriptions for visually impaired users, ensuring descriptions accurately reflect actual visual content and don't misrepresent important details or context.",
          "goal": "Reliability"
        },
        {
          "description": "Evaluating whether visual grounding mechanisms correctly highlight image regions corresponding to generated text descriptions, enabling users to verify and understand model reasoning.",
          "goal": "Explainability"
        },
        {
          "description": "Evaluating alignment in autonomous vehicle systems where camera, lidar, and radar data must be synchronised with language-based reasoning about driving scenarios, ensuring object detection, tracking, and decision explanations remain consistent across modalities.",
          "goal": "Safety"
        },
        {
          "description": "Testing e-commerce product recommendation systems to verify that visual product features align with textual descriptions and user queries, preventing mismatched recommendations that frustrate customers or misrepresent products.",
          "goal": "Reliability"
        },
        {
          "description": "Validating educational content generation tools that create visual learning materials with text explanations, ensuring diagrams, images, and written content present consistent information without contradictions that could confuse learners.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Defining ground truth for proper alignment can be subjective, especially for abstract concepts or implicit relationships between modalities."
        },
        {
          "description": "Models may appear aligned on simple cases but fail on complex scenarios requiring deep understanding of both modalities."
        },
        {
          "description": "Evaluation requires multimodal datasets with high-quality annotations linking different modalities, which are expensive to create."
        },
        {
          "description": "Adversarial testing may not cover all possible misalignment scenarios, particularly rare or subtle cases of modal inconsistency."
        },
        {
          "description": "Multimodal alignment evaluation requires processing multiple data types simultaneously, increasing computational costs by 2-5x compared to single-modality evaluation, particularly for video or high-resolution image analysis."
        },
        {
          "description": "Creating benchmarks requires expertise across multiple domains (computer vision, NLP, audio processing) and application-specific knowledge, making it difficult to assemble qualified evaluation teams."
        },
        {
          "description": "Annotating multimodal datasets with alignment ground truth is labor-intensive and expensive, typically costing 3-10x more per sample than single-modality annotation due to increased complexity."
        }
      ],
      "resources": [
        {
          "title": "CLIP",
          "url": "https://huggingface.co/docs/transformers/en/model_doc/clip",
          "source_type": "documentation"
        },
        {
          "title": "Welcome to verl's documentation! — verl documentation",
          "url": "https://verl.readthedocs.io/",
          "source_type": "documentation"
        },
        {
          "title": "TRL - Transformer Reinforcement Learning",
          "url": "https://huggingface.co/docs/trl/en/index",
          "source_type": "documentation"
        },
        {
          "title": "An Efficient Approach for Calibration of Automotive Radar–Camera With Real-Time Projection of Multimodal Data",
          "url": "https://www.semanticscholar.org/paper/07c4dd6b40c1e284e565463586bbeb66f61c0cb7",
          "source_type": "technical_paper",
          "authors": [
            "Nitish Kumar",
            "Ayush Dasgupta",
            "Venkata Satyanand Mutnuri",
            "Rajalakshmi Pachamuthu"
          ]
        },
        {
          "title": "Integration of Large Language Models and Computer Vision Algorithms in LMS: A Methodology for Automated Verification of Software Tasks and Multimodal Analysis of Educational Data",
          "url": "https://www.semanticscholar.org/paper/0991fef3f336b1ec981d83d7a7c6b8ea2e27598f",
          "source_type": "technical_paper",
          "authors": [
            "E. I. Markin",
            "V. V. Zuparova",
            "A. I. Martyshkin"
          ]
        }
      ],
      "related_techniques": [
        "adversarial-robustness-testing",
        "constitutional-ai-evaluation",
        "hallucination-detection",
        "jailbreak-resistance-testing"
      ]
    }
  ],
  "count": 7
}