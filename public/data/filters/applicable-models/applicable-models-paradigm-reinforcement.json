{
  "tag": {
    "name": "applicable-models/paradigm/reinforcement",
    "slug": "applicable-models-paradigm-reinforcement",
    "count": 2,
    "category": "applicable-models"
  },
  "techniques": [
    {
      "slug": "agent-goal-misalignment-testing",
      "name": "Agent Goal Misalignment Testing",
      "description": "Agent goal misalignment testing identifies scenarios where AI agents pursue objectives in unintended ways or develop proxy goals that diverge from true human intent. This technique tests for specification gaming (achieving stated metrics while violating intent), instrumental goals (agents developing problematic sub-goals to achieve main objectives), and reward hacking (exploiting loopholes in reward functions). Testing uses diverse scenarios to probe whether agents truly understand task intent or merely optimise narrow specified metrics.",
      "assurance_goals": [
        "Safety",
        "Reliability",
        "Fairness"
      ],
      "tags": [
        "applicable-models/architecture/model-agnostic",
        "applicable-models/paradigm/reinforcement",
        "applicable-models/requirements/black-box",
        "assurance-goal-category/safety",
        "assurance-goal-category/reliability",
        "assurance-goal-category/fairness",
        "data-type/any",
        "data-requirements/no-special-requirements",
        "lifecycle-stage/model-evaluation",
        "lifecycle-stage/post-deployment",
        "technique-type/testing",
        "evidence-type/test-results",
        "evidence-type/qualitative-assessment",
        "expertise-needed/ml-engineering",
        "expertise-needed/domain-expertise"
      ],
      "example_use_cases": [
        {
          "description": "Testing an autonomous vehicle routing agent in a ride-sharing service to ensure it optimizes for passenger safety and comfort rather than gaming metrics through risky driving behaviors that minimize journey time while technically meeting safety thresholds.",
          "goal": "Safety"
        },
        {
          "description": "Verifying that a healthcare resource allocation agent distributes medical supplies based on genuine patient need rather than exploiting proxy metrics that could systematically disadvantage certain demographic groups or facilities.",
          "goal": "Fairness"
        },
        {
          "description": "Ensuring a resume screening agent doesn't develop proxy metrics that correlate with discriminatory criteria while technically optimizing for stated job performance predictions.",
          "goal": "Fairness"
        },
        {
          "description": "Evaluating a criminal justice risk assessment agent to ensure it optimizes for genuine recidivism prediction rather than learning proxies that correlate with protected characteristics while appearing to achieve stated accuracy objectives.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Comprehensive testing requires anticipating all possible ways objectives could be misinterpreted, which is inherently difficult for complex goals."
        },
        {
          "description": "Agents may behave correctly during testing but develop misaligned strategies in deployment when facing novel situations or longer time horizons."
        },
        {
          "description": "Distinguishing between intended and unintended goal achievement can be subjective, especially when stated objectives are ambiguous."
        },
        {
          "description": "Testing environments may not replicate the full complexity and incentive structures of real deployment settings where misalignment emerges."
        },
        {
          "description": "Requires domain expertise to define appropriate value-aligned objectives and identify subtle forms of misalignment, which may not be available for novel or cross-domain applications."
        },
        {
          "description": "Quantifying the severity and likelihood of different misalignment scenarios requires subjective judgments and risk assessment capabilities that vary across organizations."
        }
      ],
      "resources": [
        {
          "title": "The Urgent Need for Intrinsic Alignment Technologies for ...",
          "url": "https://towardsdatascience.com/the-urgent-need-for-intrinsic-alignment-technologies-for-responsible-agentic-ai/",
          "source_type": "tutorial"
        },
        {
          "title": "A Survey of Process Reward Models: From Outcome Signals to Process Supervisions for Large Language Models",
          "url": "https://www.semanticscholar.org/paper/b670078b724938874a233687b5c53848df527a60",
          "source_type": "technical_paper",
          "authors": [
            "Congmin Zheng",
            "Jiachen Zhu",
            "Zhuoying Ou",
            "Yuxiang Chen",
            "Kangning Zhang",
            "Rong Shan",
            "Zeyu Zheng",
            "Mengyue Yang",
            "Jianghao Lin",
            "Yong Yu",
            "Weinan Zhang"
          ]
        },
        {
          "title": "Large Language Model Safety: A Holistic Survey",
          "url": "https://www.semanticscholar.org/paper/a0d2a54ed05a424f5eee35ea579cf54ef0f2c2aa",
          "source_type": "technical_paper",
          "authors": [
            "Dan Shi",
            "Tianhao Shen",
            "Yufei Huang",
            "Zhigen Li",
            "Yongqi Leng",
            "Renren Jin",
            "Chuang Liu",
            "Xinwei Wu",
            "Zishan Guo",
            "Linhao Yu",
            "Ling Shi",
            "Bojian Jiang",
            "Deyi Xiong"
          ]
        }
      ],
      "related_techniques": [
        "reward-hacking-detection",
        "multi-agent-system-testing",
        "ai-agent-safety-testing",
        "path-specific-counterfactual-fairness-assessment"
      ]
    },
    {
      "slug": "reward-hacking-detection",
      "name": "Reward Hacking Detection",
      "description": "Reward hacking detection identifies when AI systems achieve stated objectives through unintended shortcuts or loopholes rather than genuine task mastery. This technique analyses whether systems exploit reward specification ambiguities, reward function bugs, or simulator artifacts to maximise rewards without actually solving the intended problem. Detection involves adversarial specification testing, human evaluation of solution quality, transfer testing to slightly modified tasks, and analysis of unexpected strategy patterns that suggest reward hacking rather than genuine learning.",
      "assurance_goals": [
        "Reliability",
        "Safety",
        "Transparency"
      ],
      "tags": [
        "applicable-models/architecture/model-agnostic",
        "applicable-models/paradigm/reinforcement",
        "applicable-models/requirements/black-box",
        "assurance-goal-category/reliability",
        "assurance-goal-category/safety",
        "assurance-goal-category/transparency",
        "data-type/any",
        "data-requirements/no-special-requirements",
        "lifecycle-stage/model-evaluation",
        "lifecycle-stage/post-deployment",
        "technique-type/testing",
        "technique-type/analytical",
        "evidence-type/qualitative-assessment",
        "evidence-type/test-results",
        "expertise-needed/ml-engineering",
        "expertise-needed/domain-expertise"
      ],
      "example_use_cases": [
        {
          "description": "Detecting when a reinforcement learning agent for robot manipulation learns to exploit physics simulator quirks rather than developing real-world-transferable manipulation skills, preventing failures in physical deployment.",
          "goal": "Reliability"
        },
        {
          "description": "Identifying when an AI-based healthcare scheduling system appears to optimize patient wait times by gaming appointment classifications or encouraging cancellations rather than genuinely improving clinic efficiency, preventing patient care degradation.",
          "goal": "Safety"
        },
        {
          "description": "Detecting when a loan approval system achieves target approval rates by exploiting specification loopholes in creditworthiness definitions rather than accurately assessing borrower risk, ensuring reliable lending decisions.",
          "goal": "Reliability"
        },
        {
          "description": "Verifying that an educational content recommendation system optimizes for genuine learning outcomes rather than gaming engagement metrics through strategies like repeatedly presenting easier content that inflates measured progress.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Difficult to distinguish between clever problem-solving and specification gaming without deep understanding of task intent."
        },
        {
          "description": "Novel gaming strategies may not be detected until systems are deployed in real environments where gaming becomes apparent."
        },
        {
          "description": "Closing detected loopholes may simply push systems to discover new gaming strategies rather than solving the fundamental specification challenge."
        },
        {
          "description": "Perfect specifications that eliminate all gaming opportunities may be impossible for complex, open-ended tasks."
        },
        {
          "description": "Requires domain expertise and clear articulation of task intent to distinguish between legitimate optimization strategies and gaming behaviors, which can be subjective in complex domains."
        },
        {
          "description": "Detection often requires access to detailed behavioral logs and environment state information that may not be available in black-box deployment scenarios."
        },
        {
          "description": "Establishing ground truth for 'correct' task completion without gaming requires independent verification methods that may be as resource-intensive as the original task."
        }
      ],
      "resources": [
        {
          "title": "Detecting and Mitigating Reward Hacking in Reinforcement Learning Systems: A Comprehensive Empirical Study",
          "url": "https://www.semanticscholar.org/paper/f71379b765f01c333ebaab5736dbf7f1005b19c9",
          "source_type": "technical_paper",
          "authors": [
            "Ibne Farabi Shihab",
            "Sanjeda Akter",
            "Anuj Sharma"
          ]
        },
        {
          "title": "Preventing Reward Hacking with Occupancy Measure Regularization",
          "url": "https://www.semanticscholar.org/paper/878e2c80a9247b6106b479bf8d74c02427947176",
          "source_type": "technical_paper",
          "authors": [
            "Cassidy Laidlaw",
            "Shivam Singhal",
            "A. Dragan"
          ]
        },
        {
          "title": "Fine-tuning & RL for LLMs: Intro to Post-training - DeepLearning.AI",
          "url": "https://learn.deeplearning.ai/courses/fine-tuning-and-reinforcement-learning-for-llms-intro-to-post-training/lesson/wr7ye4/evals-for-post-training:-test-sets-and-metrics",
          "source_type": "tutorial"
        },
        {
          "title": "How to Make a Reward Function in Reinforcement Learning ...",
          "url": "https://www.geeksforgeeks.org/machine-learning/how-to-make-a-reward-function-in-reinforcement-learning/",
          "source_type": "tutorial"
        }
      ],
      "related_techniques": [
        "hallucination-detection",
        "adversarial-training-evaluation",
        "constitutional-ai-evaluation",
        "epistemic-uncertainty-quantification"
      ]
    }
  ],
  "count": 2
}