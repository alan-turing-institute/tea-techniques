{
  "tag": {
    "name": "applicable-models/architecture/neural-networks/convolutional",
    "slug": "applicable-models-architecture-neural-networks-convolutional",
    "count": 2,
    "category": "applicable-models"
  },
  "techniques": [
    {
      "slug": "gradient-weighted-class-activation-mapping",
      "name": "Gradient-weighted Class Activation Mapping",
      "description": "Grad-CAM creates visual heatmaps showing which regions of an image a convolutional neural network focuses on when making a specific classification. Unlike pixel-level techniques, Grad-CAM produces coarser region-based explanations by using gradients from the predicted class to weight the CNN's final feature maps, then projecting these weighted activations back to create an overlay on the original image. This provides intuitive visual explanations of where the model is 'looking' for evidence of different classes.",
      "assurance_goals": [
        "Explainability",
        "Fairness"
      ],
      "tags": [
        "applicable-models/architecture/neural-networks/convolutional",
        "applicable-models/requirements/architecture-specific",
        "applicable-models/requirements/gradient-access",
        "applicable-models/requirements/white-box",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/attribution-methods/gradient-based",
        "assurance-goal-category/explainability/explains/feature-importance",
        "assurance-goal-category/explainability/explains/internal-mechanisms",
        "assurance-goal-category/explainability/property/comprehensibility",
        "assurance-goal-category/explainability/property/efficiency",
        "assurance-goal-category/explainability/property/sparsity",
        "assurance-goal-category/explainability/visualization-methods/activation-maps",
        "assurance-goal-category/fairness",
        "data-requirements/access-to-model-internals",
        "data-type/image",
        "evidence-type/visualization",
        "expertise-needed/ml-engineering",
        "explanatory-scope/local",
        "lifecycle-stage/model-development",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Validating that a melanoma detection model focuses on the actual skin lesion rather than surrounding healthy skin, medical equipment, or artifacts when making cancer/benign classifications.",
          "goal": "Explainability"
        },
        {
          "description": "Debugging an autonomous vehicle's traffic sign recognition system by visualising whether the model correctly focuses on the sign itself rather than background objects, shadows, or irrelevant visual elements.",
          "goal": "Explainability"
        },
        {
          "description": "Auditing a medical imaging system for racial bias by examining whether diagnostic predictions inappropriately focus on skin tone regions rather than actual pathological features, ensuring equitable healthcare AI deployment.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Requires access to the CNN's internal feature maps and gradients, limiting use to white-box scenarios."
        },
        {
          "description": "Resolution is constrained by the final convolutional layer's feature map size, producing coarser localisation than pixel-level methods."
        },
        {
          "description": "Only applicable to CNN architectures with clearly defined convolutional layers, not suitable for other neural network types."
        },
        {
          "description": "May highlight regions that correlate with the class but aren't causally important for the model's decision-making process."
        }
      ],
      "resources": [
        {
          "title": "Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization",
          "url": "https://arxiv.org/abs/1610.02391",
          "source_type": "technical_paper",
          "authors": [
            "Ramprasaath R. Selvaraju",
            "Michael Cogswell",
            "Abhishek Das",
            "Ramakrishna Vedantam",
            "Devi Parikh",
            "Dhruv Batra"
          ],
          "publication_date": "2016-10-07"
        },
        {
          "title": "jacobgil/pytorch-grad-cam",
          "url": "https://github.com/jacobgil/pytorch-grad-cam",
          "source_type": "software_package"
        },
        {
          "title": "Grad-CAM: Visualize class activation maps with Keras, TensorFlow ...",
          "url": "https://pyimagesearch.com/2020/03/09/grad-cam-visualize-class-activation-maps-with-keras-tensorflow-and-deep-learning/",
          "source_type": "tutorial"
        },
        {
          "title": "kazuto1011/grad-cam-pytorch",
          "url": "https://github.com/kazuto1011/grad-cam-pytorch",
          "source_type": "software_package"
        },
        {
          "title": "A Tutorial on Explainable Image Classification for Dementia Stages Using Convolutional Neural Network and Gradient-weighted Class Activation Mapping",
          "url": "https://www.semanticscholar.org/paper/8b1139cb06cfe5ba69e2fd05e1450b43df031a02",
          "source_type": "technical_paper",
          "authors": [
            "Kevin Kam Fung Yuen"
          ],
          "publication_date": "2024-08-20"
        },
        {
          "title": "A Guide to Grad-CAM in Deep Learning - Analytics Vidhya",
          "url": "https://www.analyticsvidhya.com/blog/2023/12/grad-cam-in-deep-learning/",
          "source_type": "tutorial"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "acronym": "Grad-CAM",
      "related_techniques": [
        "integrated-gradients",
        "saliency-maps",
        "occlusion-sensitivity",
        "local-interpretable-model-agnostic-explanations"
      ]
    },
    {
      "slug": "embedding-bias-analysis",
      "name": "Embedding Bias Analysis",
      "description": "Embedding bias analysis examines learned representations to identify biases, spurious correlations, and problematic clustering patterns in vector embeddings. This technique uses dimensionality reduction, clustering analysis, and geometric fairness metrics to examine whether embeddings encode sensitive attributes, place certain groups in disadvantaged regions of representation space, or learn stereotypical associations. Analysis reveals whether embeddings used for retrieval, recommendation, or downstream tasks contain fairness issues that will propagate to applications.",
      "assurance_goals": [
        "Fairness",
        "Explainability",
        "Transparency"
      ],
      "tags": [
        "applicable-models/architecture/neural-networks",
        "applicable-models/architecture/neural-networks/transformer",
        "applicable-models/architecture/neural-networks/transformer/llm",
        "applicable-models/architecture/neural-networks/convolutional",
        "applicable-models/requirements/model-internals",
        "assurance-goal-category/fairness",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/explains/internal-mechanisms",
        "assurance-goal-category/explainability/visualization-methods/activation-maps",
        "assurance-goal-category/transparency",
        "data-type/text",
        "data-type/image",
        "data-requirements/training-data-required",
        "lifecycle-stage/model-evaluation",
        "lifecycle-stage/post-deployment",
        "technique-type/analytical",
        "technique-type/testing",
        "evidence-type/quantitative-metric",
        "evidence-type/visual-artifact",
        "evidence-type/qualitative-assessment",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistical-knowledge",
        "explanatory-scope/global",
        "fairness-approach/fairness-metrics"
      ],
      "example_use_cases": [
        {
          "description": "Auditing word embeddings for biased gender associations like 'doctor' being closer to 'male' than 'female', identifying problematic stereotypes that could affect downstream NLP applications.",
          "goal": "Fairness"
        },
        {
          "description": "Analyzing face recognition embeddings to understand whether certain demographic groups are clustered more tightly (enabling easier discrimination) or more dispersed (enabling easier misidentification).",
          "goal": "Explainability"
        },
        {
          "description": "Transparently documenting embedding space properties including which attributes are encoded and what associations exist, enabling informed decisions about appropriate use cases.",
          "goal": "Transparency"
        },
        {
          "description": "Analyzing clinical note embeddings in a medical diagnosis support system to detect whether disease associations cluster along demographic lines (e.g., certain conditions being systematically closer to particular age or ethnic groups in the embedding space), which could lead to biased treatment recommendations.",
          "goal": "Fairness"
        },
        {
          "description": "Examining embeddings in a legal document analysis system to identify whether risk assessment features encode problematic associations between demographic attributes and recidivism concepts, revealing bias that could affect sentencing or parole decisions.",
          "goal": "Explainability"
        },
        {
          "description": "Auditing loan application embeddings to detect whether protected characteristics (ethnicity, gender, age) cluster near creditworthiness concepts in ways that could enable indirect discrimination in lending decisions.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Defining ground truth for 'fair' or 'unbiased' embedding geometries is challenging and may depend on application-specific requirements."
        },
        {
          "description": "Debiasing embeddings can reduce performance on downstream tasks or remove useful information along with unwanted biases."
        },
        {
          "description": "High-dimensional embedding spaces are difficult to visualize and interpret comprehensively, potentially missing subtle bias patterns."
        },
        {
          "description": "Embeddings may encode bias in complex, non-linear ways that simple geometric metrics fail to capture."
        },
        {
          "description": "Requires access to model internals and training data to extract and analyze embeddings, making this technique infeasible for black-box commercial models or systems where proprietary data cannot be shared."
        },
        {
          "description": "Analyzing large-scale embedding spaces can be computationally expensive and requires sophisticated understanding of both vector space geometry and the specific domain to interpret results meaningfully."
        },
        {
          "description": "Effective bias analysis requires representative test datasets covering relevant demographic groups and attributes, which may be difficult to obtain for sensitive domains or protected characteristics."
        }
      ],
      "resources": [
        {
          "title": "lmcinnes/umap",
          "url": "https://github.com/lmcinnes/umap",
          "source_type": "software_package"
        },
        {
          "title": "harveyslash/TSNE-UMAP-Embedding-Visualisation",
          "url": "https://github.com/harveyslash/TSNE-UMAP-Embedding-Visualisation",
          "source_type": "software_package"
        },
        {
          "title": "A new embedding quality assessment method for manifold learning",
          "url": "https://www.sciencedirect.com/science/article/pii/S092523121200389X",
          "source_type": "technical_paper"
        },
        {
          "title": "Unsupervised embedding quality evaluation",
          "url": "https://proceedings.mlr.press/v221/tsitsulin23a.html",
          "source_type": "technical_paper"
        },
        {
          "title": "Semantic Certainty Assessment in Vector Retrieval Systems: A Novel Framework for Embedding Quality Evaluation",
          "url": "https://arxiv.org/abs/2507.05933",
          "source_type": "technical_paper"
        }
      ],
      "related_techniques": [
        "attention-visualisation-in-transformers",
        "concept-activation-vectors",
        "chain-of-thought-faithfulness-evaluation",
        "few-shot-fairness-evaluation"
      ]
    }
  ],
  "count": 2
}