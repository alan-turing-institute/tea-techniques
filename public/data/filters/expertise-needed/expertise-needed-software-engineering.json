{
  "tag": {
    "name": "expertise-needed/software-engineering",
    "slug": "expertise-needed-software-engineering",
    "count": 9,
    "category": "expertise-needed"
  },
  "techniques": [
    {
      "slug": "federated-learning",
      "name": "Federated Learning",
      "description": "Federated learning enables collaborative model training across multiple distributed parties (devices, organisations, or data centres) without requiring centralised data sharing. Participants train models locally on their private datasets and only share model updates (gradients, weights, or aggregated statistics) with a central coordinator. This distributed approach serves multiple purposes: preserving data privacy and sovereignty, reducing communication costs, enabling learning from diverse data sources, improving model robustness through heterogeneous training, and facilitating compliance with data protection regulations whilst maintaining model performance comparable to centralised training.",
      "assurance_goals": [
        "Privacy",
        "Reliability",
        "Safety",
        "Fairness"
      ],
      "tags": [
        "applicable-models/architecture/linear-models",
        "applicable-models/architecture/neural-networks",
        "applicable-models/paradigm/parametric",
        "applicable-models/paradigm/supervised",
        "applicable-models/requirements/gradient-access",
        "applicable-models/requirements/white-box",
        "assurance-goal-category/fairness",
        "assurance-goal-category/privacy",
        "assurance-goal-category/reliability",
        "assurance-goal-category/safety",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/ml-engineering",
        "expertise-needed/software-engineering",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Developing a smartphone keyboard prediction model by learning from users' typing patterns without their text ever leaving their devices, enabling personalised predictions whilst maintaining complete data privacy.",
          "goal": "Privacy"
        },
        {
          "description": "Training a medical diagnosis model across multiple hospitals without sharing patient records, ensuring model robustness by learning from diverse patient populations and clinical practices across different institutions.",
          "goal": "Reliability"
        },
        {
          "description": "Creating a cybersecurity threat detection model by federating learning across financial institutions without exposing sensitive transaction data, reducing systemic risk whilst maintaining competitive confidentiality.",
          "goal": "Safety"
        },
        {
          "description": "Building a fair credit scoring model by training across multiple regions and demographics without centralising sensitive financial data, ensuring representation from diverse populations whilst respecting local data sovereignty laws.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Communication overhead can be substantial, especially with frequent model updates and large models, potentially limiting scalability and increasing training time compared to centralised approaches."
        },
        {
          "description": "Statistical heterogeneity across participants (non-IID data distributions) can lead to training instability, slower convergence, and reduced model performance compared to centralised training on pooled data."
        },
        {
          "description": "System heterogeneity in computational capabilities, network connectivity, and availability of participating devices can create bottlenecks and introduce bias towards more capable participants."
        },
        {
          "description": "Privacy vulnerabilities remain through gradient leakage attacks, model inversion, and membership inference attacks that can potentially reconstruct sensitive information from shared model updates."
        },
        {
          "description": "Coordination complexity increases with the number of participants, requiring sophisticated aggregation protocols, fault tolerance mechanisms, and secure communication infrastructure."
        }
      ],
      "resources": [
        {
          "title": "Open Federated Learning (OpenFL) Documentation",
          "url": "https://openfl.readthedocs.io/en/stable/",
          "source_type": "documentation"
        },
        {
          "title": "Federated Learning - DeepLearning.AI",
          "url": "https://www.deeplearning.ai/short-courses/intro-to-federated-learning/",
          "source_type": "tutorial"
        },
        {
          "title": "A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection",
          "url": "http://arxiv.org/pdf/1907.09693v7",
          "source_type": "documentation",
          "authors": [
            "Qinbin Li",
            "Zeyi Wen",
            "Zhaomin Wu",
            "Sixu Hu",
            "Naibo Wang",
            "Yuan Li",
            "Xu Liu",
            "Bingsheng He"
          ],
          "publication_date": "2019-07-23"
        },
        {
          "title": "Federated learning with hybrid differential privacy for secure and reliable cross-IoT platform knowledge sharing",
          "url": "https://core.ac.uk/download/603345619.pdf",
          "source_type": "technical_paper",
          "authors": [
            "Algburi, S.",
            "Algburi, S.",
            "Anupallavi, S.",
            "Anupallavi, S.",
            "Ashokkumar, S. R.",
            "Ashokkumar, S. R.",
            "Elmedany, W.",
            "Elmedany, W.",
            "Khalaf, O. I.",
            "Khalaf, O. I.",
            "Selvaraj, D.",
            "Selvaraj, D.",
            "Sharif, M. S.",
            "Sharif, M. S."
          ],
          "publication_date": "2024-01-01"
        }
      ],
      "complexity_rating": 4,
      "computational_cost_rating": 5,
      "related_techniques": [
        "differential-privacy",
        "homomorphic-encryption",
        "synthetic-data-generation",
        "data-poisoning-detection"
      ]
    },
    {
      "slug": "runtime-monitoring-and-circuit-breakers",
      "name": "Runtime Monitoring and Circuit Breakers",
      "description": "Runtime monitoring and circuit breakers establish continuous surveillance of AI/ML systems in production, tracking critical metrics such as prediction accuracy, response times, input characteristics, output distributions, and system resource usage. When monitored parameters exceed predefined safety thresholds or exhibit anomalous patterns, automated circuit breakers immediately trigger protective actions including request throttling, service degradation, system shutdown, or failover to backup mechanisms. This approach provides real-time defensive capabilities that prevent cascading failures, ensure consistent service reliability, and maintain transparent operation status for stakeholders monitoring system health.",
      "assurance_goals": [
        "Safety",
        "Reliability",
        "Transparency"
      ],
      "tags": [
        "applicable-models/architecture/model-agnostic",
        "applicable-models/requirements/black-box",
        "assurance-goal-category/reliability",
        "assurance-goal-category/safety",
        "assurance-goal-category/safety/monitoring/anomaly-detection",
        "assurance-goal-category/transparency",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "expertise-needed/ml-engineering",
        "expertise-needed/software-engineering",
        "lifecycle-stage/system-deployment-and-use",
        "lifecycle-stage/system-deployment-and-use/monitoring",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Implementing circuit breakers in a medical AI system that automatically halt diagnosis recommendations if prediction confidence drops below 85%, error rates exceed 2%, or response times increase beyond acceptable limits, preventing potentially harmful misdiagnoses during system degradation.",
          "goal": "Safety"
        },
        {
          "description": "Deploying runtime monitoring for a recommendation engine that tracks recommendation diversity, click-through rates, and user engagement patterns, automatically switching to simpler algorithms when complex models show signs of performance degradation or unusual behaviour patterns.",
          "goal": "Reliability"
        },
        {
          "description": "Establishing transparent monitoring dashboards for a loan approval system that display real-time metrics on approval rates across demographic groups, processing times, and model confidence levels, enabling stakeholders to verify consistent and fair operation.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Threshold calibration requires extensive domain expertise and historical data analysis, as overly sensitive settings trigger excessive false alarms whilst conservative thresholds may miss genuine system failures."
        },
        {
          "description": "False positive alerts can unnecessarily disrupt service availability and user experience, potentially causing more harm than the issues they aim to prevent, especially in time-sensitive applications."
        },
        {
          "description": "Sophisticated attacks or gradual performance degradation may operate within normal metric ranges, evading detection by staying below established thresholds whilst still causing cumulative damage."
        },
        {
          "description": "Monitoring infrastructure introduces additional complexity and potential failure points, requiring robust implementation to avoid situations where the monitoring system itself becomes a source of system instability."
        },
        {
          "description": "High-frequency monitoring and circuit breaker mechanisms can add computational overhead and latency to system operations, potentially impacting performance in resource-constrained environments."
        }
      ],
      "resources": [
        {
          "title": "aiobreaker: Python Circuit Breaker for Asyncio",
          "url": "https://github.com/arlyon/aiobreaker",
          "source_type": "software_package",
          "description": "Python library implementing the Circuit Breaker design pattern for asyncio applications, preventing system-wide failures by protecting integration points with configurable failure thresholds and reset timeouts"
        },
        {
          "title": "Improving Alignment and Robustness with Circuit Breakers",
          "url": "https://arxiv.org/html/2406.04313v4",
          "source_type": "technical_paper",
          "authors": [
            "Andy Zou"
          ],
          "description": "Research paper introducing circuit breakers for AI safety that directly interrupt harmful model representations during generation, significantly reducing attack success rates while maintaining model capabilities"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "related_techniques": [
        "anomaly-detection",
        "confidence-thresholding",
        "human-in-the-loop-safeguards",
        "safety-guardrails"
      ]
    },
    {
      "slug": "mlflow-experiment-tracking",
      "name": "MLflow Experiment Tracking",
      "description": "MLflow is an open-source platform that tracks machine learning experiments by automatically logging parameters, metrics, models, and artifacts throughout the ML lifecycle. It provides a centralised repository for comparing different experimental runs, reproducing results, and managing model versions. Teams can track hyperparameters, evaluation metrics, model files, and execution environment details, creating a comprehensive audit trail that supports collaboration, reproducibility, and regulatory compliance across the entire machine learning development process.",
      "assurance_goals": [
        "Transparency",
        "Reliability"
      ],
      "tags": [
        "applicable-models/architecture/model-agnostic",
        "applicable-models/requirements/black-box",
        "assurance-goal-category/reliability",
        "assurance-goal-category/transparency",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/documentation",
        "expertise-needed/ml-engineering",
        "expertise-needed/software-engineering",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/process"
      ],
      "example_use_cases": [
        {
          "description": "Tracking medical diagnosis model experiments across different hospitals, logging hyperparameters, performance metrics, and model artifacts to ensure reproducible research and enable regulatory audits of model development processes.",
          "goal": "Transparency"
        },
        {
          "description": "Managing fraud detection model versions in production, tracking which specific model configuration and training data version is deployed, enabling quick rollback and performance comparison when system reliability issues arise.",
          "goal": "Reliability"
        },
        {
          "description": "Documenting loan approval model experiments with complete parameter tracking and performance logging across demographic groups, supporting fair lending compliance by providing transparent records of model development and validation processes.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Requires teams to adopt disciplined logging practices and may introduce overhead to development workflows if not properly integrated into existing processes."
        },
        {
          "description": "Storage costs can grow substantially with extensive artifact logging, especially for large models or high-frequency experimentation."
        },
        {
          "description": "Tracking quality depends on developers consistently logging relevant information, with incomplete logging leading to gaps in experimental records."
        },
        {
          "description": "Complex multi-stage pipelines may require custom instrumentation to capture dependencies and data flow relationships effectively."
        },
        {
          "description": "Security and access control configurations require careful setup to protect sensitive model information and experimental data in shared environments."
        }
      ],
      "resources": [
        {
          "title": "MLflow Documentation",
          "url": "https://mlflow.org/docs/latest/index.html",
          "source_type": "documentation",
          "description": "Comprehensive official documentation covering MLflow setup, tracking APIs, model management, and deployment workflows with examples and best practices"
        },
        {
          "title": "mlflow/mlflow",
          "url": "https://github.com/mlflow/mlflow",
          "source_type": "software_package",
          "description": "Official MLflow open-source repository containing the complete platform for ML experiment tracking, model management, and deployment"
        },
        {
          "title": "An MLOps Framework for Explainable Network Intrusion Detection with MLflow",
          "url": "https://ieeexplore.ieee.org/abstract/document/10733700",
          "source_type": "technical_paper",
          "authors": [
            "Vincenzo Spadari",
            "Francesco Cerasuolo",
            "Giampaolo Bovenzi",
            "Antonio Pescapè"
          ],
          "publication_date": "2024-06-26",
          "description": "Research paper demonstrating MLflow framework application for managing machine learning pipelines in network intrusion detection, covering experiment tracking, model deployment, and monitoring across security datasets"
        },
        {
          "title": "MLflow Tutorial - Machine Learning Lifecycle Management",
          "url": "https://www.mlflow.org/docs/latest/tutorials-and-examples/tutorial.html",
          "source_type": "tutorial",
          "description": "Step-by-step tutorial demonstrating MLflow experiment tracking, model packaging, and deployment using real machine learning examples"
        }
      ],
      "complexity_rating": 2,
      "computational_cost_rating": 2,
      "related_techniques": [
        "model-development-audit-trails",
        "data-version-control",
        "automated-documentation-generation",
        "model-cards"
      ]
    },
    {
      "slug": "data-version-control",
      "name": "Data Version Control",
      "description": "Data Version Control (DVC) is a Git-like version control system specifically designed for machine learning data, models, and experiments. It tracks changes to large data files, maintains reproducible ML pipelines, and creates a complete audit trail of data transformations, model training, and evaluation processes. DVC works alongside Git to provide end-to-end lineage tracking from raw data through preprocessing, training, and deployment, enabling teams to reproduce any model version and understand exactly how datasets evolved throughout the ML lifecycle.",
      "assurance_goals": [
        "Transparency",
        "Reliability"
      ],
      "tags": [
        "applicable-models/architecture/model-agnostic",
        "applicable-models/requirements/black-box",
        "assurance-goal-category/reliability",
        "assurance-goal-category/transparency",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/documentation",
        "expertise-needed/ml-engineering",
        "expertise-needed/software-engineering",
        "lifecycle-stage/data-handling",
        "lifecycle-stage/model-development",
        "lifecycle-stage/system-deployment-and-use",
        "technique-type/process"
      ],
      "example_use_cases": [
        {
          "description": "Tracking medical imaging dataset versions and model training pipelines to ensure reproducible research results, enabling hospitals to verify which specific data version and preprocessing steps were used for regulatory submissions.",
          "goal": "Transparency"
        },
        {
          "description": "Managing credit scoring model data pipelines with complete version control of training datasets, feature engineering steps, and model artifacts, ensuring reliable model reproduction and rollback capabilities when performance issues arise.",
          "goal": "Reliability"
        },
        {
          "description": "Maintaining pharmaceutical drug discovery data lineage across multiple research teams, tracking compound datasets, feature extraction processes, and model versions to support FDA submissions with complete experimental provenance.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Requires learning Git-like workflows and CLI commands, which may have a steep learning curve for teams unfamiliar with version control systems."
        },
        {
          "description": "Storage costs can be substantial for large datasets with frequent changes, especially when maintaining multiple versions and branches of data."
        },
        {
          "description": "Complex data pipelines with many interdependencies may require significant setup time and careful configuration to track properly."
        },
        {
          "description": "Performance can degrade with very large files or datasets due to checksumming and synchronisation overhead during operations."
        },
        {
          "description": "Team coordination becomes essential as improper branch management or merge conflicts can disrupt collaborative workflows."
        }
      ],
      "resources": [
        {
          "title": "DVC Documentation",
          "url": "https://dvc.org/doc",
          "source_type": "documentation",
          "description": "Comprehensive official documentation covering DVC installation, data versioning, pipeline creation, and collaborative workflows with tutorials and best practices"
        },
        {
          "title": "iterative/dvc",
          "url": "https://github.com/iterative/dvc",
          "source_type": "software_package",
          "description": "Official DVC open-source repository containing the complete data version control system for machine learning with Git integration"
        },
        {
          "title": "DVC Tutorial - Data Version Control for Machine Learning",
          "url": "https://dvc.org/doc/start",
          "source_type": "tutorial",
          "description": "Step-by-step getting started guide demonstrating DVC basics including data tracking, pipeline creation, and experiment management"
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "acronym": "DVC",
      "related_techniques": [
        "mlflow-experiment-tracking",
        "model-development-audit-trails",
        "automated-documentation-generation",
        "model-cards"
      ]
    },
    {
      "slug": "automated-documentation-generation",
      "name": "Automated Documentation Generation",
      "description": "Automated documentation generation creates and maintains up-to-date documentation using various methods including programmatic scripts, large language models (LLMs), and extraction tools. These approaches can capture model architectures, data schemas, feature importance, performance metrics, API specifications, and lineage information without manual writing. Methods range from traditional code parsing and template-based generation to modern AI-assisted documentation that can understand context and generate human-readable explanations.",
      "assurance_goals": [
        "Transparency",
        "Reliability"
      ],
      "tags": [
        "applicable-models/architecture/model-agnostic",
        "applicable-models/requirements/black-box",
        "assurance-goal-category/reliability",
        "assurance-goal-category/transparency",
        "assurance-goal-category/transparency/documentation",
        "data-requirements/no-special-requirements",
        "data-type/any",
        "evidence-type/documentation",
        "expertise-needed/software-engineering",
        "lifecycle-stage/deployment",
        "lifecycle-stage/model-development",
        "lifecycle-stage/project-design",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Automatically generating comprehensive model cards for a healthcare AI system each time a new version is deployed, including updated performance metrics across demographic groups, data lineage information, and bias evaluation results for regulatory compliance documentation.",
          "goal": "Transparency"
        },
        {
          "description": "Using LLM-powered tools to automatically document complex financial risk models by analysing code, extracting business logic, and generating human-readable explanations of model behaviour for audit trails and stakeholder communication.",
          "goal": "Transparency"
        },
        {
          "description": "Implementing automated API documentation generation for a machine learning platform that extracts endpoint specifications, parameter definitions, and usage examples, ensuring documentation stays synchronised with code changes and reducing deployment errors from outdated documentation.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "AI-generated documentation may miss critical domain context and business logic that human experts would include, potentially leading to incomplete or misleading explanations of model behaviour."
        },
        {
          "description": "Template-based approaches often struggle with unstructured information and complex relationships between code components, limiting their ability to capture nuanced system interactions."
        },
        {
          "description": "Quality heavily depends on code quality and instrumentation comprehensiveness; poorly commented or documented source code will result in inadequate generated documentation."
        },
        {
          "description": "Maintenance overhead can be significant as automated systems require configuration updates when code structures change, and generated content may need human review for accuracy and completeness."
        },
        {
          "description": "LLM-based approaches may introduce hallucinations or inaccuracies, particularly when documenting complex technical details or domain-specific terminology without proper validation mechanisms."
        }
      ],
      "resources": [
        {
          "title": "daynin/fundoc",
          "url": "https://github.com/daynin/fundoc",
          "source_type": "software_package",
          "description": "Language-agnostic documentation generator written in Rust that enables keeping documentation synchronised with code across multiple file types and programming languages."
        },
        {
          "title": "Generative AI for Software Development - DeepLearning.AI",
          "url": "https://www.deeplearning.ai/courses/generative-ai-for-software-development/",
          "source_type": "tutorial",
          "description": "Comprehensive course covering AI-powered documentation techniques including LLM-assisted documentation generation, formatting for automated tools, and improving code documentation quality."
        },
        {
          "title": "Documentation Generator Analysis — Wiser Documentation",
          "url": "https://chiplicity.readthedocs.io/en/latest/On_Software/DocumentationGenerator.html",
          "source_type": "documentation",
          "description": "Detailed analysis and comparison of documentation generator tools including Sphinx, Doxygen, and other approaches for automated documentation workflows."
        },
        {
          "title": "pyTooling/sphinx-reports",
          "url": "https://github.com/pyTooling/sphinx-reports",
          "source_type": "software_package",
          "description": "Sphinx extension that automatically integrates software development reports (unit tests, coverage, documentation coverage) into documentation as appendix pages."
        }
      ],
      "complexity_rating": 3,
      "computational_cost_rating": 2,
      "related_techniques": [
        "model-cards",
        "datasheets-for-datasets",
        "mlflow-experiment-tracking",
        "model-development-audit-trails"
      ]
    },
    {
      "slug": "model-development-audit-trails",
      "name": "Model Development Audit Trails",
      "description": "Model development audit trails create comprehensive, immutable records of all decisions, experiments, and changes throughout the ML lifecycle. This technique captures dataset versions, training configurations, hyperparameter searches, model architecture changes, evaluation results, and deployment decisions in tamper-evident logs. Audit trails enable reproducibility, accountability, and regulatory compliance by documenting who made what changes when and why, supporting post-hoc investigation of model failures or unexpected behaviours.",
      "assurance_goals": [
        "Transparency",
        "Reliability",
        "Safety"
      ],
      "tags": [
        "applicable-models/architecture/model-agnostic",
        "applicable-models/requirements/black-box",
        "assurance-goal-category/transparency",
        "assurance-goal-category/reliability",
        "assurance-goal-category/safety",
        "data-type/any",
        "lifecycle-stage/project-planning",
        "lifecycle-stage/data-collection",
        "lifecycle-stage/model-development",
        "lifecycle-stage/model-evaluation",
        "lifecycle-stage/deployment",
        "lifecycle-stage/post-deployment",
        "technique-type/procedural",
        "technique-type/governance-framework",
        "evidence-type/documentation",
        "expertise-needed/software-engineering",
        "expertise-needed/ml-engineering",
        "data-requirements/no-special-requirements"
      ],
      "example_use_cases": [
        {
          "description": "Maintaining detailed audit trails for medical AI development enabling investigators to trace how training data, model architecture, and evaluation decisions led to specific diagnostic behaviors during regulatory review.",
          "goal": "Transparency"
        },
        {
          "description": "Recording all model updates and performance changes over time to support root cause analysis when deployed systems exhibit unexpected behavior or reliability degradation.",
          "goal": "Reliability"
        },
        {
          "description": "Documenting safety-critical decisions like dataset filtering, bias testing, and red teaming results to demonstrate due diligence in preventing harmful deployments.",
          "goal": "Safety"
        },
        {
          "description": "Documenting credit scoring model development for regulatory compliance, maintaining detailed records of data sources, feature engineering decisions, fairness testing, and validation results to demonstrate adherence to fair lending requirements during audits.",
          "goal": "Transparency"
        },
        {
          "description": "Creating comprehensive audit trails for criminal justice risk assessment tools to enable external review of training data selection, bias mitigation techniques, and validation methodologies when legal challenges question algorithmic fairness.",
          "goal": "Fairness"
        },
        {
          "description": "Maintaining development logs for autonomous vehicle perception systems to support accident investigations, enabling forensic analysis of which model version was deployed, what training data informed its behavior, and what testing validated its safety.",
          "goal": "Safety"
        }
      ],
      "limitations": [
        {
          "description": "Comprehensive logging generates large volumes of data requiring significant storage infrastructure and data management."
        },
        {
          "description": "Audit trails may contain sensitive information about proprietary techniques, requiring careful access control and redaction procedures."
        },
        {
          "description": "Creating meaningful audit trails requires discipline and tooling integration that may slow development velocity."
        },
        {
          "description": "Retrospective analysis of audit trails can be time-consuming and requires expertise to extract actionable insights from complex logs."
        },
        {
          "description": "Implementing comprehensive audit trail systems requires integrating with diverse development tools (version control, experiment tracking, data pipelines), which can be complex and may require custom development for organisation-specific workflows."
        },
        {
          "description": "Storage costs can be substantial, with comprehensive model development projects generating terabytes of logs, experimental artifacts, and dataset versions requiring long-term retention for compliance purposes."
        }
      ],
      "resources": [
        {
          "title": "mlflow/mlflow",
          "url": "https://github.com/mlflow/mlflow",
          "source_type": "software_package"
        },
        {
          "title": "iterative/dvc",
          "url": "https://github.com/iterative/dvc",
          "source_type": "software_package"
        },
        {
          "title": "Advances in Data Lineage, Auditing, and Governance in Distributed Cloud Data Ecosystems",
          "url": "https://www.researchgate.net/publication/392917516_Advances_in_Data_Lineage_Auditing_and_Governance_in_Distributed_Cloud_Data_Ecosystems",
          "source_type": "technical_paper"
        },
        {
          "title": "Logging requirement for continuous auditing of responsible machine learning-based applications",
          "url": "https://link.springer.com/article/10.1007/s10664-025-10656-8",
          "source_type": "technical_paper"
        }
      ],
      "related_techniques": [
        "api-usage-pattern-monitoring",
        "out-of-domain-detection",
        "preferential-sampling",
        "relabelling"
      ]
    },
    {
      "slug": "model-extraction-defence-testing",
      "name": "Model Extraction Defence Testing",
      "description": "Model extraction defence testing evaluates protections against attackers who attempt to steal model functionality by querying it and training surrogate models. This technique assesses defences like query limiting, output perturbation, watermarking, and fingerprinting by simulating extraction attacks and measuring how much model functionality can be replicated. Testing evaluates both the effectiveness of defences in preventing extraction and their impact on legitimate use cases, ensuring security measures don't excessively degrade user experience.",
      "assurance_goals": [
        "Security",
        "Privacy",
        "Transparency"
      ],
      "tags": [
        "applicable-models/architecture/model-agnostic",
        "applicable-models/requirements/black-box",
        "assurance-goal-category/security",
        "assurance-goal-category/privacy",
        "assurance-goal-category/transparency",
        "data-type/any",
        "data-requirements/training-data-required",
        "lifecycle-stage/model-evaluation",
        "lifecycle-stage/post-deployment",
        "technique-type/testing",
        "evidence-type/quantitative-metric",
        "evidence-type/test-results",
        "expertise-needed/ml-engineering",
        "expertise-needed/software-engineering"
      ],
      "example_use_cases": [
        {
          "description": "Testing protections for a proprietary fraud detection API to ensure competitors cannot recreate the model's decision boundaries through systematic querying, by simulating extraction attacks using query budgets, active learning strategies, and substitute model training.",
          "goal": "Security"
        },
        {
          "description": "Evaluating whether a medical diagnosis model's query limits and output perturbations prevent extraction while protecting patient privacy embedded in the model's learned patterns.",
          "goal": "Privacy"
        },
        {
          "description": "Assessing watermarking techniques that enable model owners to prove when competitors have extracted their model, providing transparent evidence for intellectual property claims.",
          "goal": "Transparency"
        },
        {
          "description": "Evaluating whether rate limiting and output obfuscation for an automated essay grading API prevent competitors from extracting the scoring model through systematic submission of probe essays designed to reverse-engineer grading criteria.",
          "goal": "Security"
        },
        {
          "description": "Testing whether a traffic prediction API's defensive perturbations prevent extraction of the underlying routing optimization model whilst maintaining sufficient accuracy for legitimate urban planning applications.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Sophisticated attackers may use transfer learning, active learning, or knowledge distillation to extract models with 10-50x fewer queries than static defences anticipate, and can adapt their strategies as they probe defences, requiring dynamic rather than static protection mechanisms."
        },
        {
          "description": "Defensive measures like output perturbation can degrade model utility for legitimate users, creating tension between security and usability."
        },
        {
          "description": "Difficult to distinguish between legitimate high-volume use and malicious extraction attempts, potentially blocking valid users."
        },
        {
          "description": "Watermarking and fingerprinting techniques may be removed or obscured by attackers who post-process extracted models."
        },
        {
          "description": "Difficult to validate defence effectiveness without exposing the model to actual extraction attempts, and limited public benchmarks make it challenging to compare defence strategies objectively across different model types and threat scenarios."
        },
        {
          "description": "Requires specialised expertise in adversarial machine learning and attack simulation to design realistic extraction scenarios, making it challenging for organisations without dedicated security teams to implement comprehensive testing."
        }
      ],
      "resources": [
        {
          "title": "Welcome to the Adversarial Robustness Toolbox — Adversarial ...",
          "url": "https://adversarial-robustness-toolbox.readthedocs.io/",
          "source_type": "software_package"
        },
        {
          "title": "Adversarial Machine Learning: Defense Strategies",
          "url": "https://neptune.ai/blog/adversarial-machine-learning-defense-strategies",
          "source_type": "tutorial"
        },
        {
          "title": "Hypothesis Testing and Beyond: a Mini Survey on Membership Inference Attacks",
          "url": "https://www.semanticscholar.org/paper/ac4dcda6b7490d525c24d27100b7f9428ee01265",
          "source_type": "technical_paper",
          "authors": [
            "Jiajie Liu",
            "Zixuan Zhang",
            "Haonan Li",
            "Weijian Song",
            "Yiyang Lin",
            "Jinxin Zuo"
          ]
        }
      ],
      "related_techniques": [
        "adversarial-robustness-testing",
        "membership-inference-attack-testing",
        "model-watermarking-and-theft-detection",
        "adversarial-training-evaluation"
      ]
    },
    {
      "slug": "multi-agent-system-testing",
      "name": "Multi-Agent System Testing",
      "description": "Multi-agent system testing evaluates safety and reliability of systems where multiple AI agents interact, coordinate, or compete. This technique assesses emergent behaviours, communication protocols, conflict resolution, and whether agents maintain objectives appropriately. Testing produces reports on agent interaction patterns, coalition formation, failure cascade analysis, and safety property violations in multi-agent scenarios.",
      "assurance_goals": [
        "Safety",
        "Reliability",
        "Security"
      ],
      "tags": [
        "applicable-models/architecture/model-agnostic",
        "applicable-models/requirements/black-box",
        "assurance-goal-category/safety",
        "assurance-goal-category/reliability",
        "assurance-goal-category/security",
        "data-type/any",
        "lifecycle-stage/model-evaluation",
        "lifecycle-stage/post-deployment",
        "lifecycle-stage/system-deployment-and-use-monitoring",
        "technique-type/testing",
        "technique-type/analytical",
        "evidence-type/test-results",
        "evidence-type/documentation",
        "evidence-type/qualitative-assessment",
        "expertise-needed/ml-engineering",
        "expertise-needed/domain-expertise",
        "expertise-needed/software-engineering",
        "data-requirements/no-special-requirements"
      ],
      "example_use_cases": [
        {
          "description": "Testing a warehouse management system with multiple autonomous robots to ensure they coordinate safely without collisions, deadlocks, or inefficient resource contention.",
          "goal": "Safety"
        },
        {
          "description": "Verifying that a multi-agent traffic management system maintains reliable traffic flow and emergency vehicle prioritisation even when individual intersection agents face sensor failures or conflicting optimisation objectives.",
          "goal": "Reliability"
        },
        {
          "description": "Testing a collaborative diagnostic system where multiple AI agents analyze medical images, ensuring they reach reliable consensus without one dominant agent's biases propagating through the system or creating security vulnerabilities in patient data handling.",
          "goal": "Security"
        },
        {
          "description": "Evaluating a multi-agent algorithmic trading system to ensure coordinated agents don't inadvertently create market manipulation patterns or cascade failures during high-volatility conditions.",
          "goal": "Safety"
        }
      ],
      "limitations": [
        {
          "description": "Combinatorial explosion of possible agent interactions makes comprehensive testing infeasible beyond small numbers of agents."
        },
        {
          "description": "Emergent behaviors may only appear in specific scenarios that are difficult to anticipate and test systematically."
        },
        {
          "description": "Formal verification methods don't scale well to complex multi-agent systems with learning components that adapt their behavior over time, requiring hybrid approaches combining testing and monitoring."
        },
        {
          "description": "Testing environments may not capture all real-world complexities of agent deployment, communication delays, and failure modes."
        },
        {
          "description": "Simulating realistic multi-agent environments requires significant computational resources and domain-specific modeling expertise, particularly for systems with complex physical or social dynamics."
        },
        {
          "description": "Continuous monitoring in deployed systems is essential but challenging, as agents may develop new interaction patterns over time that weren't observed during initial testing phases."
        }
      ],
      "resources": [
        {
          "title": "chaosync-org/awesome-ai-agent-testing",
          "url": "https://github.com/chaosync-org/awesome-ai-agent-testing",
          "source_type": "software_package"
        },
        {
          "title": "RV4JaCa - Towards Runtime Verification of Multi-Agent Systems and Robotic Applications",
          "url": "https://www.semanticscholar.org/paper/c1091bd2ca87d3de1a8b152fb6ba0af944fcfe73",
          "source_type": "technical_paper",
          "authors": [
            "D. Engelmann",
            "Angelo Ferrando",
            "Alison R. Panisson",
            "D. Ancona",
            "Rafael Heitor Bordini",
            "V. Mascardi"
          ]
        },
        {
          "title": "A synergistic and extensible framework for multi-agent system verification",
          "url": "https://www.semanticscholar.org/paper/922ff8eb6a98b86402990c4a1df68a6a8be685c0",
          "source_type": "technical_paper",
          "authors": [
            "J. Hunter",
            "F. Raimondi",
            "Neha Rungta",
            "Richard Stocker"
          ]
        },
        {
          "title": "Distributed Control Design and Safety Verification for Multi-Agent Systems",
          "url": "https://www.semanticscholar.org/paper/da353344f00519d4ea1672da2a84154473a07647",
          "source_type": "technical_paper",
          "authors": [
            "Han Wang",
            "Antonis Papachristodoulou",
            "Kostas Margellos"
          ]
        },
        {
          "title": "Applying process mining approach to support the verification of a multi-agent system",
          "url": "https://www.semanticscholar.org/paper/de3d883dcf0a0f0d0bc3a5285484ba0f8150b8ba",
          "source_type": "technical_paper",
          "authors": [
            "C. Ou-Yang",
            "Y. Juan"
          ]
        }
      ],
      "related_techniques": [
        "ai-agent-safety-testing",
        "agent-goal-misalignment-testing",
        "prompt-injection-testing",
        "chain-of-thought-faithfulness-evaluation"
      ]
    },
    {
      "slug": "ai-agent-safety-testing",
      "name": "AI Agent Safety Testing",
      "description": "AI agent safety testing evaluates autonomous AI agents that interact with external tools, APIs, and systems to ensure they operate safely and as intended. This technique assesses whether agents correctly understand tool specifications, respect usage constraints, handle errors gracefully, avoid unintended side effects, and make safe decisions during multi-step reasoning. Testing includes boundary condition validation (extreme parameters, edge cases), permission verification (ensuring agents don't exceed authorized actions), and cascade failure analysis (how tool errors propagate through agent workflows).",
      "assurance_goals": [
        "Safety",
        "Security",
        "Reliability"
      ],
      "tags": [
        "applicable-models/architecture/neural-networks/transformer/llm",
        "applicable-models/paradigm/generative",
        "applicable-models/requirements/black-box",
        "assurance-goal-category/safety",
        "assurance-goal-category/security",
        "assurance-goal-category/reliability",
        "data-type/any",
        "data-requirements/no-special-requirements",
        "lifecycle-stage/model-evaluation",
        "lifecycle-stage/post-deployment",
        "technique-type/testing",
        "evidence-type/test-results",
        "evidence-type/quantitative-metric",
        "evidence-type/documentation",
        "expertise-needed/ml-engineering",
        "expertise-needed/software-engineering",
        "expertise-needed/domain-expertise"
      ],
      "example_use_cases": [
        {
          "description": "Testing an AI agent with database access to ensure it only executes safe read queries and cannot be manipulated into running destructive operations like deletions or schema modifications.",
          "goal": "Safety"
        },
        {
          "description": "Testing a healthcare AI agent with electronic health record access to ensure it correctly interprets permission levels, cannot be prompted to access unauthorised patient data, and maintains audit logs of all record queries.",
          "goal": "Security"
        },
        {
          "description": "Verifying that a customer service agent with CRM and payment processing tools cannot be manipulated through adversarial prompts to refund transactions outside policy boundaries or expose customer financial information.",
          "goal": "Security"
        },
        {
          "description": "Ensuring an educational AI assistant with gradebook access reliably validates student identity, cannot be socially engineered into changing grades, and handles grade calculation edge cases without data corruption.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Comprehensive testing of all possible tool interactions, parameter combinations, and prompt variations is infeasible for agents with access to many tools, requiring risk-based prioritisation of test scenarios."
        },
        {
          "description": "Agents may exhibit unexpected emergent behaviors when composing multiple tools in novel ways not anticipated during testing."
        },
        {
          "description": "Difficult to test for all possible security vulnerabilities, especially when tools themselves may have undiscovered vulnerabilities."
        },
        {
          "description": "Testing in sandboxed environments may not capture all real-world failure modes and integration issues."
        },
        {
          "description": "Requires specialised expertise in both LLM security (prompt injection, jailbreaking) and domain-specific safety considerations, which may not exist within a single team."
        },
        {
          "description": "As underlying LLMs and available tools evolve, previously safe agent behaviors may become unsafe, necessitating continuous re-evaluation rather than one-time testing."
        },
        {
          "description": "Creating realistic adversarial test cases that anticipate how malicious users might manipulate agents requires red-teaming skills and understanding of social engineering tactics."
        }
      ],
      "resources": [
        {
          "title": "Data Points: OpenAI SDK helps devs build apps in ChatGPT",
          "url": "https://charonhub.deeplearning.ai/openai-sdk-helps-devs-build-apps-in-chatgpt/",
          "source_type": "tutorial"
        },
        {
          "title": "Evaluating LLMs/Agents with MLflow | MLflow",
          "url": "https://mlflow.org/docs/latest/genai/eval-monitor/",
          "source_type": "documentation"
        },
        {
          "title": "A Developer's Guide to Building Scalable AI: Workflows vs Agents ...",
          "url": "https://towardsdatascience.com/a-developers-guide-to-building-scalable-ai-workflows-vs-agents/",
          "source_type": "tutorial"
        },
        {
          "title": "LangGraph: Build Stateful AI Agents in Python – Real Python",
          "url": "https://realpython.com/langgraph-python/",
          "source_type": "tutorial"
        },
        {
          "title": "Design, Develop, and Deploy Multi-Agent Systems with CrewAI ...",
          "url": "https://learn.deeplearning.ai/courses/design-develop-and-deploy-multi-agent-systems-with-crewai/lesson/qpa2u/what-are-ai-agents",
          "source_type": "tutorial"
        }
      ],
      "related_techniques": [
        "toxicity-and-bias-detection",
        "multi-agent-system-testing",
        "prompt-injection-testing",
        "jailbreak-resistance-testing"
      ]
    }
  ],
  "count": 9
}