{
  "tag": {
    "name": "data-requirements/training-data-required",
    "slug": "data-requirements-training-data-required",
    "count": 10,
    "category": "data-requirements"
  },
  "techniques": [
    {
      "slug": "constitutional-ai-evaluation",
      "name": "Constitutional AI Evaluation",
      "description": "Constitutional AI evaluation assesses models trained to follow explicit behavioural principles or 'constitutions' that specify desired values and constraints. This technique verifies that models consistently adhere to specified principles across diverse scenarios, measuring both principle compliance and handling of principle conflicts. Evaluation includes testing boundary cases where principles might conflict, measuring robustness to adversarial attempts to violate principles, and assessing whether models can explain their reasoning in terms of constitutional principles.",
      "assurance_goals": [
        "Safety",
        "Transparency",
        "Reliability"
      ],
      "tags": [
        "applicable-models/architecture/neural-networks/transformer/llm",
        "applicable-models/requirements/white-box",
        "applicable-models/paradigm/supervised",
        "assurance-goal-category/safety",
        "assurance-goal-category/transparency",
        "assurance-goal-category/reliability",
        "data-type/text",
        "data-requirements/labeled-data-required",
        "data-requirements/training-data-required",
        "lifecycle-stage/model-evaluation",
        "lifecycle-stage/post-deployment",
        "technique-type/testing",
        "technique-type/analytical",
        "evidence-type/quantitative-metric",
        "evidence-type/qualitative-assessment",
        "evidence-type/test-results",
        "expertise-needed/ml-engineering",
        "expertise-needed/domain-expertise"
      ],
      "example_use_cases": [
        {
          "description": "Evaluating a general-purpose AI assistant with the principle 'provide helpful information while refusing requests for illegal activities' to ensure it consistently distinguishes between legitimate chemistry education queries and attempts to obtain instructions for synthesising controlled substances or explosives.",
          "goal": "Safety"
        },
        {
          "description": "Testing whether an AI assistant can transparently explain its decisions by referencing the specific constitutional principles that guided its responses, enabling users to understand value-based reasoning.",
          "goal": "Transparency"
        },
        {
          "description": "Verifying that an AI news aggregator consistently applies stated principles about neutrality versus editorial perspective across diverse political topics and international news sources.",
          "goal": "Reliability"
        },
        {
          "description": "Evaluating a mental health support chatbot designed with principles like 'be empathetic and supportive while never providing medical diagnoses or replacing professional care' to verify it consistently maintains this boundary across varied emotional crises and user requests.",
          "goal": "Safety"
        },
        {
          "description": "Testing an AI homework assistant built on principles of 'guide learning without providing direct answers' to ensure it maintains this educational philosophy across different subjects, student ages, and question complexities without reverting to simply solving problems.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Constitutional principles may be vague or subject to interpretation, making it difficult to objectively measure compliance."
        },
        {
          "description": "Principles can conflict in complex scenarios, and evaluation must assess whether the model's priority ordering matches intended values."
        },
        {
          "description": "Models may learn to superficially cite principles in explanations without genuinely using them in decision-making processes."
        },
        {
          "description": "Creating comprehensive test sets covering all relevant principle applications and edge cases requires significant domain expertise and resources."
        },
        {
          "description": "Constitutional principles that seem clear in one cultural or linguistic context may be interpreted differently across diverse user populations, making it challenging to evaluate whether the model appropriately adapts its principle application or inappropriately varies its values."
        },
        {
          "description": "As organisational values evolve or principles are refined based on deployment experience, re-evaluation becomes necessary, but comparing results across principle versions is challenging without confounding changes in the model itself versus changes in evaluation criteria."
        }
      ],
      "resources": [
        {
          "title": "chrbradley/constitutional-reasoning-engine",
          "url": "https://github.com/chrbradley/constitutional-reasoning-engine",
          "source_type": "software_package"
        },
        {
          "title": "C3ai: Crafting and evaluating constitutions for constitutional ai",
          "url": "https://dl.acm.org/doi/abs/10.1145/3696410.3714705",
          "source_type": "technical_paper",
          "authors": [
            "Y Kyrychenko",
            "K Zhou",
            "E Bogucka"
          ]
        },
        {
          "title": "Towards Principled AI Alignment: An Evaluation and Augmentation of Inverse Constitutional AI",
          "url": "https://dash.harvard.edu/items/54a0845c-a3a3-4c73-9278-e4c4927c2e1a",
          "source_type": "technical_paper",
          "authors": [
            "E An"
          ]
        },
        {
          "title": "Constitution or Collapse? Exploring Constitutional AI with Llama 3-8B",
          "url": "https://arxiv.org/abs/2504.04918",
          "source_type": "technical_paper",
          "authors": [
            "X Zhang"
          ]
        }
      ],
      "related_techniques": [
        "ai-agent-safety-testing",
        "hallucination-detection",
        "jailbreak-resistance-testing",
        "multimodal-alignment-evaluation"
      ]
    },
    {
      "slug": "continual-learning-stability-testing",
      "name": "Continual Learning Stability Testing",
      "description": "Continual learning stability testing evaluates whether models that learn from streaming data maintain performance on previously learned tasks while acquiring new capabilities. This technique measures catastrophic forgetting (performance degradation on old tasks), forward transfer (whether old knowledge helps new learning), and backward transfer (whether new learning damages old performance). Testing includes challenging scenarios where data distributions shift significantly and evaluates whether stability techniques like experience replay or regularisation effectively preserve knowledge.",
      "assurance_goals": [
        "Reliability",
        "Safety",
        "Fairness"
      ],
      "tags": [
        "applicable-models/architecture/model-agnostic",
        "applicable-models/architecture/neural-networks",
        "applicable-models/requirements/black-box",
        "assurance-goal-category/reliability",
        "assurance-goal-category/safety",
        "assurance-goal-category/fairness",
        "data-requirements/training-data-required",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/test-results",
        "expertise-needed/ml-engineering",
        "expertise-needed/domain-expertise",
        "lifecycle-stage/model-evaluation",
        "lifecycle-stage/post-deployment",
        "lifecycle-stage/system-deployment-and-use-monitoring",
        "technique-type/testing"
      ],
      "example_use_cases": [
        {
          "description": "Testing whether a content moderation model updated with new harmful content patterns maintains reliable detection of previously learned violation types without catastrophic forgetting.",
          "goal": "Reliability"
        },
        {
          "description": "Ensuring a medical diagnosis AI that continuously learns from new clinical cases doesn't forget how to recognize previously mastered conditions, preventing safety regressions.",
          "goal": "Safety"
        },
        {
          "description": "Verifying that fairness improvements from continual learning don't introduce new biases or degrade performance for previously well-served demographic groups.",
          "goal": "Fairness"
        },
        {
          "description": "Testing whether a fraud detection system that continuously learns from new fraud patterns maintains its ability to detect previously identified fraud types, preventing financial losses from regression to older attack vectors.",
          "goal": "Reliability"
        },
        {
          "description": "Verifying that a customer service chatbot updated with new product knowledge doesn't degrade in handling established customer issues, maintaining consistent service quality across evolving capabilities.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Comprehensive testing requires maintaining evaluation datasets for all previously learned tasks, which becomes burdensome as systems learn continuously."
        },
        {
          "description": "Trade-offs between plasticity (learning new tasks well) and stability (retaining old knowledge) are fundamental and difficult to optimize simultaneously."
        },
        {
          "description": "Techniques that prevent catastrophic forgetting often require storing samples of old data, raising privacy and storage concerns."
        },
        {
          "description": "Defining acceptable forgetting levels is application-dependent and may conflict with the need to adapt to changing environments."
        },
        {
          "description": "Comprehensive stability testing requires re-running full evaluation suites after each update, creating computational costs that scale linearly with model lifespan and update frequency."
        }
      ],
      "resources": [
        {
          "title": "Metrics — Continuum 0.1.0 documentation",
          "url": "https://continuum.readthedocs.io/en/stable/_tutorials/metrics/metrics.html",
          "source_type": "tutorial"
        },
        {
          "title": "chrhenning/hypercl",
          "url": "https://github.com/chrhenning/hypercl",
          "source_type": "software_package"
        },
        {
          "title": "kjaved0/awesome-continual-learning",
          "url": "https://github.com/kjaved0/awesome-continual-learning",
          "source_type": "software_package"
        },
        {
          "title": "Continual evaluation for lifelong learning: Identifying the stability gap",
          "url": "https://arxiv.org/abs/2205.13452",
          "source_type": "technical_paper",
          "authors": [
            "M De Lange",
            "G van de Ven",
            "T Tuytelaars"
          ]
        },
        {
          "title": "Toward understanding catastrophic forgetting in continual learning",
          "url": "https://arxiv.org/abs/1908.01091",
          "source_type": "technical_paper",
          "authors": [
            "CV Nguyen",
            "A Achille",
            "M Lam",
            "T Hassner"
          ]
        }
      ],
      "related_techniques": [
        "adversarial-training-evaluation",
        "safety-envelope-testing",
        "agent-goal-misalignment-testing",
        "cross-validation"
      ]
    },
    {
      "slug": "data-poisoning-detection",
      "name": "Data Poisoning Detection",
      "description": "Data poisoning detection identifies malicious training data designed to compromise model behaviour. This technique detects various poisoning attacks including backdoor injection (triggers causing specific behaviours), availability attacks (degrading overall performance), and targeted attacks (causing errors on specific inputs). Detection methods include statistical outlier analysis, influence function analysis to identify high-impact training points, and validation-based approaches that test for suspicious model behaviours indicative of poisoning.",
      "assurance_goals": [
        "Security",
        "Reliability",
        "Safety"
      ],
      "tags": [
        "applicable-models/architecture/model-agnostic",
        "applicable-models/requirements/white-box",
        "applicable-models/requirements/gradient-access",
        "assurance-goal-category/security",
        "assurance-goal-category/reliability",
        "assurance-goal-category/safety",
        "data-requirements/training-data-required",
        "data-type/any",
        "lifecycle-stage/data-collection",
        "lifecycle-stage/model-development",
        "lifecycle-stage/model-evaluation",
        "technique-type/analytical",
        "technique-type/testing",
        "evidence-type/quantitative-metric",
        "evidence-type/test-results",
        "expertise-needed/ml-engineering",
        "expertise-needed/domain-expertise"
      ],
      "example_use_cases": [
        {
          "description": "Scanning training data for an autonomous driving system to detect images that might contain backdoor triggers designed to cause unsafe behavior when specific objects appear in the environment.",
          "goal": "Safety"
        },
        {
          "description": "Protecting a federated learning system for hospital patient diagnosis models from malicious participants who might inject poisoned medical records designed to create backdoors that misclassify specific patient profiles or degrade overall diagnostic reliability.",
          "goal": "Security"
        },
        {
          "description": "Ensuring a content moderation model hasn't been poisoned with data designed to make it fail on specific types of harmful content, maintaining reliable safety filtering.",
          "goal": "Reliability"
        },
        {
          "description": "Detecting poisoned training data in recidivism prediction models used for sentencing recommendations, where malicious actors might inject manipulated records to systematically bias predictions for specific demographic groups.",
          "goal": "Safety"
        },
        {
          "description": "Scanning training data for algorithmic trading models to identify poisoned market data designed to create exploitable patterns, ensuring reliable trading decisions and preventing market manipulation vulnerabilities.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Sophisticated poisoning attacks can be designed to evade statistical detection by mimicking benign data distributions closely."
        },
        {
          "description": "High false positive rates may lead to incorrectly filtering legitimate but unusual training examples, reducing training data quality and diversity."
        },
        {
          "description": "Computationally expensive to apply influence function analysis or deep inspection to very large training datasets, potentially requiring hours to days for thorough analysis of datasets with millions of samples, particularly when using gradient-based detection methods."
        },
        {
          "description": "Difficult to detect poisoning in scenarios where attackers have knowledge of detection methods and can adapt attacks accordingly."
        },
        {
          "description": "Establishing ground truth for what constitutes 'poisoned' versus legitimate but unusual data is challenging, particularly when dealing with naturally occurring outliers or edge cases in the data distribution."
        }
      ],
      "resources": [
        {
          "title": "art.defences.detector.poison — Adversarial Robustness Toolbox ...",
          "url": "https://adversarial-robustness-toolbox.readthedocs.io/en/latest/modules/defences/detector_poisoning.html",
          "source_type": "documentation"
        },
        {
          "title": "Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT Systems using Large Language Models",
          "url": "http://arxiv.org/pdf/2511.02894v1",
          "source_type": "technical_paper",
          "authors": [
            "W. K. M Mithsara",
            "Ning Yang",
            "Ahmed Imteaj",
            "Hussein Zangoti",
            "Abdur R. Shahid"
          ],
          "publication_date": "2025-11-04"
        },
        {
          "title": "SAFELOC: Overcoming Data Poisoning Attacks in Heterogeneous Federated Machine Learning for Indoor Localization",
          "url": "http://arxiv.org/pdf/2411.09055v1",
          "source_type": "technical_paper",
          "authors": [
            "Akhil Singampalli",
            "Danish Gufran",
            "Sudeep Pasricha"
          ],
          "publication_date": "2024-11-13"
        },
        {
          "title": "Introduction — trojai 0.2.22 documentation",
          "url": "https://trojai.readthedocs.io/en/latest/intro.html",
          "source_type": "documentation"
        },
        {
          "title": "Understanding Influence Functions and Datamodels via Harmonic Analysis",
          "url": "http://arxiv.org/pdf/2210.01072v1",
          "source_type": "technical_paper",
          "authors": [
            "Nikunj Saunshi",
            "Arushi Gupta",
            "Mark Braverman",
            "Sanjeev Arora"
          ],
          "publication_date": "2022-10-03"
        }
      ],
      "related_techniques": [
        "anomaly-detection",
        "adversarial-robustness-testing",
        "influence-functions",
        "cross-validation"
      ]
    },
    {
      "slug": "embedding-bias-analysis",
      "name": "Embedding Bias Analysis",
      "description": "Embedding bias analysis examines learned representations to identify biases, spurious correlations, and problematic clustering patterns in vector embeddings. This technique uses dimensionality reduction, clustering analysis, and geometric fairness metrics to examine whether embeddings encode sensitive attributes, place certain groups in disadvantaged regions of representation space, or learn stereotypical associations. Analysis reveals whether embeddings used for retrieval, recommendation, or downstream tasks contain fairness issues that will propagate to applications.",
      "assurance_goals": [
        "Fairness",
        "Explainability",
        "Transparency"
      ],
      "tags": [
        "applicable-models/architecture/neural-networks",
        "applicable-models/architecture/neural-networks/transformer",
        "applicable-models/architecture/neural-networks/transformer/llm",
        "applicable-models/architecture/neural-networks/convolutional",
        "applicable-models/requirements/model-internals",
        "assurance-goal-category/fairness",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/explains/internal-mechanisms",
        "assurance-goal-category/explainability/visualization-methods/activation-maps",
        "assurance-goal-category/transparency",
        "data-type/text",
        "data-type/image",
        "data-requirements/training-data-required",
        "lifecycle-stage/model-evaluation",
        "lifecycle-stage/post-deployment",
        "technique-type/analytical",
        "technique-type/testing",
        "evidence-type/quantitative-metric",
        "evidence-type/visual-artifact",
        "evidence-type/qualitative-assessment",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistical-knowledge",
        "explanatory-scope/global",
        "fairness-approach/fairness-metrics"
      ],
      "example_use_cases": [
        {
          "description": "Auditing word embeddings for biased gender associations like 'doctor' being closer to 'male' than 'female', identifying problematic stereotypes that could affect downstream NLP applications.",
          "goal": "Fairness"
        },
        {
          "description": "Analyzing face recognition embeddings to understand whether certain demographic groups are clustered more tightly (enabling easier discrimination) or more dispersed (enabling easier misidentification).",
          "goal": "Explainability"
        },
        {
          "description": "Transparently documenting embedding space properties including which attributes are encoded and what associations exist, enabling informed decisions about appropriate use cases.",
          "goal": "Transparency"
        },
        {
          "description": "Analyzing clinical note embeddings in a medical diagnosis support system to detect whether disease associations cluster along demographic lines (e.g., certain conditions being systematically closer to particular age or ethnic groups in the embedding space), which could lead to biased treatment recommendations.",
          "goal": "Fairness"
        },
        {
          "description": "Examining embeddings in a legal document analysis system to identify whether risk assessment features encode problematic associations between demographic attributes and recidivism concepts, revealing bias that could affect sentencing or parole decisions.",
          "goal": "Explainability"
        },
        {
          "description": "Auditing loan application embeddings to detect whether protected characteristics (ethnicity, gender, age) cluster near creditworthiness concepts in ways that could enable indirect discrimination in lending decisions.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Defining ground truth for 'fair' or 'unbiased' embedding geometries is challenging and may depend on application-specific requirements."
        },
        {
          "description": "Debiasing embeddings can reduce performance on downstream tasks or remove useful information along with unwanted biases."
        },
        {
          "description": "High-dimensional embedding spaces are difficult to visualize and interpret comprehensively, potentially missing subtle bias patterns."
        },
        {
          "description": "Embeddings may encode bias in complex, non-linear ways that simple geometric metrics fail to capture."
        },
        {
          "description": "Requires access to model internals and training data to extract and analyze embeddings, making this technique infeasible for black-box commercial models or systems where proprietary data cannot be shared."
        },
        {
          "description": "Analyzing large-scale embedding spaces can be computationally expensive and requires sophisticated understanding of both vector space geometry and the specific domain to interpret results meaningfully."
        },
        {
          "description": "Effective bias analysis requires representative test datasets covering relevant demographic groups and attributes, which may be difficult to obtain for sensitive domains or protected characteristics."
        }
      ],
      "resources": [
        {
          "title": "lmcinnes/umap",
          "url": "https://github.com/lmcinnes/umap",
          "source_type": "software_package"
        },
        {
          "title": "harveyslash/TSNE-UMAP-Embedding-Visualisation",
          "url": "https://github.com/harveyslash/TSNE-UMAP-Embedding-Visualisation",
          "source_type": "software_package"
        },
        {
          "title": "A new embedding quality assessment method for manifold learning",
          "url": "https://www.sciencedirect.com/science/article/pii/S092523121200389X",
          "source_type": "technical_paper"
        },
        {
          "title": "Unsupervised embedding quality evaluation",
          "url": "https://proceedings.mlr.press/v221/tsitsulin23a.html",
          "source_type": "technical_paper"
        },
        {
          "title": "Semantic Certainty Assessment in Vector Retrieval Systems: A Novel Framework for Embedding Quality Evaluation",
          "url": "https://arxiv.org/abs/2507.05933",
          "source_type": "technical_paper"
        }
      ],
      "related_techniques": [
        "attention-visualisation-in-transformers",
        "concept-activation-vectors",
        "chain-of-thought-faithfulness-evaluation",
        "few-shot-fairness-evaluation"
      ]
    },
    {
      "slug": "hallucination-detection",
      "name": "Hallucination Detection",
      "description": "Hallucination detection identifies when generative models produce factually incorrect, fabricated, or ungrounded outputs. This technique combines automated consistency checking, self-consistency methods, uncertainty quantification, and human evaluation. It produces detection scores distinguishing intrinsic hallucinations (contradicting sources) from extrinsic hallucinations (unverifiable claims), enabling filtering or user warnings.",
      "assurance_goals": [
        "Reliability",
        "Transparency",
        "Safety"
      ],
      "tags": [
        "applicable-models/architecture/neural-networks/transformer",
        "applicable-models/architecture/neural-networks/transformer/llm",
        "applicable-models/paradigm/generative",
        "applicable-models/requirements/black-box",
        "assurance-goal-category/reliability",
        "assurance-goal-category/transparency",
        "assurance-goal-category/safety",
        "data-type/text",
        "data-requirements/training-data-required",
        "evidence-type/quantitative-metric",
        "evidence-type/test-results",
        "evidence-type/qualitative-assessment",
        "expertise-needed/ml-engineering",
        "expertise-needed/domain-expertise",
        "lifecycle-stage/model-evaluation",
        "lifecycle-stage/post-deployment",
        "technique-type/testing",
        "technique-type/analytical"
      ],
      "example_use_cases": [
        {
          "description": "Monitoring a medical information system to detect when it generates unsupported clinical claims or fabricates research citations, preventing patients from receiving incorrect health advice.",
          "goal": "Safety"
        },
        {
          "description": "Evaluating an AI journalism assistant to ensure generated article drafts don't fabricate quotes, misattribute sources, or create false claims that could damage credibility and public trust.",
          "goal": "Reliability"
        },
        {
          "description": "Implementing confidence scoring and uncertainty indicators in AI assistants that transparently signal when responses may contain hallucinated information versus verified facts.",
          "goal": "Transparency"
        },
        {
          "description": "Validating an AI legal research tool used by public defenders to ensure generated case law summaries don't fabricate judicial opinions, misstate holdings, or invent precedents that could undermine legal arguments and defendants' rights.",
          "goal": "Reliability"
        },
        {
          "description": "Monitoring an AI-powered financial reporting assistant to detect when it generates unsubstantiated market analysis, fabricates company earnings data, or creates false attributions to analysts, protecting investors from misleading information.",
          "goal": "Safety"
        }
      ],
      "limitations": [
        {
          "description": "Automated detection methods may miss subtle hallucinations or flag correct but unusual information as potentially fabricated."
        },
        {
          "description": "Requires access to reliable ground truth knowledge sources for fact-checking, which may not exist for many domains or recent events."
        },
        {
          "description": "Self-consistency methods assume inconsistency indicates hallucination, but models can consistently hallucinate the same false information."
        },
        {
          "description": "Human evaluation is expensive and subjective, with annotators potentially disagreeing about what constitutes hallucination versus interpretation."
        },
        {
          "description": "Detection effectiveness degrades for rapidly evolving domains or emerging topics where ground truth knowledge bases may be outdated, incomplete, or unavailable, making it difficult to distinguish hallucinations from genuinely novel or recent information."
        },
        {
          "description": "Particularly challenging to apply in creative writing, storytelling, or subjective analysis contexts where the boundary between acceptable creative license and problematic hallucination is domain-dependent and context-specific."
        }
      ],
      "resources": [
        {
          "title": "vectara/hallucination-leaderboard",
          "url": "https://github.com/vectara/hallucination-leaderboard",
          "source_type": "software_package"
        },
        {
          "title": "How to Perform Hallucination Detection for LLMs | Towards Data ...",
          "url": "https://towardsdatascience.com/how-to-perform-hallucination-detection-for-llms-b8cb8b72e697/",
          "source_type": "tutorial"
        },
        {
          "title": "SelfCheckAgent: Zero-Resource Hallucination Detection in Generative Large Language Models",
          "url": "https://www.semanticscholar.org/paper/e7cd95ec57302fc5897bed07bee87a388747f750",
          "source_type": "technical_paper",
          "authors": [
            "Diyana Muhammed",
            "Gollam Rabby",
            "Soren Auer"
          ]
        },
        {
          "title": "SINdex: Semantic INconsistency Index for Hallucination Detection in LLMs",
          "url": "https://www.semanticscholar.org/paper/06ef8d4343f35184b4dec004365dfe1a562e08fc",
          "source_type": "technical_paper",
          "authors": [
            "Samir Abdaljalil",
            "H. Kurban",
            "Parichit Sharma",
            "E. Serpedin",
            "Rachad Atat"
          ]
        },
        {
          "title": "Uncertainty Quantification for Hallucination Detection in Large Language Models: Foundations, Methodology, and Future Directions",
          "url": "https://www.semanticscholar.org/paper/76912e6ea42bdebb2795708dac381a9b268b391c",
          "source_type": "technical_paper",
          "authors": [
            "Sungmin Kang",
            "Y. Bakman",
            "D. Yaldiz",
            "Baturalp Buyukates",
            "S. Avestimehr"
          ]
        }
      ],
      "related_techniques": [
        "retrieval-augmented-generation-evaluation",
        "chain-of-thought-faithfulness-evaluation",
        "epistemic-uncertainty-quantification",
        "confidence-thresholding"
      ]
    },
    {
      "slug": "machine-unlearning",
      "name": "Machine Unlearning",
      "description": "Machine unlearning enables removal of specific training data's influence from trained models without complete retraining. This technique addresses privacy rights like GDPR's right to be forgotten by selectively erasing learned patterns associated with particular data points, individuals, or sensitive attributes. Methods include exact unlearning (provably equivalent to retraining without the data), approximate unlearning (efficient algorithms that closely approximate retraining), and certified unlearning (providing formal guarantees about information removal).",
      "assurance_goals": [
        "Privacy",
        "Fairness",
        "Transparency"
      ],
      "tags": [
        "applicable-models/architecture/model-agnostic",
        "applicable-models/architecture/neural-networks",
        "applicable-models/requirements/white-box",
        "applicable-models/requirements/model-internals",
        "assurance-goal-category/privacy",
        "assurance-goal-category/fairness",
        "assurance-goal-category/transparency",
        "data-requirements/training-data-required",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/documentation",
        "evidence-type/test-results",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistical-knowledge",
        "lifecycle-stage/model-development",
        "lifecycle-stage/post-deployment",
        "technique-type/algorithmic"
      ],
      "example_use_cases": [
        {
          "description": "Responding to user deletion requests in a social media recommendation system by removing all influence of that user's historical interactions, ensuring GDPR compliance and verifiable data removal.",
          "goal": "Privacy"
        },
        {
          "description": "Removing specific patient records from a hospital's diagnostic model after a patient withdraws consent, ensuring the model no longer reflects patterns from that individual's medical history while maintaining clinical accuracy for other patients.",
          "goal": "Fairness"
        },
        {
          "description": "Enabling a financial institution to demonstrate regulatory compliance by providing cryptographic proof that a former customer's transaction history has been completely removed from credit risk assessment models following account closure.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Exact unlearning for complex models like deep neural networks is computationally expensive, often nearly as costly as full retraining."
        },
        {
          "description": "Approximate unlearning methods may not provide strong guarantees that information has been fully removed, potentially leaving residual influence."
        },
        {
          "description": "Difficult to verify unlearning effectiveness, as adversaries might extract information about supposedly removed data through membership inference or other attacks."
        },
        {
          "description": "Repeated unlearning requests can degrade model performance significantly, especially if many data points are removed from the training distribution."
        },
        {
          "description": "Requires access to model architecture and training process details (white-box access), making it difficult to apply to third-party models or models where internal structure is proprietary."
        },
        {
          "description": "Particularly challenging for very large foundation models where even storing checkpoints for potential retraining is infeasible, limiting practical applicability to smaller, domain-specific models."
        }
      ],
      "resources": [
        {
          "title": "tamlhp/awesome-machine-unlearning",
          "url": "https://github.com/tamlhp/awesome-machine-unlearning",
          "source_type": "software_package"
        },
        {
          "title": "jjbrophy47/machine_unlearning",
          "url": "https://github.com/jjbrophy47/machine_unlearning",
          "source_type": "software_package"
        },
        {
          "title": "Right to be forgotten in the era of large language models",
          "url": "https://link.springer.com/article/10.1007/s43681-024-00573-9",
          "source_type": "technical_paper"
        },
        {
          "title": "Digital forgetting in large language models: A survey of unlearning methods",
          "url": "https://link.springer.com/article/10.1007/s10462-024-11078-6",
          "source_type": "technical_paper"
        },
        {
          "title": "Machine unlearning for traditional models and large language models",
          "url": "https://arxiv.org/abs/2404.01206",
          "source_type": "technical_paper"
        }
      ],
      "related_techniques": [
        "federated-learning",
        "homomorphic-encryption",
        "membership-inference-attack-testing",
        "influence-functions"
      ]
    },
    {
      "slug": "model-extraction-defence-testing",
      "name": "Model Extraction Defence Testing",
      "description": "Model extraction defence testing evaluates protections against attackers who attempt to steal model functionality by querying it and training surrogate models. This technique assesses defences like query limiting, output perturbation, watermarking, and fingerprinting by simulating extraction attacks and measuring how much model functionality can be replicated. Testing evaluates both the effectiveness of defences in preventing extraction and their impact on legitimate use cases, ensuring security measures don't excessively degrade user experience.",
      "assurance_goals": [
        "Security",
        "Privacy",
        "Transparency"
      ],
      "tags": [
        "applicable-models/architecture/model-agnostic",
        "applicable-models/requirements/black-box",
        "assurance-goal-category/security",
        "assurance-goal-category/privacy",
        "assurance-goal-category/transparency",
        "data-type/any",
        "data-requirements/training-data-required",
        "lifecycle-stage/model-evaluation",
        "lifecycle-stage/post-deployment",
        "technique-type/testing",
        "evidence-type/quantitative-metric",
        "evidence-type/test-results",
        "expertise-needed/ml-engineering",
        "expertise-needed/software-engineering"
      ],
      "example_use_cases": [
        {
          "description": "Testing protections for a proprietary fraud detection API to ensure competitors cannot recreate the model's decision boundaries through systematic querying, by simulating extraction attacks using query budgets, active learning strategies, and substitute model training.",
          "goal": "Security"
        },
        {
          "description": "Evaluating whether a medical diagnosis model's query limits and output perturbations prevent extraction while protecting patient privacy embedded in the model's learned patterns.",
          "goal": "Privacy"
        },
        {
          "description": "Assessing watermarking techniques that enable model owners to prove when competitors have extracted their model, providing transparent evidence for intellectual property claims.",
          "goal": "Transparency"
        },
        {
          "description": "Evaluating whether rate limiting and output obfuscation for an automated essay grading API prevent competitors from extracting the scoring model through systematic submission of probe essays designed to reverse-engineer grading criteria.",
          "goal": "Security"
        },
        {
          "description": "Testing whether a traffic prediction API's defensive perturbations prevent extraction of the underlying routing optimization model whilst maintaining sufficient accuracy for legitimate urban planning applications.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Sophisticated attackers may use transfer learning, active learning, or knowledge distillation to extract models with 10-50x fewer queries than static defences anticipate, and can adapt their strategies as they probe defences, requiring dynamic rather than static protection mechanisms."
        },
        {
          "description": "Defensive measures like output perturbation can degrade model utility for legitimate users, creating tension between security and usability."
        },
        {
          "description": "Difficult to distinguish between legitimate high-volume use and malicious extraction attempts, potentially blocking valid users."
        },
        {
          "description": "Watermarking and fingerprinting techniques may be removed or obscured by attackers who post-process extracted models."
        },
        {
          "description": "Difficult to validate defence effectiveness without exposing the model to actual extraction attempts, and limited public benchmarks make it challenging to compare defence strategies objectively across different model types and threat scenarios."
        },
        {
          "description": "Requires specialised expertise in adversarial machine learning and attack simulation to design realistic extraction scenarios, making it challenging for organisations without dedicated security teams to implement comprehensive testing."
        }
      ],
      "resources": [
        {
          "title": "Welcome to the Adversarial Robustness Toolbox — Adversarial ...",
          "url": "https://adversarial-robustness-toolbox.readthedocs.io/",
          "source_type": "software_package"
        },
        {
          "title": "Adversarial Machine Learning: Defense Strategies",
          "url": "https://neptune.ai/blog/adversarial-machine-learning-defense-strategies",
          "source_type": "tutorial"
        },
        {
          "title": "Hypothesis Testing and Beyond: a Mini Survey on Membership Inference Attacks",
          "url": "https://www.semanticscholar.org/paper/ac4dcda6b7490d525c24d27100b7f9428ee01265",
          "source_type": "technical_paper",
          "authors": [
            "Jiajie Liu",
            "Zixuan Zhang",
            "Haonan Li",
            "Weijian Song",
            "Yiyang Lin",
            "Jinxin Zuo"
          ]
        }
      ],
      "related_techniques": [
        "adversarial-robustness-testing",
        "membership-inference-attack-testing",
        "model-watermarking-and-theft-detection",
        "adversarial-training-evaluation"
      ]
    },
    {
      "slug": "membership-inference-attack-testing",
      "name": "Membership Inference Attack Testing",
      "description": "Membership inference attack testing evaluates whether adversaries can determine if specific data points were included in a model's training set. This technique simulates attacks where adversaries use model confidence scores, prediction patterns, or loss values to distinguish training data from non-training data. Testing measures privacy leakage by calculating attack success rates, precision-recall trade-offs, and advantage over random guessing. Results inform decisions about privacy-enhancing techniques like differential privacy or regularisation.",
      "assurance_goals": [
        "Privacy",
        "Security",
        "Transparency"
      ],
      "tags": [
        "applicable-models/architecture/model-agnostic",
        "applicable-models/requirements/black-box",
        "assurance-goal-category/privacy",
        "assurance-goal-category/security",
        "assurance-goal-category/transparency",
        "data-requirements/training-data-required",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/test-results",
        "expertise-needed/ml-engineering",
        "expertise-needed/security-expertise",
        "lifecycle-stage/model-evaluation",
        "lifecycle-stage/post-deployment",
        "technique-type/testing",
        "technique-type/analytical"
      ],
      "example_use_cases": [
        {
          "description": "Testing a genomics research model to ensure attackers cannot determine which individuals' genetic data were used in training, protecting highly sensitive hereditary and health information from privacy breaches.",
          "goal": "Privacy"
        },
        {
          "description": "Evaluating whether a facial recognition system leaks information about whose faces were in the training set, preventing unauthorized identification of individuals in training data.",
          "goal": "Security"
        },
        {
          "description": "Auditing a credit scoring model used by multiple lenders to verify and transparently document that the model doesn't leak information about which specific customers' financial histories were used in training, supporting fair lending compliance reporting.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Attack success rates vary significantly depending on model architecture, training procedures, and data characteristics, making it difficult to establish universal thresholds for acceptable privacy."
        },
        {
          "description": "Sophisticated attackers with shadow models or auxiliary data may achieve attack success rates 2-3x higher than standard evaluation scenarios test."
        },
        {
          "description": "Trade-off between model utility and privacy protection means defending against membership inference often reduces model accuracy."
        },
        {
          "description": "Testing requires access to both training and non-training data from the same distribution, which may not always be available for realistic evaluation."
        }
      ],
      "resources": [
        {
          "title": "SoK: Comparing Different Membership Inference Attacks with a Comprehensive Benchmark",
          "url": "https://www.semanticscholar.org/paper/e5604a82a8a1deb71c46bc8fa4a54e742dc98305",
          "source_type": "technical_paper",
          "authors": [
            "Jun Niu",
            "Xiaoyan Zhu",
            "Moxuan Zeng",
            "Ge-ming Zhang",
            "Qingyang Zhao",
            "Chu-Chun Huang",
            "Yang Zhang",
            "Suyu An",
            "Yangzhong Wang",
            "Xinghui Yue",
            "Zhipeng He",
            "Weihao Guo",
            "Kuo Shen",
            "Peng Liu",
            "Yulong Shen",
            "Xiaohong Jiang",
            "Jianfeng Ma",
            "Yuqing Zhang"
          ]
        },
        {
          "title": "SoK: Membership Inference Attacks on LLMs are Rushing Nowhere (and How to Fix It)",
          "url": "https://www.semanticscholar.org/paper/c438dcb2b3d7e1a9cee8d9b1035b96c39b53941c",
          "source_type": "technical_paper",
          "authors": [
            "Matthieu Meeus",
            "Igor Shilov",
            "Shubham Jain",
            "Manuel Faysse",
            "Marek Rei",
            "Y. Montjoye"
          ]
        },
        {
          "title": "art.attacks.inference.membership_inference — Adversarial ...",
          "url": "https://adversarial-robustness-toolbox.readthedocs.io/en/latest/modules/attacks/inference/membership_inference.html",
          "source_type": "documentation"
        },
        {
          "title": "Library of Attacks — tapas 0.1 documentation",
          "url": "https://tapas-privacy.readthedocs.io/en/latest/library-of-attacks.html",
          "source_type": "documentation"
        },
        {
          "title": "Intro to Federated Learning - DeepLearning.AI",
          "url": "https://learn.deeplearning.ai/courses/intro-to-federated-learning/lesson/go7bi/data-privacy",
          "source_type": "tutorial"
        }
      ],
      "related_techniques": [
        "model-extraction-defence-testing",
        "adversarial-training-evaluation",
        "synthetic-data-evaluation",
        "adversarial-robustness-testing"
      ]
    },
    {
      "slug": "out-of-domain-detection",
      "name": "Out-of-Domain Detection",
      "description": "Out-of-domain (OOD) detection identifies user inputs that fall outside an AI system's intended domain or capabilities, enabling graceful handling rather than generating unreliable responses. This technique trains classifiers or uses embedding-based methods to recognise when queries require knowledge or capabilities the system doesn't possess. OOD detection enables systems to explicitly decline, redirect, or request clarification rather than hallucinating answers or applying inappropriate reasoning to unfamiliar domains.",
      "assurance_goals": [
        "Reliability",
        "Safety",
        "Transparency"
      ],
      "tags": [
        "applicable-models/architecture/model-agnostic",
        "applicable-models/architecture/neural-networks/transformer/llm",
        "applicable-models/paradigm/supervised",
        "applicable-models/requirements/black-box",
        "assurance-goal-category/reliability",
        "assurance-goal-category/safety",
        "assurance-goal-category/transparency",
        "data-requirements/training-data-required",
        "data-requirements/labeled-data-required",
        "data-type/text",
        "evidence-type/quantitative-metric",
        "evidence-type/test-results",
        "expertise-needed/ml-engineering",
        "expertise-needed/domain-expertise",
        "lifecycle-stage/model-development",
        "lifecycle-stage/deployment",
        "lifecycle-stage/post-deployment",
        "technique-type/algorithmic",
        "technique-type/procedural"
      ],
      "example_use_cases": [
        {
          "description": "Enabling a medical chatbot specialized in diabetes management to recognize and deflect questions about unrelated conditions, avoiding potentially dangerous medical advice outside its training domain.",
          "goal": "Safety"
        },
        {
          "description": "Ensuring an educational AI tutor for high school mathematics reliably identifies advanced university-level questions and redirects students to appropriate resources rather than attempting explanations beyond its scope.",
          "goal": "Reliability"
        },
        {
          "description": "Transparently communicating system limitations by explicitly informing users when queries exceed the AI's scope rather than silently providing low-quality responses.",
          "goal": "Transparency"
        },
        {
          "description": "Protecting an insurance claims processing system trained on standard claims from making unreliable decisions on unusual or complex cases (e.g., natural disasters, emerging fraud patterns) by detecting and routing them to specialist adjusters.",
          "goal": "Reliability"
        },
        {
          "description": "Ensuring an autonomous vehicle's perception system detects when it encounters road conditions, weather patterns, or infrastructure types outside its training distribution (e.g., unmapped construction zones, unusual signage), triggering increased caution or human intervention.",
          "goal": "Safety"
        }
      ],
      "limitations": [
        {
          "description": "Defining precise scope boundaries can be challenging, especially for general-purpose systems or domains with fuzzy edges."
        },
        {
          "description": "May produce false positives that reject legitimate queries at the boundary of the system's capabilities, degrading user experience."
        },
        {
          "description": "Users may rephrase out-of-scope queries to bypass detection, requiring robust handling of paraphrases and edge cases."
        },
        {
          "description": "Difficult to maintain accurate scope detection as systems are updated and capabilities expand or shift over time."
        },
        {
          "description": "Establishing reliable domain boundaries requires substantial labeled data from both in-domain and out-of-domain examples, which can be expensive to collect and annotate systematically."
        },
        {
          "description": "Real-time OOD detection adds inference latency (typically 10-50ms per query depending on method), which may impact user experience in time-sensitive applications."
        }
      ],
      "resources": [
        {
          "title": "silverriver/OOD4NLU",
          "url": "https://github.com/silverriver/OOD4NLU",
          "source_type": "software_package"
        },
        {
          "title": "rivercold/BERT-unsupervised-OOD",
          "url": "https://github.com/rivercold/BERT-unsupervised-OOD",
          "source_type": "software_package"
        },
        {
          "title": "SLAD-ml/few-shot-ood",
          "url": "https://github.com/SLAD-ml/few-shot-ood",
          "source_type": "software_package"
        },
        {
          "title": "pris-nlp/Generative_distance-based_OOD",
          "url": "https://github.com/pris-nlp/Generative_distance-based_OOD",
          "source_type": "software_package"
        },
        {
          "title": "A Unified Survey on Anomaly, Novelty, Open-Set, and Out-of-Distribution Detection: Solutions and Future Challenges",
          "url": "https://www.semanticscholar.org/paper/8b153cc2c7f5ea9f307f12ea945a5e9196ee5c52",
          "source_type": "technical_paper",
          "authors": [
            "Mohammadreza Salehi",
            "Hossein Mirzaei",
            "Dan Hendrycks",
            "Yixuan Li",
            "M. H. Rohban",
            "M. Sabokrou"
          ]
        }
      ],
      "related_techniques": [
        "deep-ensembles",
        "hallucination-detection",
        "anomaly-detection",
        "conformal-prediction"
      ]
    },
    {
      "slug": "synthetic-data-evaluation",
      "name": "Synthetic Data Evaluation",
      "description": "Synthetic data evaluation assesses whether synthetic datasets protect individual privacy while maintaining statistical utility and fidelity to real data. This technique evaluates three key dimensions: privacy (through disclosure risk metrics and re-identification attack success rates), utility (by comparing statistical properties and model performance), and fidelity (measuring distributional similarity to real data). It produces evaluation reports quantifying the privacy-utility-fidelity trade-offs.",
      "assurance_goals": [
        "Privacy",
        "Transparency",
        "Reliability"
      ],
      "tags": [
        "applicable-models/architecture/model-agnostic",
        "applicable-models/requirements/black-box",
        "assurance-goal-category/privacy",
        "assurance-goal-category/transparency",
        "assurance-goal-category/reliability",
        "data-requirements/training-data-required",
        "data-type/tabular",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/documentation",
        "evidence-type/test-results",
        "expertise-needed/statistical-knowledge",
        "expertise-needed/domain-expertise",
        "expertise-needed/ml-engineering",
        "lifecycle-stage/data-collection",
        "lifecycle-stage/model-evaluation",
        "technique-type/analytical",
        "technique-type/testing"
      ],
      "example_use_cases": [
        {
          "description": "Validating synthetic patient data generated for medical research to ensure individual patients cannot be re-identified while maintaining statistical relationships needed for valid clinical studies.",
          "goal": "Privacy"
        },
        {
          "description": "Validating that machine learning models for predicting student outcomes trained on synthetic educational data maintain reliable performance comparable to models trained on real student records, while enabling researchers to share datasets without FERPA violations.",
          "goal": "Reliability"
        },
        {
          "description": "Ensuring fraud detection models trained on synthetic credit card transactions maintain reliable performance comparable to models trained on sensitive real transaction data.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Trade-off between privacy and utility means strong privacy guarantees often significantly degrade data quality and analytical value."
        },
        {
          "description": "Difficult to validate that synthetic data protects against all possible privacy attacks, especially sophisticated adversaries with auxiliary information."
        },
        {
          "description": "Utility metrics may not capture subtle distributional differences that matter for specific downstream tasks or edge case analyses."
        },
        {
          "description": "Synthetic data may introduce artificial patterns or miss rare but important real-world phenomena, limiting use for certain applications."
        },
        {
          "description": "Requires significant domain expertise to properly validate fidelity and utility for specific use cases, as generic statistical metrics may not capture domain-specific requirements or failure modes."
        },
        {
          "description": "Synthetic data may not preserve fairness properties or bias patterns from original data in predictable ways, requiring careful fairness testing when synthetic data is used to train decision-making models."
        }
      ],
      "resources": [
        {
          "title": "SCU-TrustworthyAI/SynEval",
          "url": "https://github.com/SCU-TrustworthyAI/SynEval",
          "source_type": "software_package"
        },
        {
          "title": "Evaluating synthetic data | Towards Data Science",
          "url": "https://towardsdatascience.com/evaluating-synthetic-data-c5833f6b2f15/",
          "source_type": "tutorial"
        },
        {
          "title": "Privacy Mechanisms and Evaluation Metrics for Synthetic Data Generation: A Systematic Review",
          "url": "https://www.semanticscholar.org/paper/e76d1dde9340ed1bfef28808297df51791ce4506",
          "source_type": "technical_paper",
          "authors": [
            "Pablo A. Osorio-Marulanda",
            "Gorka Epelde",
            "Mikel Hernandez",
            "Imanol Isasa",
            "Nicolas Moreno Reyes",
            "A. B. Iraola"
          ]
        },
        {
          "title": "Can We Trust Synthetic Data in Medicine? A Scoping Review of Privacy and Utility Metrics",
          "url": "https://www.semanticscholar.org/paper/5e1eb0df4db1316cff416aee9e7676517778a780",
          "source_type": "technical_paper",
          "authors": [
            "B. Kaabachi",
            "J. Despraz",
            "T. Meurers",
            "K. Otte",
            "M. Halilovic",
            "F. Prasser",
            "J. Raisaro"
          ]
        },
        {
          "title": "Welcome to TAPAS's documentation! — tapas 0.1 documentation",
          "url": "https://tapas-privacy.readthedocs.io/",
          "source_type": "documentation"
        }
      ],
      "related_techniques": [
        "membership-inference-attack-testing",
        "synthetic-data-generation",
        "differential-privacy",
        "machine-unlearning"
      ]
    }
  ],
  "count": 10
}