{
  "tag": {
    "name": "data-requirements/labeled-data-required",
    "slug": "data-requirements-labeled-data-required",
    "count": 9,
    "category": "data-requirements"
  },
  "techniques": [
    {
      "slug": "adversarial-training-evaluation",
      "name": "Adversarial Training Evaluation",
      "description": "Adversarial training evaluation assesses whether models trained with adversarial examples have genuinely improved robustness rather than merely overfitting to specific attack methods. This technique tests robustness against diverse attack algorithms including those not used during training, measures certified robustness bounds, and evaluates whether adversarial training creates exploitable trade-offs in clean accuracy or introduces new vulnerabilities. Evaluation ensures adversarial training provides genuine security benefits rather than superficial improvements.",
      "assurance_goals": [
        "Security",
        "Reliability",
        "Transparency"
      ],
      "tags": [
        "applicable-models/architecture/model-agnostic",
        "applicable-models/architecture/neural-networks",
        "applicable-models/requirements/black-box",
        "assurance-goal-category/security",
        "assurance-goal-category/reliability",
        "assurance-goal-category/transparency",
        "data-type/any",
        "data-requirements/labeled-data-required",
        "lifecycle-stage/model-evaluation",
        "lifecycle-stage/post-deployment",
        "technique-type/testing",
        "technique-type/analytical",
        "evidence-type/quantitative-metric",
        "evidence-type/test-results",
        "expertise-needed/ml-engineering",
        "expertise-needed/domain-expertise"
      ],
      "example_use_cases": [
        {
          "description": "Verifying that an adversarially-trained facial recognition system demonstrates genuine robustness against diverse attack types beyond those used in training, preventing false confidence in security.",
          "goal": "Security"
        },
        {
          "description": "Ensuring adversarial training of a spam filter improves reliable detection of adversarial emails without significantly degrading performance on normal messages.",
          "goal": "Reliability"
        },
        {
          "description": "Evaluating whether adversarial training of a loan approval model maintains fair lending decisions whilst improving robustness against applicants attempting to game the system through strategic feature manipulation.",
          "goal": "Reliability"
        },
        {
          "description": "Assessing whether adversarially-trained automated essay grading systems remain reliable on standard student submissions whilst becoming more robust to attempts at deliberately confusing the model with adversarial writing patterns.",
          "goal": "Reliability"
        },
        {
          "description": "Verifying that adversarial training of a medical imaging classifier for tumour detection maintains diagnostic accuracy on routine cases whilst improving robustness to image quality variations and potential adversarial attacks.",
          "goal": "Security"
        }
      ],
      "limitations": [
        {
          "description": "Models may overfit to adversarial examples in training data without generalizing to fundamentally different attack strategies."
        },
        {
          "description": "Adversarial training typically reduces clean accuracy by 2-10 percentage points, requiring careful evaluation of security-accuracy trade-offs for each application."
        },
        {
          "description": "Difficult to achieve certified robustness guarantees that hold against all possible attacks within a specified threat model."
        },
        {
          "description": "Adversarial training increases training time by 2-10x compared to standard training, as each batch requires generating adversarial examples, making it resource-intensive for large models or datasets."
        },
        {
          "description": "Requires diverse attack types for comprehensive evaluation beyond training distribution, necessitating ongoing research into emerging attack methods and regular re-evaluation cycles."
        }
      ],
      "resources": [
        {
          "title": "Adversarial Training: A Survey",
          "url": "https://www.semanticscholar.org/paper/3181007ef16737020b51d5f77ec2855bf2b82b0e",
          "source_type": "review_paper",
          "authors": [
            "Mengnan Zhao",
            "Lihe Zhang",
            "Jingwen Ye",
            "Huchuan Lu",
            "Baocai Yin",
            "Xinchao Wang"
          ]
        },
        {
          "title": "Welcome to the Adversarial Robustness Toolbox — Adversarial ...",
          "url": "https://adversarial-robustness-toolbox.readthedocs.io/",
          "source_type": "software_package"
        },
        {
          "title": "Training and evaluating networks via command line — robustness ...",
          "url": "https://robustness.readthedocs.io/en/latest/example_usage/cli_usage.html",
          "source_type": "documentation"
        },
        {
          "title": "art.metrics — Adversarial Robustness Toolbox 1.17.0 documentation",
          "url": "https://adversarial-robustness-toolbox.readthedocs.io/en/latest/modules/metrics.html",
          "source_type": "documentation"
        },
        {
          "title": "ApaNet: adversarial perturbations alleviation network for face verification",
          "url": "https://www.semanticscholar.org/paper/32d4829d19601ab92037ecedb69f1f7e803d455f",
          "source_type": "technical_paper",
          "authors": [
            "Guangling Sun",
            "Haoqi Hu",
            "Yuying Su",
            "Qi Liu",
            "Xiaofeng Lu"
          ]
        }
      ],
      "related_techniques": [
        "api-usage-pattern-monitoring",
        "continual-learning-stability-testing",
        "model-watermarking-and-theft-detection",
        "safety-envelope-testing"
      ]
    },
    {
      "slug": "chain-of-thought-faithfulness-evaluation",
      "name": "Chain-of-Thought Faithfulness Evaluation",
      "description": "Chain-of-thought faithfulness evaluation assesses the quality and faithfulness of step-by-step reasoning produced by language models. This technique evaluates whether intermediate reasoning steps are logically valid, factually accurate, and actually responsible for final answers (rather than post-hoc rationalisations). Evaluation methods include consistency checking (whether altered reasoning changes answers), counterfactual testing (injecting errors in reasoning chains), and comparison between reasoning paths for equivalent problems to ensure systematic rather than spurious reasoning.",
      "assurance_goals": [
        "Explainability",
        "Reliability",
        "Transparency"
      ],
      "tags": [
        "applicable-models/architecture/neural-networks/transformer/llm",
        "applicable-models/paradigm/generative",
        "applicable-models/requirements/black-box",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/explains/internal-mechanisms",
        "assurance-goal-category/explainability/property/fidelity",
        "assurance-goal-category/reliability",
        "assurance-goal-category/transparency",
        "data-type/text",
        "data-requirements/labeled-data-required",
        "evidence-type/qualitative-assessment",
        "evidence-type/test-results",
        "evidence-type/quantitative-metric",
        "expertise-needed/ml-engineering",
        "expertise-needed/domain-expertise",
        "expertise-needed/statistical-knowledge",
        "explanatory-scope/local",
        "lifecycle-stage/model-evaluation",
        "lifecycle-stage/post-deployment",
        "technique-type/testing",
        "technique-type/analytical"
      ],
      "example_use_cases": [
        {
          "description": "Evaluating a chemistry tutoring AI that guides students through chemical reaction balancing, ensuring each reasoning step correctly applies conservation of mass and charge rather than producing superficially plausible but scientifically incorrect pathways.",
          "goal": "Explainability"
        },
        {
          "description": "Ensuring a legal reasoning assistant produces reliable analysis by verifying that its chain-of-thought explanations correctly apply relevant statutes and precedents without logical gaps.",
          "goal": "Reliability"
        },
        {
          "description": "Testing science education AI that explains complex concepts step-by-step, verifying reasoning chains reflect sound pedagogical logic that helps students build understanding rather than just memorize facts.",
          "goal": "Transparency"
        },
        {
          "description": "Assessing an automated financial advisory system's investment recommendations to verify that its chain-of-thought explanations correctly apply financial principles, accurately calculate risk metrics, and logically justify portfolio allocations to clients.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Models may generate reasoning that appears valid but is actually post-hoc rationalization rather than the actual computational process leading to answers."
        },
        {
          "description": "Difficult to establish ground truth for complex reasoning tasks where multiple valid reasoning paths may exist."
        },
        {
          "description": "Verification requires domain expertise to judge whether reasoning steps are genuinely valid or merely superficially plausible."
        },
        {
          "description": "Computationally expensive to generate and verify multiple reasoning paths for comprehensive consistency checking."
        },
        {
          "description": "Scaling to production environments with high-volume requests is challenging, as thorough faithfulness evaluation may require generating multiple alternative reasoning paths for comparison, significantly increasing latency and cost."
        }
      ],
      "resources": [
        {
          "title": "Chain of Verification: Prompt Engineering for Unparalleled Accuracy",
          "url": "https://www.analyticsvidhya.com/blog/2024/07/chain-of-verification/",
          "source_type": "tutorial"
        },
        {
          "title": "Chain of Verification Using LangChain Expression Language & LLM",
          "url": "https://www.analyticsvidhya.com/blog/2023/12/chain-of-verification-implementation-using-langchain-expression-language-and-llm/",
          "source_type": "tutorial"
        }
      ],
      "related_techniques": [
        "few-shot-fairness-evaluation",
        "retrieval-augmented-generation-evaluation",
        "hallucination-detection",
        "constitutional-ai-evaluation"
      ]
    },
    {
      "slug": "constitutional-ai-evaluation",
      "name": "Constitutional AI Evaluation",
      "description": "Constitutional AI evaluation assesses models trained to follow explicit behavioural principles or 'constitutions' that specify desired values and constraints. This technique verifies that models consistently adhere to specified principles across diverse scenarios, measuring both principle compliance and handling of principle conflicts. Evaluation includes testing boundary cases where principles might conflict, measuring robustness to adversarial attempts to violate principles, and assessing whether models can explain their reasoning in terms of constitutional principles.",
      "assurance_goals": [
        "Safety",
        "Transparency",
        "Reliability"
      ],
      "tags": [
        "applicable-models/architecture/neural-networks/transformer/llm",
        "applicable-models/requirements/white-box",
        "applicable-models/paradigm/supervised",
        "assurance-goal-category/safety",
        "assurance-goal-category/transparency",
        "assurance-goal-category/reliability",
        "data-type/text",
        "data-requirements/labeled-data-required",
        "data-requirements/training-data-required",
        "lifecycle-stage/model-evaluation",
        "lifecycle-stage/post-deployment",
        "technique-type/testing",
        "technique-type/analytical",
        "evidence-type/quantitative-metric",
        "evidence-type/qualitative-assessment",
        "evidence-type/test-results",
        "expertise-needed/ml-engineering",
        "expertise-needed/domain-expertise"
      ],
      "example_use_cases": [
        {
          "description": "Evaluating a general-purpose AI assistant with the principle 'provide helpful information while refusing requests for illegal activities' to ensure it consistently distinguishes between legitimate chemistry education queries and attempts to obtain instructions for synthesising controlled substances or explosives.",
          "goal": "Safety"
        },
        {
          "description": "Testing whether an AI assistant can transparently explain its decisions by referencing the specific constitutional principles that guided its responses, enabling users to understand value-based reasoning.",
          "goal": "Transparency"
        },
        {
          "description": "Verifying that an AI news aggregator consistently applies stated principles about neutrality versus editorial perspective across diverse political topics and international news sources.",
          "goal": "Reliability"
        },
        {
          "description": "Evaluating a mental health support chatbot designed with principles like 'be empathetic and supportive while never providing medical diagnoses or replacing professional care' to verify it consistently maintains this boundary across varied emotional crises and user requests.",
          "goal": "Safety"
        },
        {
          "description": "Testing an AI homework assistant built on principles of 'guide learning without providing direct answers' to ensure it maintains this educational philosophy across different subjects, student ages, and question complexities without reverting to simply solving problems.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Constitutional principles may be vague or subject to interpretation, making it difficult to objectively measure compliance."
        },
        {
          "description": "Principles can conflict in complex scenarios, and evaluation must assess whether the model's priority ordering matches intended values."
        },
        {
          "description": "Models may learn to superficially cite principles in explanations without genuinely using them in decision-making processes."
        },
        {
          "description": "Creating comprehensive test sets covering all relevant principle applications and edge cases requires significant domain expertise and resources."
        },
        {
          "description": "Constitutional principles that seem clear in one cultural or linguistic context may be interpreted differently across diverse user populations, making it challenging to evaluate whether the model appropriately adapts its principle application or inappropriately varies its values."
        },
        {
          "description": "As organisational values evolve or principles are refined based on deployment experience, re-evaluation becomes necessary, but comparing results across principle versions is challenging without confounding changes in the model itself versus changes in evaluation criteria."
        }
      ],
      "resources": [
        {
          "title": "chrbradley/constitutional-reasoning-engine",
          "url": "https://github.com/chrbradley/constitutional-reasoning-engine",
          "source_type": "software_package"
        },
        {
          "title": "C3ai: Crafting and evaluating constitutions for constitutional ai",
          "url": "https://dl.acm.org/doi/abs/10.1145/3696410.3714705",
          "source_type": "technical_paper",
          "authors": [
            "Y Kyrychenko",
            "K Zhou",
            "E Bogucka"
          ]
        },
        {
          "title": "Towards Principled AI Alignment: An Evaluation and Augmentation of Inverse Constitutional AI",
          "url": "https://dash.harvard.edu/items/54a0845c-a3a3-4c73-9278-e4c4927c2e1a",
          "source_type": "technical_paper",
          "authors": [
            "E An"
          ]
        },
        {
          "title": "Constitution or Collapse? Exploring Constitutional AI with Llama 3-8B",
          "url": "https://arxiv.org/abs/2504.04918",
          "source_type": "technical_paper",
          "authors": [
            "X Zhang"
          ]
        }
      ],
      "related_techniques": [
        "ai-agent-safety-testing",
        "hallucination-detection",
        "jailbreak-resistance-testing",
        "multimodal-alignment-evaluation"
      ]
    },
    {
      "slug": "few-shot-fairness-evaluation",
      "name": "Few-Shot Fairness Evaluation",
      "description": "Few-shot fairness evaluation assesses whether in-context learning with few-shot examples introduces or amplifies biases in model predictions. This technique systematically varies demographic characteristics in few-shot examples and measures how these variations affect model outputs for different groups. Evaluation includes testing prompt sensitivity (how example selection impacts fairness), stereotype amplification (whether biased examples disproportionately affect outputs), and consistency (whether similar inputs receive equitable treatment regardless of example composition).",
      "assurance_goals": [
        "Fairness",
        "Reliability",
        "Transparency"
      ],
      "tags": [
        "applicable-models/architecture/neural-networks/transformer/llm",
        "applicable-models/paradigm/generative",
        "applicable-models/requirements/black-box",
        "assurance-goal-category/fairness",
        "assurance-goal-category/fairness/fairness-metrics",
        "assurance-goal-category/reliability",
        "assurance-goal-category/transparency",
        "data-requirements/labeled-data-required",
        "data-type/text",
        "evidence-type/quantitative-metric",
        "evidence-type/test-results",
        "evidence-type/documentation",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistical-knowledge",
        "expertise-needed/domain-expertise",
        "lifecycle-stage/model-evaluation",
        "lifecycle-stage/post-deployment",
        "technique-type/testing",
        "technique-type/analytical"
      ],
      "example_use_cases": [
        {
          "description": "Testing whether a resume screening LLM's few-shot examples inadvertently introduce gender bias by showing more male examples for technical positions, affecting how it evaluates subsequent applicants.",
          "goal": "Fairness"
        },
        {
          "description": "Ensuring a customer service classifier maintains reliable performance across demographic groups regardless of which few-shot examples users or developers choose to include in prompts.",
          "goal": "Reliability"
        },
        {
          "description": "Documenting how few-shot example selection affects fairness metrics, transparently reporting sensitivity to example composition in deployment guidelines.",
          "goal": "Transparency"
        },
        {
          "description": "Evaluating whether a medical triage LLM's few-shot examples for symptom assessment inadvertently encode demographic biases (e.g., more examples of cardiac symptoms in male patients), leading to differential urgency assessments across patient populations.",
          "goal": "Fairness"
        },
        {
          "description": "Testing whether few-shot examples used in a legal case summarization system introduce racial or socioeconomic bias in how defendant backgrounds or case circumstances are characterized, affecting case outcome predictions.",
          "goal": "Fairness"
        },
        {
          "description": "Assessing whether few-shot examples in a loan application review assistant systematically present more favourable examples for certain demographic profiles, biasing the model's assessment of creditworthiness for subsequent applications.",
          "goal": "Fairness"
        },
        {
          "description": "Verifying that an automated essay grading system maintains consistent standards across student demographics when few-shot examples inadvertently represent particular writing styles, dialects, or cultural references more prominently.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Vast number of possible few-shot example combinations makes exhaustive testing infeasible, requiring sampling strategies that may miss important configurations."
        },
        {
          "description": "Fairness may be highly sensitive to subtle differences in example wording or formatting, making it difficult to provide robust guarantees."
        },
        {
          "description": "Trade-offs between example diversity and task performance may force choices between fairness and accuracy."
        },
        {
          "description": "Results may not generalise across different prompt templates or instruction formats, requiring separate evaluation for each prompting strategy."
        },
        {
          "description": "Requires carefully labeled datasets with demographic annotations to measure fairness across groups, which may be unavailable, expensive to create, or raise privacy concerns in sensitive domains."
        },
        {
          "description": "Designing representative test sets that capture realistic few-shot example distributions requires deep domain expertise and understanding of how the system will be used in practice."
        },
        {
          "description": "Evaluating fairness across multiple demographic groups with various few-shot configurations can be computationally expensive, particularly for large language models with high inference costs."
        }
      ],
      "resources": [
        {
          "title": "tensorflow/fairness-indicators",
          "url": "https://github.com/tensorflow/fairness-indicators",
          "source_type": "software_package"
        },
        {
          "title": "AI4Bharat/indic-bias",
          "url": "https://github.com/AI4Bharat/indic-bias",
          "source_type": "software_package"
        },
        {
          "title": "Fairness-guided few-shot prompting for large language models",
          "url": "https://scholar.google.com/scholar?q=fairness+guided+few+shot+prompting",
          "source_type": "technical_paper"
        },
        {
          "title": "Few-shot fairness: Unveiling LLM's potential for fairness-aware classification",
          "url": "https://scholar.google.com/scholar?q=few+shot+fairness+unveiling+llm",
          "source_type": "technical_paper"
        },
        {
          "title": "Selecting shots for demographic fairness in few-shot learning with large language models",
          "url": "https://scholar.google.com/scholar?q=selecting+shots+demographic+fairness",
          "source_type": "technical_paper"
        }
      ],
      "related_techniques": [
        "prompt-robustness-testing",
        "toxicity-and-bias-detection",
        "sensitivity-analysis-for-fairness",
        "path-specific-counterfactual-fairness-assessment"
      ]
    },
    {
      "slug": "epistemic-uncertainty-quantification",
      "name": "Epistemic Uncertainty Quantification",
      "description": "Epistemic uncertainty quantification systematically measures model uncertainty about what it knows, partially knows, and doesn't know across different domains and topics. This technique uses probing datasets, confidence calibration analysis, and consistency testing to quantify epistemic uncertainty (uncertainty due to lack of knowledge) as distinct from aleatoric uncertainty (inherent data uncertainty). Uncertainty quantification enables appropriate deployment scoping, communicating model limitations, and identifying knowledge gaps requiring additional training or human oversight.",
      "assurance_goals": [
        "Transparency",
        "Reliability",
        "Safety"
      ],
      "tags": [
        "applicable-models/architecture/model-agnostic",
        "applicable-models/requirements/black-box",
        "assurance-goal-category/transparency",
        "assurance-goal-category/reliability",
        "assurance-goal-category/safety",
        "data-requirements/labeled-data-required",
        "data-type/any",
        "evidence-type/quantitative-metric",
        "evidence-type/documentation",
        "evidence-type/visual-artifact",
        "expertise-needed/ml-engineering",
        "expertise-needed/domain-expertise",
        "expertise-needed/statistical-knowledge",
        "lifecycle-stage/model-evaluation",
        "lifecycle-stage/post-deployment",
        "technique-type/analytical",
        "technique-type/testing"
      ],
      "example_use_cases": [
        {
          "description": "Mapping a medical AI's knowledge boundaries to identify conditions it diagnoses reliably versus conditions requiring specialist referral, enabling safe deployment with appropriate scope limitations.",
          "goal": "Safety"
        },
        {
          "description": "Identifying knowledge gaps in a public policy analysis AI to ensure it provides reliable information about well-researched policy areas while disclaiming uncertainty about emerging policy domains or local contexts.",
          "goal": "Reliability"
        },
        {
          "description": "Transparently documenting model knowledge boundaries in user-facing applications, helping users understand when to trust AI outputs versus seek additional verification.",
          "goal": "Transparency"
        },
        {
          "description": "Quantifying uncertainty in an automated loan approval system to identify applications where the model's knowledge boundaries are exceeded (e.g., novel business types or unusual financial situations), triggering human expert review rather than automated rejection or approval.",
          "goal": "Reliability"
        },
        {
          "description": "Mapping knowledge boundaries in an educational AI tutor to distinguish between well-covered curriculum topics and emerging areas where the model lacks sufficient training data, ensuring students receive reliable guidance and appropriate referrals to human instructors.",
          "goal": "Safety"
        }
      ],
      "limitations": [
        {
          "description": "Comprehensive knowledge mapping across all possible domains and topics is infeasible, requiring prioritization of important knowledge areas."
        },
        {
          "description": "Knowledge boundaries may be fuzzy rather than discrete, making it difficult to establish clear cutoffs between known and unknown."
        },
        {
          "description": "Models may be confidently wrong in some areas, making calibration and confidence signals unreliable indicators of actual knowledge."
        },
        {
          "description": "Knowledge boundaries shift as models are updated or fine-tuned, requiring continuous remapping to maintain accuracy."
        },
        {
          "description": "Uncertainty quantification methods can add significant computational overhead (10-100x inference time for ensemble-based approaches), making real-time deployment challenging for latency-sensitive applications."
        },
        {
          "description": "Interpreting uncertainty estimates requires statistical expertise and domain knowledge to set appropriate thresholds for triggering human review or system warnings."
        }
      ],
      "resources": [
        {
          "title": "ZBox1005/CoT-UQ",
          "url": "https://github.com/ZBox1005/CoT-UQ",
          "source_type": "software_package"
        },
        {
          "title": "Knowledge boundary of large language models: A survey",
          "url": "https://aclanthology.org/2025.acl-long.256/",
          "source_type": "technical_paper"
        },
        {
          "title": "Teaching large language models to express knowledge boundary from their own signals",
          "url": "https://aclanthology.org/2025.knowllm-1.3/",
          "source_type": "technical_paper"
        },
        {
          "title": "Benchmarking knowledge boundary for large language models",
          "url": "https://arxiv.org/abs/2402.11493",
          "source_type": "technical_paper"
        },
        {
          "title": "Investigating the factual knowledge boundary of large language models with retrieval augmentation",
          "url": "https://aclanthology.org/2025.coling-main.250/",
          "source_type": "technical_paper"
        }
      ],
      "related_techniques": [
        "prediction-intervals",
        "jackknife-resampling",
        "hallucination-detection",
        "bootstrapping"
      ]
    },
    {
      "slug": "multimodal-alignment-evaluation",
      "name": "Multimodal Alignment Evaluation",
      "description": "Multimodal alignment evaluation assesses whether different modalities (vision, language, audio) are synchronised and consistent in multimodal AI systems. This technique tests whether descriptions match content, sounds associate with correct sources, and cross-modal reasoning maintains accuracy. It produces alignment scores, grounding quality metrics, and reports on modality-specific failure patterns.",
      "assurance_goals": [
        "Reliability",
        "Explainability",
        "Safety"
      ],
      "tags": [
        "applicable-models/architecture/neural-networks",
        "applicable-models/architecture/neural-networks/transformer",
        "applicable-models/requirements/black-box",
        "assurance-goal-category/reliability",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/explains/internal-mechanisms",
        "assurance-goal-category/safety",
        "data-type/image",
        "data-type/text",
        "data-requirements/labeled-data-required",
        "lifecycle-stage/model-evaluation",
        "lifecycle-stage/post-deployment",
        "technique-type/testing",
        "technique-type/analytical",
        "evidence-type/quantitative-metric",
        "evidence-type/qualitative-assessment",
        "evidence-type/test-results",
        "expertise-needed/ml-engineering",
        "expertise-needed/domain-expertise"
      ],
      "example_use_cases": [
        {
          "description": "Verifying that a vision-language model for medical imaging correctly associates diagnostic findings in radiology reports with corresponding visual features in scans, preventing misdiagnosis from misaligned interpretations.",
          "goal": "Safety"
        },
        {
          "description": "Testing accessibility tools that generate image descriptions for visually impaired users, ensuring descriptions accurately reflect actual visual content and don't misrepresent important details or context.",
          "goal": "Reliability"
        },
        {
          "description": "Evaluating whether visual grounding mechanisms correctly highlight image regions corresponding to generated text descriptions, enabling users to verify and understand model reasoning.",
          "goal": "Explainability"
        },
        {
          "description": "Evaluating alignment in autonomous vehicle systems where camera, lidar, and radar data must be synchronised with language-based reasoning about driving scenarios, ensuring object detection, tracking, and decision explanations remain consistent across modalities.",
          "goal": "Safety"
        },
        {
          "description": "Testing e-commerce product recommendation systems to verify that visual product features align with textual descriptions and user queries, preventing mismatched recommendations that frustrate customers or misrepresent products.",
          "goal": "Reliability"
        },
        {
          "description": "Validating educational content generation tools that create visual learning materials with text explanations, ensuring diagrams, images, and written content present consistent information without contradictions that could confuse learners.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Defining ground truth for proper alignment can be subjective, especially for abstract concepts or implicit relationships between modalities."
        },
        {
          "description": "Models may appear aligned on simple cases but fail on complex scenarios requiring deep understanding of both modalities."
        },
        {
          "description": "Evaluation requires multimodal datasets with high-quality annotations linking different modalities, which are expensive to create."
        },
        {
          "description": "Adversarial testing may not cover all possible misalignment scenarios, particularly rare or subtle cases of modal inconsistency."
        },
        {
          "description": "Multimodal alignment evaluation requires processing multiple data types simultaneously, increasing computational costs by 2-5x compared to single-modality evaluation, particularly for video or high-resolution image analysis."
        },
        {
          "description": "Creating benchmarks requires expertise across multiple domains (computer vision, NLP, audio processing) and application-specific knowledge, making it difficult to assemble qualified evaluation teams."
        },
        {
          "description": "Annotating multimodal datasets with alignment ground truth is labor-intensive and expensive, typically costing 3-10x more per sample than single-modality annotation due to increased complexity."
        }
      ],
      "resources": [
        {
          "title": "CLIP",
          "url": "https://huggingface.co/docs/transformers/en/model_doc/clip",
          "source_type": "documentation"
        },
        {
          "title": "Welcome to verl's documentation! — verl documentation",
          "url": "https://verl.readthedocs.io/",
          "source_type": "documentation"
        },
        {
          "title": "TRL - Transformer Reinforcement Learning",
          "url": "https://huggingface.co/docs/trl/en/index",
          "source_type": "documentation"
        },
        {
          "title": "An Efficient Approach for Calibration of Automotive Radar–Camera With Real-Time Projection of Multimodal Data",
          "url": "https://www.semanticscholar.org/paper/07c4dd6b40c1e284e565463586bbeb66f61c0cb7",
          "source_type": "technical_paper",
          "authors": [
            "Nitish Kumar",
            "Ayush Dasgupta",
            "Venkata Satyanand Mutnuri",
            "Rajalakshmi Pachamuthu"
          ]
        },
        {
          "title": "Integration of Large Language Models and Computer Vision Algorithms in LMS: A Methodology for Automated Verification of Software Tasks and Multimodal Analysis of Educational Data",
          "url": "https://www.semanticscholar.org/paper/0991fef3f336b1ec981d83d7a7c6b8ea2e27598f",
          "source_type": "technical_paper",
          "authors": [
            "E. I. Markin",
            "V. V. Zuparova",
            "A. I. Martyshkin"
          ]
        }
      ],
      "related_techniques": [
        "adversarial-robustness-testing",
        "constitutional-ai-evaluation",
        "hallucination-detection",
        "jailbreak-resistance-testing"
      ]
    },
    {
      "slug": "out-of-domain-detection",
      "name": "Out-of-Domain Detection",
      "description": "Out-of-domain (OOD) detection identifies user inputs that fall outside an AI system's intended domain or capabilities, enabling graceful handling rather than generating unreliable responses. This technique trains classifiers or uses embedding-based methods to recognise when queries require knowledge or capabilities the system doesn't possess. OOD detection enables systems to explicitly decline, redirect, or request clarification rather than hallucinating answers or applying inappropriate reasoning to unfamiliar domains.",
      "assurance_goals": [
        "Reliability",
        "Safety",
        "Transparency"
      ],
      "tags": [
        "applicable-models/architecture/model-agnostic",
        "applicable-models/architecture/neural-networks/transformer/llm",
        "applicable-models/paradigm/supervised",
        "applicable-models/requirements/black-box",
        "assurance-goal-category/reliability",
        "assurance-goal-category/safety",
        "assurance-goal-category/transparency",
        "data-requirements/training-data-required",
        "data-requirements/labeled-data-required",
        "data-type/text",
        "evidence-type/quantitative-metric",
        "evidence-type/test-results",
        "expertise-needed/ml-engineering",
        "expertise-needed/domain-expertise",
        "lifecycle-stage/model-development",
        "lifecycle-stage/deployment",
        "lifecycle-stage/post-deployment",
        "technique-type/algorithmic",
        "technique-type/procedural"
      ],
      "example_use_cases": [
        {
          "description": "Enabling a medical chatbot specialized in diabetes management to recognize and deflect questions about unrelated conditions, avoiding potentially dangerous medical advice outside its training domain.",
          "goal": "Safety"
        },
        {
          "description": "Ensuring an educational AI tutor for high school mathematics reliably identifies advanced university-level questions and redirects students to appropriate resources rather than attempting explanations beyond its scope.",
          "goal": "Reliability"
        },
        {
          "description": "Transparently communicating system limitations by explicitly informing users when queries exceed the AI's scope rather than silently providing low-quality responses.",
          "goal": "Transparency"
        },
        {
          "description": "Protecting an insurance claims processing system trained on standard claims from making unreliable decisions on unusual or complex cases (e.g., natural disasters, emerging fraud patterns) by detecting and routing them to specialist adjusters.",
          "goal": "Reliability"
        },
        {
          "description": "Ensuring an autonomous vehicle's perception system detects when it encounters road conditions, weather patterns, or infrastructure types outside its training distribution (e.g., unmapped construction zones, unusual signage), triggering increased caution or human intervention.",
          "goal": "Safety"
        }
      ],
      "limitations": [
        {
          "description": "Defining precise scope boundaries can be challenging, especially for general-purpose systems or domains with fuzzy edges."
        },
        {
          "description": "May produce false positives that reject legitimate queries at the boundary of the system's capabilities, degrading user experience."
        },
        {
          "description": "Users may rephrase out-of-scope queries to bypass detection, requiring robust handling of paraphrases and edge cases."
        },
        {
          "description": "Difficult to maintain accurate scope detection as systems are updated and capabilities expand or shift over time."
        },
        {
          "description": "Establishing reliable domain boundaries requires substantial labeled data from both in-domain and out-of-domain examples, which can be expensive to collect and annotate systematically."
        },
        {
          "description": "Real-time OOD detection adds inference latency (typically 10-50ms per query depending on method), which may impact user experience in time-sensitive applications."
        }
      ],
      "resources": [
        {
          "title": "silverriver/OOD4NLU",
          "url": "https://github.com/silverriver/OOD4NLU",
          "source_type": "software_package"
        },
        {
          "title": "rivercold/BERT-unsupervised-OOD",
          "url": "https://github.com/rivercold/BERT-unsupervised-OOD",
          "source_type": "software_package"
        },
        {
          "title": "SLAD-ml/few-shot-ood",
          "url": "https://github.com/SLAD-ml/few-shot-ood",
          "source_type": "software_package"
        },
        {
          "title": "pris-nlp/Generative_distance-based_OOD",
          "url": "https://github.com/pris-nlp/Generative_distance-based_OOD",
          "source_type": "software_package"
        },
        {
          "title": "A Unified Survey on Anomaly, Novelty, Open-Set, and Out-of-Distribution Detection: Solutions and Future Challenges",
          "url": "https://www.semanticscholar.org/paper/8b153cc2c7f5ea9f307f12ea945a5e9196ee5c52",
          "source_type": "review_paper",
          "authors": [
            "Mohammadreza Salehi",
            "Hossein Mirzaei",
            "Dan Hendrycks",
            "Yixuan Li",
            "M. H. Rohban",
            "M. Sabokrou"
          ]
        }
      ],
      "related_techniques": [
        "deep-ensembles",
        "hallucination-detection",
        "anomaly-detection",
        "conformal-prediction"
      ]
    },
    {
      "slug": "retrieval-augmented-generation-evaluation",
      "name": "Retrieval-Augmented Generation Evaluation",
      "description": "RAG evaluation assesses systems combining retrieval and generation by measuring retrieval quality, generation faithfulness, and overall performance. This technique evaluates whether retrieved context is relevant, whether responses faithfully represent information without hallucination, and how systems handle insufficient context. Key metrics include retrieval precision/recall, answer relevance, faithfulness scores, and citation accuracy.",
      "assurance_goals": [
        "Reliability",
        "Transparency",
        "Explainability"
      ],
      "tags": [
        "applicable-models/architecture/neural-networks/transformer/llm",
        "applicable-models/paradigm/generative",
        "applicable-models/requirements/black-box",
        "assurance-goal-category/reliability",
        "assurance-goal-category/transparency",
        "assurance-goal-category/explainability",
        "assurance-goal-category/explainability/explains/internal-mechanisms",
        "assurance-goal-category/explainability/property/fidelity",
        "data-requirements/labeled-data-required",
        "data-type/text",
        "evidence-type/quantitative-metric",
        "evidence-type/qualitative-assessment",
        "evidence-type/test-results",
        "expertise-needed/ml-engineering",
        "expertise-needed/domain-expertise",
        "lifecycle-stage/model-evaluation",
        "lifecycle-stage/post-deployment",
        "technique-type/testing",
        "technique-type/analytical"
      ],
      "example_use_cases": [
        {
          "description": "Evaluating a technical support knowledge system at a software company to ensure it retrieves relevant troubleshooting documentation and generates accurate solutions without fabricating configuration steps or commands not present in official documentation.",
          "goal": "Reliability"
        },
        {
          "description": "Assessing whether a legal research assistant properly cites source documents when generating case summaries, enabling lawyers to verify information and trace conclusions back to authoritative sources.",
          "goal": "Transparency"
        },
        {
          "description": "Evaluating a scientific literature review system to verify generated research summaries accurately synthesize findings across papers, clearly indicating contradictory results or missing evidence in the knowledge base.",
          "goal": "Explainability"
        },
        {
          "description": "Evaluating a clinical decision support system that retrieves relevant medical literature and patient records to ensure generated treatment recommendations accurately reflect evidence-based guidelines without extrapolating beyond available clinical data.",
          "goal": "Reliability"
        },
        {
          "description": "Assessing a government benefits information chatbot to verify it retrieves relevant policy documents and accurately communicates eligibility criteria without hallucinating benefits, amounts, or requirements that could mislead citizens seeking assistance.",
          "goal": "Transparency"
        }
      ],
      "limitations": [
        {
          "description": "Evaluation requires high-quality ground truth datasets with known correct retrievals and answers, which may be expensive or impossible to create for specialized domains."
        },
        {
          "description": "Faithfulness assessment can be subjective and difficult to automate, often requiring human judgment to determine whether responses accurately represent retrieved context."
        },
        {
          "description": "Trade-offs between retrieval precision and recall mean optimizing for one metric may degrade the other, requiring domain-specific balancing decisions."
        },
        {
          "description": "Metrics may not capture subtle quality issues like incomplete answers, misleading emphasis, or failure to synthesize information from multiple retrieved sources."
        },
        {
          "description": "Evaluating multi-hop reasoning—where answers require synthesising information across multiple retrieved documents—is particularly challenging, as standard metrics may not capture whether the system correctly chains information or makes unsupported logical leaps."
        },
        {
          "description": "Difficult to isolate whether poor performance stems from retrieval failures (finding wrong documents), generation failures (misusing correct documents), or interaction effects, complicating diagnosis and improvement efforts."
        }
      ],
      "resources": [
        {
          "title": "Evaluation of Retrieval-Augmented Generation: A Survey",
          "url": "https://www.semanticscholar.org/paper/3c6a6c8de005ef5722a54847747f65922e79d622",
          "source_type": "review_paper",
          "authors": [
            "Hao Yu",
            "Aoran Gan",
            "Kai Zhang",
            "Shiwei Tong",
            "Qi Liu",
            "Zhaofeng Liu"
          ]
        },
        {
          "title": "LLM RAG Evaluation with MLflow Example Notebook | MLflow",
          "url": "https://mlflow.org/docs/3.0.1/llms/rag/notebooks/mlflow-e2e-evaluation/",
          "source_type": "tutorial"
        },
        {
          "title": "hoorangyee/LRAGE",
          "url": "https://github.com/hoorangyee/LRAGE",
          "source_type": "software_package"
        },
        {
          "title": "Evaluating RAG Applications with RAGAs | Towards Data Science",
          "url": "https://towardsdatascience.com/evaluating-rag-applications-with-ragas-81d67b0ee31a/",
          "source_type": "tutorial"
        },
        {
          "title": "Enhancing the Precision and Interpretability of Retrieval-Augmented Generation (RAG) in Legal Technology: A Survey",
          "url": "https://www.semanticscholar.org/paper/f51e92ce3f63cf04c26374c0ca33a2a751931cf6",
          "source_type": "technical_paper",
          "authors": [
            "Mahd Hindi",
            "Linda Mohammed",
            "Ommama Maaz",
            "Abdulmalik Alwarafy"
          ]
        }
      ],
      "related_techniques": [
        "hallucination-detection",
        "chain-of-thought-faithfulness-evaluation",
        "out-of-domain-detection",
        "prompt-robustness-testing"
      ]
    },
    {
      "slug": "toxicity-and-bias-detection",
      "name": "Toxicity and Bias Detection",
      "description": "Toxicity and bias detection uses automated classifiers and human review to identify harmful, offensive, or biased content in model outputs. This technique employs tools trained to detect toxic language, hate speech, stereotypes, and demographic biases through targeted testing with adversarial prompts. Detection covers explicit toxicity, implicit bias, and distributional unfairness across demographic groups.",
      "assurance_goals": [
        "Safety",
        "Fairness",
        "Reliability"
      ],
      "tags": [
        "applicable-models/architecture/model-agnostic",
        "applicable-models/architecture/neural-networks/transformer/llm",
        "applicable-models/requirements/black-box",
        "applicable-models/paradigm/generative",
        "assurance-goal-category/safety",
        "assurance-goal-category/fairness",
        "assurance-goal-category/fairness-metrics",
        "assurance-goal-category/reliability",
        "data-type/text",
        "data-requirements/labeled-data-required",
        "lifecycle-stage/model-evaluation",
        "lifecycle-stage/post-deployment",
        "lifecycle-stage/system-deployment-and-use-monitoring",
        "technique-type/testing",
        "technique-type/analytical",
        "evidence-type/quantitative-metric",
        "evidence-type/qualitative-assessment",
        "evidence-type/test-results",
        "expertise-needed/ml-engineering",
        "expertise-needed/domain-expertise"
      ],
      "example_use_cases": [
        {
          "description": "Screening a chatbot's responses for toxic language, hate speech, and harmful content before deployment in public-facing applications where vulnerable users including children might interact with it.",
          "goal": "Safety"
        },
        {
          "description": "Testing whether a content generation model produces stereotypical or discriminatory outputs when prompted with queries about different demographic groups, professions, or social characteristics.",
          "goal": "Fairness"
        },
        {
          "description": "Screening an AI writing assistant for educational content to ensure it maintains appropriate language and doesn't generate offensive material that could be harmful in classroom or academic settings.",
          "goal": "Reliability"
        },
        {
          "description": "Screening a mental health support chatbot to ensure it doesn't generate stigmatising language about mental health conditions, substance use, or marginalised communities, which could cause harm to vulnerable patients seeking help.",
          "goal": "Safety"
        },
        {
          "description": "Monitoring a banking chatbot's responses to detect bias in how it addresses customers from different demographic groups, ensuring equitable treatment in explaining financial products, fees, or denial reasons.",
          "goal": "Fairness"
        },
        {
          "description": "Testing a content moderation assistant to ensure it consistently identifies hate speech, harassment, and discriminatory content across different demographic targets without over-flagging minority language patterns or dialect variations.",
          "goal": "Fairness"
        },
        {
          "description": "Evaluating an AI interview scheduling assistant to verify it doesn't generate biased language or make stereotypical assumptions when communicating with candidates from diverse backgrounds or with non-traditional career paths.",
          "goal": "Fairness"
        }
      ],
      "limitations": [
        {
          "description": "Toxicity classifiers themselves may have biases, potentially flagging legitimate discussions of sensitive topics or minority language patterns as toxic."
        },
        {
          "description": "Context-dependent nature of toxicity makes automated detection challenging, as the same phrase may be harmful or harmless depending on usage context."
        },
        {
          "description": "Evolving language and cultural differences mean toxicity definitions change over time and vary across communities, requiring constant updating."
        },
        {
          "description": "Sophisticated models may generate subtle bias or coded language that evades automated detection while still being harmful."
        },
        {
          "description": "Comprehensive toxicity detection often requires human review to validate automated findings and handle edge cases, which is resource-intensive and may not scale to high-volume applications or real-time content generation."
        },
        {
          "description": "Toxicity classifiers require large labeled datasets of harmful content for training and validation, which are expensive to create, emotionally taxing for annotators, and raise ethical concerns about exposing people to harmful material."
        },
        {
          "description": "Configuring detection thresholds involves tradeoffs between false positives (over-censoring legitimate content) and false negatives (missing harmful content), with different stakeholders often disagreeing on acceptable balance points."
        },
        {
          "description": "Detection performance often degrades significantly for non-English languages, code-switching, dialects, and internet slang, limiting effectiveness for global or multilingual applications."
        }
      ],
      "resources": [
        {
          "title": "Evaluating Toxicity in Large Language Models",
          "url": "https://www.analyticsvidhya.com/blog/2025/03/evaluating-toxicity-in-large-language-models/",
          "source_type": "tutorial"
        },
        {
          "title": "What are Guardrails AI?",
          "url": "https://www.analyticsvidhya.com/blog/2024/05/building-responsible-ai-with-guardrails-ai/",
          "source_type": "tutorial"
        },
        {
          "title": "Toxic Comment Classification using BERT - GeeksforGeeks",
          "url": "https://www.geeksforgeeks.org/machine-learning/toxic-comment-classification-using-bert/",
          "source_type": "tutorial"
        },
        {
          "title": "Episode #188: Measuring Bias, Toxicity, and Truthfulness in LLMs ...",
          "url": "https://realpython.com/podcasts/rpp/188/",
          "source_type": "tutorial"
        },
        {
          "title": "Responsible AI in the Era of Generative AI",
          "url": "https://www.analyticsvidhya.com/blog/2024/09/responsible-generative-ai/",
          "source_type": "tutorial"
        }
      ],
      "related_techniques": [
        "few-shot-fairness-evaluation",
        "demographic-parity-assessment",
        "counterfactual-fairness-assessment",
        "sensitivity-analysis-for-fairness"
      ]
    }
  ],
  "count": 9
}