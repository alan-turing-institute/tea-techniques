{
  "goal": "fairness",
  "dimension": "fairness-metrics",
  "subcategories": [],
  "techniques": [
    {
      "slug": "few-shot-fairness-evaluation",
      "name": "Few-Shot Fairness Evaluation",
      "description": "Few-shot fairness evaluation assesses whether in-context learning with few-shot examples introduces or amplifies biases in model predictions. This technique systematically varies demographic characteristics in few-shot examples and measures how these variations affect model outputs for different groups. Evaluation includes testing prompt sensitivity (how example selection impacts fairness), stereotype amplification (whether biased examples disproportionately affect outputs), and consistency (whether similar inputs receive equitable treatment regardless of example composition).",
      "assurance_goals": [
        "Fairness",
        "Reliability",
        "Transparency"
      ],
      "tags": [
        "applicable-models/architecture/neural-networks/transformer/llm",
        "applicable-models/paradigm/generative",
        "applicable-models/requirements/black-box",
        "assurance-goal-category/fairness",
        "assurance-goal-category/fairness/fairness-metrics",
        "assurance-goal-category/reliability",
        "assurance-goal-category/transparency",
        "data-requirements/labeled-data-required",
        "data-type/text",
        "evidence-type/quantitative-metric",
        "evidence-type/test-results",
        "evidence-type/documentation",
        "expertise-needed/ml-engineering",
        "expertise-needed/statistical-knowledge",
        "expertise-needed/domain-expertise",
        "lifecycle-stage/model-evaluation",
        "lifecycle-stage/post-deployment",
        "technique-type/testing",
        "technique-type/analytical"
      ],
      "example_use_cases": [
        {
          "description": "Testing whether a resume screening LLM's few-shot examples inadvertently introduce gender bias by showing more male examples for technical positions, affecting how it evaluates subsequent applicants.",
          "goal": "Fairness"
        },
        {
          "description": "Ensuring a customer service classifier maintains reliable performance across demographic groups regardless of which few-shot examples users or developers choose to include in prompts.",
          "goal": "Reliability"
        },
        {
          "description": "Documenting how few-shot example selection affects fairness metrics, transparently reporting sensitivity to example composition in deployment guidelines.",
          "goal": "Transparency"
        },
        {
          "description": "Evaluating whether a medical triage LLM's few-shot examples for symptom assessment inadvertently encode demographic biases (e.g., more examples of cardiac symptoms in male patients), leading to differential urgency assessments across patient populations.",
          "goal": "Fairness"
        },
        {
          "description": "Testing whether few-shot examples used in a legal case summarization system introduce racial or socioeconomic bias in how defendant backgrounds or case circumstances are characterized, affecting case outcome predictions.",
          "goal": "Fairness"
        },
        {
          "description": "Assessing whether few-shot examples in a loan application review assistant systematically present more favourable examples for certain demographic profiles, biasing the model's assessment of creditworthiness for subsequent applications.",
          "goal": "Fairness"
        },
        {
          "description": "Verifying that an automated essay grading system maintains consistent standards across student demographics when few-shot examples inadvertently represent particular writing styles, dialects, or cultural references more prominently.",
          "goal": "Reliability"
        }
      ],
      "limitations": [
        {
          "description": "Vast number of possible few-shot example combinations makes exhaustive testing infeasible, requiring sampling strategies that may miss important configurations."
        },
        {
          "description": "Fairness may be highly sensitive to subtle differences in example wording or formatting, making it difficult to provide robust guarantees."
        },
        {
          "description": "Trade-offs between example diversity and task performance may force choices between fairness and accuracy."
        },
        {
          "description": "Results may not generalise across different prompt templates or instruction formats, requiring separate evaluation for each prompting strategy."
        },
        {
          "description": "Requires carefully labeled datasets with demographic annotations to measure fairness across groups, which may be unavailable, expensive to create, or raise privacy concerns in sensitive domains."
        },
        {
          "description": "Designing representative test sets that capture realistic few-shot example distributions requires deep domain expertise and understanding of how the system will be used in practice."
        },
        {
          "description": "Evaluating fairness across multiple demographic groups with various few-shot configurations can be computationally expensive, particularly for large language models with high inference costs."
        }
      ],
      "resources": [
        {
          "title": "tensorflow/fairness-indicators",
          "url": "https://github.com/tensorflow/fairness-indicators",
          "source_type": "software_package"
        },
        {
          "title": "AI4Bharat/indic-bias",
          "url": "https://github.com/AI4Bharat/indic-bias",
          "source_type": "software_package"
        },
        {
          "title": "Fairness-guided few-shot prompting for large language models",
          "url": "https://scholar.google.com/scholar?q=fairness+guided+few+shot+prompting",
          "source_type": "technical_paper"
        },
        {
          "title": "Few-shot fairness: Unveiling LLM's potential for fairness-aware classification",
          "url": "https://scholar.google.com/scholar?q=few+shot+fairness+unveiling+llm",
          "source_type": "technical_paper"
        },
        {
          "title": "Selecting shots for demographic fairness in few-shot learning with large language models",
          "url": "https://scholar.google.com/scholar?q=selecting+shots+demographic+fairness",
          "source_type": "technical_paper"
        }
      ],
      "related_techniques": [
        "prompt-robustness-testing",
        "toxicity-and-bias-detection",
        "sensitivity-analysis-for-fairness",
        "path-specific-counterfactual-fairness-assessment"
      ]
    }
  ],
  "count": 1
}