[
  {
    "slug": "shapley-additive-explanations",
    "name": "SHapley Additive exPlanations",
    "description": "SHAP explains model predictions by quantifying how much each input feature contributes to the outcome. It assigns an importance score to every feature, indicating whether it pushes the prediction towards or away from the average. The method systematically evaluates how predictions change as features are included or excluded, drawing on game theory concepts to ensure a fair distribution of contributions.",
    "assurance_goals": [
      "Explainability",
      "Fairness",
      "Reliability"
    ],
    "tags": [
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/attribution-methods/perturbation-based",
      "assurance-goal-category/explainability/explains/feature-importance",
      "assurance-goal-category/explainability/property/completeness",
      "assurance-goal-category/explainability/property/consistency",
      "assurance-goal-category/fairness",
      "assurance-goal-category/reliability",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic",
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box"
    ],
    "searchText": "shapley additive explanations shap explains model predictions by quantifying how much each input feature contributes to the outcome. it assigns an importance score to every feature, indicating whether it pushes the prediction towards or away from the average. the method systematically evaluates how predictions change as features are included or excluded, drawing on game theory concepts to ensure a fair distribution of contributions. explainability fairness reliability assurance-goal-category/explainability assurance-goal-category/explainability/attribution-methods/perturbation-based assurance-goal-category/explainability/explains/feature-importance assurance-goal-category/explainability/property/completeness assurance-goal-category/explainability/property/consistency assurance-goal-category/fairness assurance-goal-category/reliability data-requirements/no-special-requirements data-type/any evidence-type/quantitative-metric expertise-needed/statistics explanatory-scope/global explanatory-scope/local lifecycle-stage/model-development technique-type/algorithmic applicable-models/architecture/model-agnostic applicable-models/requirements/black-box"
  },
  {
    "slug": "permutation-importance",
    "name": "Permutation Importance",
    "description": "Permutation Importance quantifies a feature's contribution to a model's performance by randomly shuffling its values and measuring the resulting drop in predictive accuracy. If shuffling a feature significantly degrades the model's performance, that feature is considered important. This model-agnostic technique helps identify which inputs are genuinely driving predictions, rather than just being correlated with the outcome.",
    "assurance_goals": [
      "Explainability",
      "Reliability"
    ],
    "tags": [
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/attribution-methods/perturbation-based",
      "assurance-goal-category/explainability/explains/feature-importance",
      "assurance-goal-category/explainability/property/consistency",
      "assurance-goal-category/explainability/property/fidelity",
      "assurance-goal-category/reliability",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic",
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box"
    ],
    "searchText": "permutation importance permutation importance quantifies a feature's contribution to a model's performance by randomly shuffling its values and measuring the resulting drop in predictive accuracy. if shuffling a feature significantly degrades the model's performance, that feature is considered important. this model-agnostic technique helps identify which inputs are genuinely driving predictions, rather than just being correlated with the outcome. explainability reliability assurance-goal-category/explainability assurance-goal-category/explainability/attribution-methods/perturbation-based assurance-goal-category/explainability/explains/feature-importance assurance-goal-category/explainability/property/consistency assurance-goal-category/explainability/property/fidelity assurance-goal-category/reliability data-requirements/no-special-requirements data-type/any evidence-type/quantitative-metric expertise-needed/statistics explanatory-scope/global lifecycle-stage/model-development technique-type/algorithmic applicable-models/architecture/model-agnostic applicable-models/requirements/black-box"
  },
  {
    "slug": "mean-decrease-impurity",
    "name": "Mean Decrease Impurity",
    "description": "Mean Decrease Impurity (MDI) quantifies a feature's importance in tree-based models (e.g., Random Forests, Gradient Boosting Machines) by measuring the total reduction in impurity (e.g., Gini impurity, entropy) across all splits where the feature is used. Features that lead to larger, more consistent reductions in impurity are considered more important, indicating their effectiveness in creating homogeneous child nodes and improving predictive accuracy. This makes MDI a computationally efficient method for feature selection and model validation in tree-based ensembles.",
    "assurance_goals": [
      "Explainability",
      "Reliability"
    ],
    "tags": [
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/attribution-methods/model-specific",
      "assurance-goal-category/explainability/explains/feature-importance",
      "assurance-goal-category/explainability/property/efficiency",
      "assurance-goal-category/explainability/property/completeness",
      "assurance-goal-category/reliability",
      "data-requirements/no-special-requirements",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic",
      "applicable-models/architecture/tree-based",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/white-box"
    ],
    "searchText": "mean decrease impurity mean decrease impurity (mdi) quantifies a feature's importance in tree-based models (e.g., random forests, gradient boosting machines) by measuring the total reduction in impurity (e.g., gini impurity, entropy) across all splits where the feature is used. features that lead to larger, more consistent reductions in impurity are considered more important, indicating their effectiveness in creating homogeneous child nodes and improving predictive accuracy. this makes mdi a computationally efficient method for feature selection and model validation in tree-based ensembles. explainability reliability assurance-goal-category/explainability assurance-goal-category/explainability/attribution-methods/model-specific assurance-goal-category/explainability/explains/feature-importance assurance-goal-category/explainability/property/efficiency assurance-goal-category/explainability/property/completeness assurance-goal-category/reliability data-requirements/no-special-requirements data-type/tabular evidence-type/quantitative-metric expertise-needed/statistics explanatory-scope/global lifecycle-stage/model-development technique-type/algorithmic applicable-models/architecture/tree-based applicable-models/paradigm/supervised applicable-models/requirements/white-box"
  },
  {
    "slug": "monte-carlo-dropout",
    "name": "Monte Carlo Dropout",
    "description": "Monte Carlo Dropout estimates prediction uncertainty by applying dropout (randomly setting neural network weights to zero) during inference rather than just training. It performs multiple forward passes through the network with different random dropout patterns and collects the resulting predictions to form a distribution. Low variance across predictions indicates epistemic certainty (the model is confident), while high variance suggests epistemic uncertainty (the model is unsure). This technique transforms any dropout-trained neural network into a Bayesian approximation for uncertainty quantification.",
    "assurance_goals": [
      "Explainability",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks",
      "applicable-models/paradigm/probabilistic",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/architecture-specific",
      "applicable-models/requirements/model-internals",
      "applicable-models/requirements/probabilistic-output",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/explains/prediction-confidence",
      "assurance-goal-category/explainability/property/consistency",
      "assurance-goal-category/explainability/property/efficiency",
      "assurance-goal-category/explainability/uncertainty-analysis/prediction-uncertainty",
      "assurance-goal-category/reliability",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "searchText": "monte carlo dropout monte carlo dropout estimates prediction uncertainty by applying dropout (randomly setting neural network weights to zero) during inference rather than just training. it performs multiple forward passes through the network with different random dropout patterns and collects the resulting predictions to form a distribution. low variance across predictions indicates epistemic certainty (the model is confident), while high variance suggests epistemic uncertainty (the model is unsure). this technique transforms any dropout-trained neural network into a bayesian approximation for uncertainty quantification. explainability reliability applicable-models/architecture/neural-networks applicable-models/paradigm/probabilistic applicable-models/paradigm/supervised applicable-models/requirements/architecture-specific applicable-models/requirements/model-internals applicable-models/requirements/probabilistic-output assurance-goal-category/explainability assurance-goal-category/explainability/explains/prediction-confidence assurance-goal-category/explainability/property/consistency assurance-goal-category/explainability/property/efficiency assurance-goal-category/explainability/uncertainty-analysis/prediction-uncertainty assurance-goal-category/reliability data-requirements/no-special-requirements data-type/any evidence-type/quantitative-metric expertise-needed/ml-engineering expertise-needed/statistics explanatory-scope/global lifecycle-stage/model-development lifecycle-stage/system-deployment-and-use technique-type/algorithmic"
  },
  {
    "slug": "out-of-distribution-detector-for-neural-networks",
    "name": "Out-of-Distribution Detector for Neural Networks",
    "description": "ODIN (Out-of-Distribution Detector for Neural Networks) identifies when a neural network encounters inputs significantly different from its training distribution. It enhances detection by applying temperature scaling to soften the model's output distribution and adding small, carefully calibrated perturbations to the input that push in-distribution samples towards higher confidence predictions. By measuring the maximum softmax probability after these adjustments, ODIN can effectively distinguish between in-distribution and out-of-distribution inputs, flagging potentially unreliable predictions before they cause downstream errors.",
    "assurance_goals": [
      "Explainability",
      "Reliability",
      "Safety"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks",
      "applicable-models/paradigm/discriminative",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/probabilistic-output",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/attribution-methods/perturbation-based",
      "assurance-goal-category/explainability/explains/decision-boundaries",
      "assurance-goal-category/explainability/explains/prediction-confidence",
      "assurance-goal-category/explainability/property/consistency",
      "assurance-goal-category/explainability/property/fidelity",
      "assurance-goal-category/explainability/uncertainty-analysis/prediction-uncertainty",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "searchText": "out-of-distribution detector for neural networks odin (out-of-distribution detector for neural networks) identifies when a neural network encounters inputs significantly different from its training distribution. it enhances detection by applying temperature scaling to soften the model's output distribution and adding small, carefully calibrated perturbations to the input that push in-distribution samples towards higher confidence predictions. by measuring the maximum softmax probability after these adjustments, odin can effectively distinguish between in-distribution and out-of-distribution inputs, flagging potentially unreliable predictions before they cause downstream errors. explainability reliability safety applicable-models/architecture/neural-networks applicable-models/paradigm/discriminative applicable-models/paradigm/supervised applicable-models/requirements/probabilistic-output applicable-models/requirements/white-box assurance-goal-category/explainability assurance-goal-category/explainability/attribution-methods/perturbation-based assurance-goal-category/explainability/explains/decision-boundaries assurance-goal-category/explainability/explains/prediction-confidence assurance-goal-category/explainability/property/consistency assurance-goal-category/explainability/property/fidelity assurance-goal-category/explainability/uncertainty-analysis/prediction-uncertainty assurance-goal-category/reliability assurance-goal-category/safety data-requirements/no-special-requirements data-type/any evidence-type/quantitative-metric expertise-needed/ml-engineering expertise-needed/statistics explanatory-scope/global lifecycle-stage/model-development technique-type/algorithmic"
  },
  {
    "slug": "permutation-tests",
    "name": "Permutation Tests",
    "description": "Permutation tests assess the statistical significance of observed results (such as model accuracy, feature importance, or group differences) by comparing them to what would occur purely by chance. The technique randomly shuffles labels or data thousands of times, recalculating the metric of interest each time to build an empirical null distribution. If the actual observed result falls in the extreme tail of this distribution (typically beyond the 95th or 99th percentile), it provides strong evidence that the relationship is genuine rather than due to random chance, without requiring parametric assumptions about data distributions.",
    "assurance_goals": [
      "Explainability",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/attribution-methods/perturbation-based",
      "assurance-goal-category/explainability/explains/feature-importance",
      "assurance-goal-category/explainability/property/consistency",
      "assurance-goal-category/explainability/property/fidelity",
      "assurance-goal-category/reliability",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "searchText": "permutation tests permutation tests assess the statistical significance of observed results (such as model accuracy, feature importance, or group differences) by comparing them to what would occur purely by chance. the technique randomly shuffles labels or data thousands of times, recalculating the metric of interest each time to build an empirical null distribution. if the actual observed result falls in the extreme tail of this distribution (typically beyond the 95th or 99th percentile), it provides strong evidence that the relationship is genuine rather than due to random chance, without requiring parametric assumptions about data distributions. explainability reliability applicable-models/architecture/model-agnostic applicable-models/requirements/black-box assurance-goal-category/explainability assurance-goal-category/explainability/attribution-methods/perturbation-based assurance-goal-category/explainability/explains/feature-importance assurance-goal-category/explainability/property/consistency assurance-goal-category/explainability/property/fidelity assurance-goal-category/reliability data-requirements/no-special-requirements data-type/any evidence-type/quantitative-metric expertise-needed/statistics explanatory-scope/global lifecycle-stage/model-development lifecycle-stage/system-deployment-and-use technique-type/algorithmic"
  },
  {
    "slug": "synthetic-data-generation",
    "name": "Synthetic Data Generation",
    "description": "Synthetic data generation creates artificial datasets that aim to preserve the statistical properties, distributions, and relationships of real data whilst containing no actual records from real individuals. The technique encompasses various approaches including generative adversarial networks (GANs), variational autoencoders (VAEs), statistical sampling methods, and privacy-preserving techniques like differential privacy. Beyond privacy protection, synthetic data serves multiple purposes: augmenting limited datasets, balancing class distributions, testing model robustness, enabling data sharing across organisations, and supporting fairness assessments by generating representative samples for underrepresented groups.",
    "assurance_goals": [
      "Privacy",
      "Fairness",
      "Reliability",
      "Safety"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks/generative/gan",
      "applicable-models/architecture/neural-networks/generative/vae",
      "applicable-models/architecture/probabilistic",
      "applicable-models/paradigm/generative",
      "applicable-models/paradigm/unsupervised",
      "applicable-models/requirements/training-data",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/privacy",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/structured-output",
      "expertise-needed/cryptography",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "lifecycle-stage/data-handling",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "searchText": "synthetic data generation synthetic data generation creates artificial datasets that aim to preserve the statistical properties, distributions, and relationships of real data whilst containing no actual records from real individuals. the technique encompasses various approaches including generative adversarial networks (gans), variational autoencoders (vaes), statistical sampling methods, and privacy-preserving techniques like differential privacy. beyond privacy protection, synthetic data serves multiple purposes: augmenting limited datasets, balancing class distributions, testing model robustness, enabling data sharing across organisations, and supporting fairness assessments by generating representative samples for underrepresented groups. privacy fairness reliability safety applicable-models/architecture/neural-networks/generative/gan applicable-models/architecture/neural-networks/generative/vae applicable-models/architecture/probabilistic applicable-models/paradigm/generative applicable-models/paradigm/unsupervised applicable-models/requirements/training-data applicable-models/requirements/white-box assurance-goal-category/fairness assurance-goal-category/privacy assurance-goal-category/reliability assurance-goal-category/safety data-requirements/no-special-requirements data-type/any evidence-type/structured-output expertise-needed/cryptography expertise-needed/ml-engineering expertise-needed/statistics lifecycle-stage/data-handling lifecycle-stage/model-development technique-type/algorithmic"
  },
  {
    "slug": "federated-learning",
    "name": "Federated Learning",
    "description": "Federated learning enables collaborative model training across multiple distributed parties (devices, organisations, or data centres) without requiring centralised data sharing. Participants train models locally on their private datasets and only share model updates (gradients, weights, or aggregated statistics) with a central coordinator. This distributed approach serves multiple purposes: preserving data privacy and sovereignty, reducing communication costs, enabling learning from diverse data sources, improving model robustness through heterogeneous training, and facilitating compliance with data protection regulations whilst maintaining model performance comparable to centralised training.",
    "assurance_goals": [
      "Privacy",
      "Reliability",
      "Safety",
      "Fairness"
    ],
    "tags": [
      "applicable-models/architecture/linear-models",
      "applicable-models/architecture/neural-networks",
      "applicable-models/paradigm/parametric",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/gradient-access",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/privacy",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/software-engineering",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "searchText": "federated learning federated learning enables collaborative model training across multiple distributed parties (devices, organisations, or data centres) without requiring centralised data sharing. participants train models locally on their private datasets and only share model updates (gradients, weights, or aggregated statistics) with a central coordinator. this distributed approach serves multiple purposes: preserving data privacy and sovereignty, reducing communication costs, enabling learning from diverse data sources, improving model robustness through heterogeneous training, and facilitating compliance with data protection regulations whilst maintaining model performance comparable to centralised training. privacy reliability safety fairness applicable-models/architecture/linear-models applicable-models/architecture/neural-networks applicable-models/paradigm/parametric applicable-models/paradigm/supervised applicable-models/requirements/gradient-access applicable-models/requirements/white-box assurance-goal-category/fairness assurance-goal-category/privacy assurance-goal-category/reliability assurance-goal-category/safety data-requirements/no-special-requirements data-type/any evidence-type/quantitative-metric expertise-needed/ml-engineering expertise-needed/software-engineering lifecycle-stage/model-development lifecycle-stage/system-deployment-and-use technique-type/algorithmic"
  },
  {
    "slug": "prediction-intervals",
    "name": "Prediction Intervals",
    "description": "Prediction intervals provide a range of plausible values around a model's prediction, expressing uncertainty as 'the true value will likely fall between X and Y with Z% confidence'. For example, instead of predicting 'house price: £300,000', a prediction interval might say 'house price: £280,000 to £320,000 with 95% confidence'. This technique works by calculating upper and lower bounds that account for both model uncertainty (how confident the model is) and inherent randomness in the data. Prediction intervals are crucial for informed decision-making, as they help users understand the reliability and precision of predictions, enabling better risk assessment and planning.",
    "assurance_goals": [
      "Reliability",
      "Transparency",
      "Fairness"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/reliability",
      "assurance-goal-category/reliability/uncertainty-quantification",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/prediction-interval",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "searchText": "prediction intervals prediction intervals provide a range of plausible values around a model's prediction, expressing uncertainty as 'the true value will likely fall between x and y with z% confidence'. for example, instead of predicting 'house price: £300,000', a prediction interval might say 'house price: £280,000 to £320,000 with 95% confidence'. this technique works by calculating upper and lower bounds that account for both model uncertainty (how confident the model is) and inherent randomness in the data. prediction intervals are crucial for informed decision-making, as they help users understand the reliability and precision of predictions, enabling better risk assessment and planning. reliability transparency fairness applicable-models/architecture/model-agnostic applicable-models/paradigm/supervised applicable-models/requirements/black-box assurance-goal-category/fairness assurance-goal-category/reliability assurance-goal-category/reliability/uncertainty-quantification assurance-goal-category/transparency data-requirements/no-special-requirements data-type/any evidence-type/prediction-interval evidence-type/quantitative-metric expertise-needed/statistics lifecycle-stage/model-development lifecycle-stage/system-deployment-and-use technique-type/algorithmic"
  },
  {
    "slug": "quantile-regression",
    "name": "Quantile Regression",
    "description": "Quantile regression estimates specific percentiles (quantiles) of the target variable rather than just predicting the average outcome. For example, instead of predicting 'average house price = £300,000', it can predict 'there's a 10% chance the price will be below £250,000, 50% chance below £300,000, and 90% chance below £380,000'. This technique reveals how input features affect different parts of the outcome distribution - perhaps property size strongly influences luxury homes (90th percentile) but barely affects budget properties (10th percentile). By capturing the full conditional distribution, quantile regression provides rich uncertainty information and enables robust prediction intervals.",
    "assurance_goals": [
      "Reliability",
      "Transparency",
      "Fairness"
    ],
    "tags": [
      "applicable-models/architecture/linear-models/regression",
      "applicable-models/architecture/neural-networks",
      "applicable-models/architecture/tree-based/gradient-boosting",
      "applicable-models/paradigm/parametric",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/reliability",
      "assurance-goal-category/reliability/uncertainty-quantification",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/prediction-interval",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "searchText": "quantile regression quantile regression estimates specific percentiles (quantiles) of the target variable rather than just predicting the average outcome. for example, instead of predicting 'average house price = £300,000', it can predict 'there's a 10% chance the price will be below £250,000, 50% chance below £300,000, and 90% chance below £380,000'. this technique reveals how input features affect different parts of the outcome distribution - perhaps property size strongly influences luxury homes (90th percentile) but barely affects budget properties (10th percentile). by capturing the full conditional distribution, quantile regression provides rich uncertainty information and enables robust prediction intervals. reliability transparency fairness applicable-models/architecture/linear-models/regression applicable-models/architecture/neural-networks applicable-models/architecture/tree-based/gradient-boosting applicable-models/paradigm/parametric applicable-models/paradigm/supervised applicable-models/requirements/white-box assurance-goal-category/fairness assurance-goal-category/reliability assurance-goal-category/reliability/uncertainty-quantification assurance-goal-category/transparency data-requirements/no-special-requirements data-type/any evidence-type/prediction-interval evidence-type/quantitative-metric expertise-needed/statistics lifecycle-stage/model-development lifecycle-stage/system-deployment-and-use technique-type/algorithmic"
  },
  {
    "slug": "conformal-prediction",
    "name": "Conformal Prediction",
    "description": "Conformal prediction provides mathematically guaranteed uncertainty quantification by creating prediction sets that contain the true outcome with a specified probability (e.g., exactly 95% coverage). The technique works by measuring how 'strange' or 'nonconforming' new predictions are compared to calibration data - if a prediction seems unusual, it gets wider intervals. For example, in medical diagnosis, instead of saying 'likely cancer', it might say 'possible diagnoses: {cancer, benign tumour} with 95% confidence'. This distribution-free method works with any underlying model (neural networks, random forests, etc.) and requires no assumptions about data distribution, making it a robust framework for reliable uncertainty estimates in high-stakes applications.",
    "assurance_goals": [
      "Reliability",
      "Transparency",
      "Fairness"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/reliability",
      "assurance-goal-category/reliability/uncertainty-quantification",
      "assurance-goal-category/transparency",
      "data-requirements/calibration-set",
      "data-type/any",
      "evidence-type/prediction-interval",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "searchText": "conformal prediction conformal prediction provides mathematically guaranteed uncertainty quantification by creating prediction sets that contain the true outcome with a specified probability (e.g., exactly 95% coverage). the technique works by measuring how 'strange' or 'nonconforming' new predictions are compared to calibration data - if a prediction seems unusual, it gets wider intervals. for example, in medical diagnosis, instead of saying 'likely cancer', it might say 'possible diagnoses: {cancer, benign tumour} with 95% confidence'. this distribution-free method works with any underlying model (neural networks, random forests, etc.) and requires no assumptions about data distribution, making it a robust framework for reliable uncertainty estimates in high-stakes applications. reliability transparency fairness applicable-models/architecture/model-agnostic applicable-models/requirements/black-box assurance-goal-category/fairness assurance-goal-category/reliability assurance-goal-category/reliability/uncertainty-quantification assurance-goal-category/transparency data-requirements/calibration-set data-type/any evidence-type/prediction-interval evidence-type/quantitative-metric expertise-needed/ml-engineering expertise-needed/statistics lifecycle-stage/model-development lifecycle-stage/system-deployment-and-use technique-type/algorithmic"
  },
  {
    "slug": "empirical-calibration",
    "name": "Empirical Calibration",
    "description": "Empirical calibration adjusts a model's predicted probabilities to match observed frequencies. For example, if events predicted with 80% confidence only occur 60% of the time, calibration would correct this overconfidence. Common techniques include Platt scaling and isotonic regression, which learn transformations that map the model's raw scores to well-calibrated probabilities, improving the reliability of confidence measures for downstream decisions.",
    "assurance_goals": [
      "Reliability",
      "Transparency",
      "Fairness"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/black-box",
      "applicable-models/requirements/probabilistic-output",
      "assurance-goal-category/fairness",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/calibration-set",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "searchText": "empirical calibration empirical calibration adjusts a model's predicted probabilities to match observed frequencies. for example, if events predicted with 80% confidence only occur 60% of the time, calibration would correct this overconfidence. common techniques include platt scaling and isotonic regression, which learn transformations that map the model's raw scores to well-calibrated probabilities, improving the reliability of confidence measures for downstream decisions. reliability transparency fairness applicable-models/architecture/model-agnostic applicable-models/paradigm/supervised applicable-models/requirements/black-box applicable-models/requirements/probabilistic-output assurance-goal-category/fairness assurance-goal-category/reliability assurance-goal-category/transparency data-requirements/calibration-set data-type/any evidence-type/quantitative-metric expertise-needed/statistics explanatory-scope/global lifecycle-stage/system-deployment-and-use technique-type/algorithmic"
  },
  {
    "slug": "temperature-scaling",
    "name": "Temperature Scaling",
    "description": "Temperature scaling adjusts a model's confidence by applying a single parameter (temperature) to its predictions. When a model is too confident in its wrong answers, temperature scaling can fix this by making the predictions more realistic. It works by dividing the model's outputs by the temperature value before converting them to probabilities. Higher temperatures make the model less confident, whilst lower temperatures increase confidence. The technique maintains the model's accuracy whilst ensuring that when it says it's 90% confident, it's actually right about 90% of the time.",
    "assurance_goals": [
      "Reliability",
      "Transparency",
      "Fairness"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks",
      "applicable-models/paradigm/discriminative",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/probabilistic-output",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/access-to-model-internals",
      "data-requirements/calibration-set",
      "data-requirements/validation-set",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "searchText": "temperature scaling temperature scaling adjusts a model's confidence by applying a single parameter (temperature) to its predictions. when a model is too confident in its wrong answers, temperature scaling can fix this by making the predictions more realistic. it works by dividing the model's outputs by the temperature value before converting them to probabilities. higher temperatures make the model less confident, whilst lower temperatures increase confidence. the technique maintains the model's accuracy whilst ensuring that when it says it's 90% confident, it's actually right about 90% of the time. reliability transparency fairness applicable-models/architecture/neural-networks applicable-models/paradigm/discriminative applicable-models/paradigm/supervised applicable-models/requirements/probabilistic-output applicable-models/requirements/white-box assurance-goal-category/fairness assurance-goal-category/reliability assurance-goal-category/transparency data-requirements/access-to-model-internals data-requirements/calibration-set data-requirements/validation-set data-type/any evidence-type/quantitative-metric expertise-needed/statistics explanatory-scope/global lifecycle-stage/model-development technique-type/algorithmic"
  },
  {
    "slug": "deep-ensembles",
    "name": "Deep Ensembles",
    "description": "Deep ensembles combine predictions from multiple neural networks trained independently with different random initializations to capture epistemic uncertainty (model uncertainty). By training several models on the same data with different starting points, the ensemble reveals how much the model's predictions depend on training randomness. The disagreement between ensemble members naturally indicates prediction uncertainty - when models agree, confidence is high; when they disagree, uncertainty is revealed. This approach provides more reliable uncertainty estimates, better out-of-distribution detection, and improved calibration compared to single models.",
    "assurance_goals": [
      "Reliability",
      "Transparency",
      "Safety"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks",
      "applicable-models/paradigm/parametric",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/training-data",
      "assurance-goal-category/reliability",
      "assurance-goal-category/reliability/uncertainty-quantification",
      "assurance-goal-category/safety",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/prediction-interval",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "searchText": "deep ensembles deep ensembles combine predictions from multiple neural networks trained independently with different random initializations to capture epistemic uncertainty (model uncertainty). by training several models on the same data with different starting points, the ensemble reveals how much the model's predictions depend on training randomness. the disagreement between ensemble members naturally indicates prediction uncertainty - when models agree, confidence is high; when they disagree, uncertainty is revealed. this approach provides more reliable uncertainty estimates, better out-of-distribution detection, and improved calibration compared to single models. reliability transparency safety applicable-models/architecture/neural-networks applicable-models/paradigm/parametric applicable-models/paradigm/supervised applicable-models/requirements/training-data assurance-goal-category/reliability assurance-goal-category/reliability/uncertainty-quantification assurance-goal-category/safety assurance-goal-category/transparency data-requirements/no-special-requirements data-type/any evidence-type/prediction-interval evidence-type/quantitative-metric expertise-needed/ml-engineering expertise-needed/statistics lifecycle-stage/model-development lifecycle-stage/system-deployment-and-use technique-type/algorithmic"
  },
  {
    "slug": "bootstrapping",
    "name": "Bootstrapping",
    "description": "Bootstrapping estimates uncertainty by repeatedly resampling the original dataset with replacement to create many new training sets, training a model on each sample, and analysing the variation in predictions. This approach provides confidence intervals and stability measures without making strong statistical assumptions. By showing how predictions change with different random samples of the data, it reveals how sensitive the model is to the specific training examples and provides robust uncertainty estimates.",
    "assurance_goals": [
      "Reliability",
      "Transparency",
      "Fairness"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/black-box",
      "applicable-models/requirements/training-data",
      "assurance-goal-category/fairness",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/access-to-training-data",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "searchText": "bootstrapping bootstrapping estimates uncertainty by repeatedly resampling the original dataset with replacement to create many new training sets, training a model on each sample, and analysing the variation in predictions. this approach provides confidence intervals and stability measures without making strong statistical assumptions. by showing how predictions change with different random samples of the data, it reveals how sensitive the model is to the specific training examples and provides robust uncertainty estimates. reliability transparency fairness applicable-models/architecture/model-agnostic applicable-models/paradigm/supervised applicable-models/requirements/black-box applicable-models/requirements/training-data assurance-goal-category/fairness assurance-goal-category/reliability assurance-goal-category/transparency data-requirements/access-to-training-data data-type/any evidence-type/quantitative-metric expertise-needed/ml-engineering expertise-needed/statistics explanatory-scope/global lifecycle-stage/model-development technique-type/algorithmic"
  },
  {
    "slug": "jackknife-resampling",
    "name": "Jackknife Resampling",
    "description": "Jackknife resampling (also called leave-one-out resampling) assesses model stability and uncertainty by systematically removing one data point at a time and retraining the model on the remaining data. Unlike bootstrapping which samples with replacement, jackknife creates n different models by excluding each of the n data points once. This systematic approach reveals how individual points influence results, provides robust estimates of prediction variance, and identifies unusually influential observations that may be outliers or leverage points affecting model reliability.",
    "assurance_goals": [
      "Reliability",
      "Transparency",
      "Fairness"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/black-box",
      "applicable-models/requirements/training-data",
      "assurance-goal-category/fairness",
      "assurance-goal-category/reliability",
      "assurance-goal-category/reliability/uncertainty-quantification",
      "assurance-goal-category/transparency",
      "data-requirements/access-to-training-data",
      "data-type/any",
      "evidence-type/prediction-interval",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "searchText": "jackknife resampling jackknife resampling (also called leave-one-out resampling) assesses model stability and uncertainty by systematically removing one data point at a time and retraining the model on the remaining data. unlike bootstrapping which samples with replacement, jackknife creates n different models by excluding each of the n data points once. this systematic approach reveals how individual points influence results, provides robust estimates of prediction variance, and identifies unusually influential observations that may be outliers or leverage points affecting model reliability. reliability transparency fairness applicable-models/architecture/model-agnostic applicable-models/paradigm/supervised applicable-models/requirements/black-box applicable-models/requirements/training-data assurance-goal-category/fairness assurance-goal-category/reliability assurance-goal-category/reliability/uncertainty-quantification assurance-goal-category/transparency data-requirements/access-to-training-data data-type/any evidence-type/prediction-interval evidence-type/quantitative-metric expertise-needed/ml-engineering expertise-needed/statistics lifecycle-stage/model-development lifecycle-stage/system-deployment-and-use technique-type/algorithmic"
  },
  {
    "slug": "cross-validation",
    "name": "Cross-validation",
    "description": "Cross-validation evaluates model performance and robustness by systematically partitioning data into multiple subsets (folds) and training/testing repeatedly on different combinations. Common approaches include k-fold (splitting into k equal parts), stratified (preserving class distributions), and leave-one-out variants. By testing on multiple independent holdout sets, it reveals how performance varies across different data subsamples, provides robust estimates of generalisation ability, and helps detect overfitting or model instability that single train-test splits might miss.",
    "assurance_goals": [
      "Reliability",
      "Transparency",
      "Fairness"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/black-box",
      "applicable-models/requirements/training-data",
      "assurance-goal-category/fairness",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "searchText": "cross-validation cross-validation evaluates model performance and robustness by systematically partitioning data into multiple subsets (folds) and training/testing repeatedly on different combinations. common approaches include k-fold (splitting into k equal parts), stratified (preserving class distributions), and leave-one-out variants. by testing on multiple independent holdout sets, it reveals how performance varies across different data subsamples, provides robust estimates of generalisation ability, and helps detect overfitting or model instability that single train-test splits might miss. reliability transparency fairness applicable-models/architecture/model-agnostic applicable-models/paradigm/supervised applicable-models/requirements/black-box applicable-models/requirements/training-data assurance-goal-category/fairness assurance-goal-category/reliability assurance-goal-category/transparency data-requirements/no-special-requirements data-type/any evidence-type/quantitative-metric expertise-needed/ml-engineering expertise-needed/statistics lifecycle-stage/model-development lifecycle-stage/system-deployment-and-use technique-type/algorithmic"
  },
  {
    "slug": "area-under-precision-recall-curve",
    "name": "Area Under Precision-Recall Curve",
    "description": "Area Under Precision-Recall Curve (AUPRC) measures model performance by plotting precision (the proportion of positive predictions that are correct) against recall (the proportion of actual positives that are correctly identified) at various classification thresholds, then calculating the area under the resulting curve. Unlike accuracy or AUC-ROC, AUPRC is particularly valuable for imbalanced datasets where the minority class is of primary interest---a perfect score is 1.0, whilst random performance equals the positive class proportion. By focusing on the precision-recall trade-off, it provides a more informative assessment than overall accuracy for scenarios where false positives and false negatives have different costs, especially when positive examples are rare.",
    "assurance_goals": [
      "Reliability",
      "Transparency",
      "Fairness"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/black-box",
      "applicable-models/requirements/probabilistic-output",
      "assurance-goal-category/fairness",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/labelled-data",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "searchText": "area under precision-recall curve area under precision-recall curve (auprc) measures model performance by plotting precision (the proportion of positive predictions that are correct) against recall (the proportion of actual positives that are correctly identified) at various classification thresholds, then calculating the area under the resulting curve. unlike accuracy or auc-roc, auprc is particularly valuable for imbalanced datasets where the minority class is of primary interest---a perfect score is 1.0, whilst random performance equals the positive class proportion. by focusing on the precision-recall trade-off, it provides a more informative assessment than overall accuracy for scenarios where false positives and false negatives have different costs, especially when positive examples are rare. reliability transparency fairness applicable-models/architecture/model-agnostic applicable-models/paradigm/supervised applicable-models/requirements/black-box applicable-models/requirements/probabilistic-output assurance-goal-category/fairness assurance-goal-category/reliability assurance-goal-category/transparency data-requirements/labelled-data data-type/any evidence-type/quantitative-metric expertise-needed/ml-engineering expertise-needed/statistics lifecycle-stage/model-development lifecycle-stage/system-deployment-and-use technique-type/algorithmic"
  },
  {
    "slug": "safety-envelope-testing",
    "name": "Safety Envelope Testing",
    "description": "Safety envelope testing systematically evaluates AI system performance at the boundaries of its intended operational domain to identify potential failure modes before deployment. The technique involves defining the system's operational design domain (ODD), creating test scenarios that approach or exceed these boundaries, and measuring performance degradation as conditions become more challenging. By testing edge cases, environmental extremes, and boundary conditions, it reveals where the system transitions from safe to unsafe operation, enabling the establishment of clear operational limits and safety margins for deployment.",
    "assurance_goals": [
      "Safety",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/test-scenarios",
      "data-type/any",
      "evidence-type/boundary-analysis",
      "evidence-type/quantitative-metric",
      "expertise-needed/domain-expertise",
      "expertise-needed/safety-engineering",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/testing"
    ],
    "searchText": "safety envelope testing safety envelope testing systematically evaluates ai system performance at the boundaries of its intended operational domain to identify potential failure modes before deployment. the technique involves defining the system's operational design domain (odd), creating test scenarios that approach or exceed these boundaries, and measuring performance degradation as conditions become more challenging. by testing edge cases, environmental extremes, and boundary conditions, it reveals where the system transitions from safe to unsafe operation, enabling the establishment of clear operational limits and safety margins for deployment. safety reliability applicable-models/architecture/model-agnostic applicable-models/requirements/black-box assurance-goal-category/reliability assurance-goal-category/safety data-requirements/test-scenarios data-type/any evidence-type/boundary-analysis evidence-type/quantitative-metric expertise-needed/domain-expertise expertise-needed/safety-engineering lifecycle-stage/model-development lifecycle-stage/system-deployment-and-use technique-type/testing"
  },
  {
    "slug": "red-teaming",
    "name": "Red Teaming",
    "description": "Red teaming involves systematic adversarial testing of AI/ML systems by dedicated specialists who attempt to identify flaws, vulnerabilities, harmful outputs, and ways to circumvent safety measures. Drawing from cybersecurity practices, red teams employ diverse attack vectors including prompt injection, adversarial examples, edge case exploitation, social engineering scenarios, and goal misalignment probes. Unlike standard testing that validates expected behaviour, red teaming specifically seeks to break systems through creative and adversarial approaches, revealing non-obvious risks and failure modes that could be exploited maliciously or cause harm in deployment.",
    "assurance_goals": [
      "Safety",
      "Reliability",
      "Fairness",
      "Security"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/qualitative-report",
      "expertise-needed/ml-engineering",
      "expertise-needed/security",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/procedural"
    ],
    "searchText": "red teaming red teaming involves systematic adversarial testing of ai/ml systems by dedicated specialists who attempt to identify flaws, vulnerabilities, harmful outputs, and ways to circumvent safety measures. drawing from cybersecurity practices, red teams employ diverse attack vectors including prompt injection, adversarial examples, edge case exploitation, social engineering scenarios, and goal misalignment probes. unlike standard testing that validates expected behaviour, red teaming specifically seeks to break systems through creative and adversarial approaches, revealing non-obvious risks and failure modes that could be exploited maliciously or cause harm in deployment. safety reliability fairness security applicable-models/architecture/model-agnostic applicable-models/requirements/black-box assurance-goal-category/fairness assurance-goal-category/reliability assurance-goal-category/safety data-requirements/no-special-requirements data-type/any evidence-type/qualitative-report expertise-needed/ml-engineering expertise-needed/security lifecycle-stage/system-deployment-and-use technique-type/procedural"
  },
  {
    "slug": "anomaly-detection",
    "name": "Anomaly Detection",
    "description": "Anomaly detection identifies unusual behaviours, inputs, or outputs that deviate significantly from established normal patterns using statistical, machine learning, or rule-based methods. Applied to AI/ML systems, it serves as a continuous monitoring mechanism that can flag unexpected model predictions, suspicious input patterns, data drift, adversarial attacks, or operational malfunctions. By establishing baselines of normal system behaviour and alerting when deviations exceed predefined thresholds, organisations can detect potential security threats, model degradation, fairness violations, or system failures before they cause significant harm.",
    "assurance_goals": [
      "Safety",
      "Reliability",
      "Fairness",
      "Security"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "applicable-models/requirements/training-data",
      "assurance-goal-category/fairness",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "assurance-goal-category/safety/monitoring/anomaly-detection",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "lifecycle-stage/system-deployment-and-use",
      "lifecycle-stage/system-deployment-and-use/monitoring",
      "technique-type/algorithmic"
    ],
    "searchText": "anomaly detection anomaly detection identifies unusual behaviours, inputs, or outputs that deviate significantly from established normal patterns using statistical, machine learning, or rule-based methods. applied to ai/ml systems, it serves as a continuous monitoring mechanism that can flag unexpected model predictions, suspicious input patterns, data drift, adversarial attacks, or operational malfunctions. by establishing baselines of normal system behaviour and alerting when deviations exceed predefined thresholds, organisations can detect potential security threats, model degradation, fairness violations, or system failures before they cause significant harm. safety reliability fairness security applicable-models/architecture/model-agnostic applicable-models/requirements/black-box applicable-models/requirements/training-data assurance-goal-category/fairness assurance-goal-category/reliability assurance-goal-category/safety assurance-goal-category/safety/monitoring/anomaly-detection data-requirements/no-special-requirements data-type/any evidence-type/quantitative-metric expertise-needed/ml-engineering expertise-needed/statistics lifecycle-stage/system-deployment-and-use lifecycle-stage/system-deployment-and-use/monitoring technique-type/algorithmic"
  },
  {
    "slug": "confidence-thresholding",
    "name": "Confidence Thresholding",
    "description": "Confidence thresholding creates decision boundaries based on model uncertainty scores, routing predictions into different handling workflows depending on their confidence levels. High-confidence predictions (e.g., above 95%) proceed automatically, whilst medium-confidence cases (e.g., 70-95%) may trigger additional validation or human review, and low-confidence predictions (below 70%) receive extensive oversight or default to conservative fallback actions. This technique enables organisations to maintain automated efficiency for clear-cut cases whilst ensuring appropriate human intervention for uncertain decisions, balancing operational speed with risk management across safety-critical applications.",
    "assurance_goals": [
      "Safety",
      "Reliability",
      "Transparency"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "applicable-models/requirements/probabilistic-output",
      "assurance-goal-category/reliability",
      "assurance-goal-category/reliability/uncertainty-quantification",
      "assurance-goal-category/safety",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "searchText": "confidence thresholding confidence thresholding creates decision boundaries based on model uncertainty scores, routing predictions into different handling workflows depending on their confidence levels. high-confidence predictions (e.g., above 95%) proceed automatically, whilst medium-confidence cases (e.g., 70-95%) may trigger additional validation or human review, and low-confidence predictions (below 70%) receive extensive oversight or default to conservative fallback actions. this technique enables organisations to maintain automated efficiency for clear-cut cases whilst ensuring appropriate human intervention for uncertain decisions, balancing operational speed with risk management across safety-critical applications. safety reliability transparency applicable-models/architecture/model-agnostic applicable-models/requirements/black-box applicable-models/requirements/probabilistic-output assurance-goal-category/reliability assurance-goal-category/reliability/uncertainty-quantification assurance-goal-category/safety assurance-goal-category/transparency data-requirements/no-special-requirements data-type/any evidence-type/quantitative-metric expertise-needed/ml-engineering expertise-needed/statistics lifecycle-stage/system-deployment-and-use technique-type/algorithmic"
  },
  {
    "slug": "runtime-monitoring-and-circuit-breakers",
    "name": "Runtime Monitoring and Circuit Breakers",
    "description": "Runtime monitoring and circuit breakers establish continuous surveillance of AI/ML systems in production, tracking critical metrics such as prediction accuracy, response times, input characteristics, output distributions, and system resource usage. When monitored parameters exceed predefined safety thresholds or exhibit anomalous patterns, automated circuit breakers immediately trigger protective actions including request throttling, service degradation, system shutdown, or failover to backup mechanisms. This approach provides real-time defensive capabilities that prevent cascading failures, ensure consistent service reliability, and maintain transparent operation status for stakeholders monitoring system health.",
    "assurance_goals": [
      "Safety",
      "Reliability",
      "Transparency"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "assurance-goal-category/safety/monitoring/anomaly-detection",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/software-engineering",
      "lifecycle-stage/system-deployment-and-use",
      "lifecycle-stage/system-deployment-and-use/monitoring",
      "technique-type/algorithmic"
    ],
    "searchText": "runtime monitoring and circuit breakers runtime monitoring and circuit breakers establish continuous surveillance of ai/ml systems in production, tracking critical metrics such as prediction accuracy, response times, input characteristics, output distributions, and system resource usage. when monitored parameters exceed predefined safety thresholds or exhibit anomalous patterns, automated circuit breakers immediately trigger protective actions including request throttling, service degradation, system shutdown, or failover to backup mechanisms. this approach provides real-time defensive capabilities that prevent cascading failures, ensure consistent service reliability, and maintain transparent operation status for stakeholders monitoring system health. safety reliability transparency applicable-models/architecture/model-agnostic applicable-models/requirements/black-box assurance-goal-category/reliability assurance-goal-category/safety assurance-goal-category/safety/monitoring/anomaly-detection assurance-goal-category/transparency data-requirements/no-special-requirements data-type/any evidence-type/quantitative-metric expertise-needed/ml-engineering expertise-needed/software-engineering lifecycle-stage/system-deployment-and-use lifecycle-stage/system-deployment-and-use/monitoring technique-type/algorithmic"
  },
  {
    "slug": "mlflow-experiment-tracking",
    "name": "MLflow Experiment Tracking",
    "description": "MLflow is an open-source platform that tracks machine learning experiments by automatically logging parameters, metrics, models, and artifacts throughout the ML lifecycle. It provides a centralised repository for comparing different experimental runs, reproducing results, and managing model versions. Teams can track hyperparameters, evaluation metrics, model files, and execution environment details, creating a comprehensive audit trail that supports collaboration, reproducibility, and regulatory compliance across the entire machine learning development process.",
    "assurance_goals": [
      "Transparency",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/documentation",
      "expertise-needed/ml-engineering",
      "expertise-needed/software-engineering",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/process"
    ],
    "searchText": "mlflow experiment tracking mlflow is an open-source platform that tracks machine learning experiments by automatically logging parameters, metrics, models, and artifacts throughout the ml lifecycle. it provides a centralised repository for comparing different experimental runs, reproducing results, and managing model versions. teams can track hyperparameters, evaluation metrics, model files, and execution environment details, creating a comprehensive audit trail that supports collaboration, reproducibility, and regulatory compliance across the entire machine learning development process. transparency reliability applicable-models/architecture/model-agnostic applicable-models/requirements/black-box assurance-goal-category/reliability assurance-goal-category/transparency data-requirements/no-special-requirements data-type/any evidence-type/documentation expertise-needed/ml-engineering expertise-needed/software-engineering lifecycle-stage/model-development lifecycle-stage/system-deployment-and-use technique-type/process"
  },
  {
    "slug": "data-version-control",
    "name": "Data Version Control",
    "description": "Data Version Control (DVC) is a Git-like version control system specifically designed for machine learning data, models, and experiments. It tracks changes to large data files, maintains reproducible ML pipelines, and creates a complete audit trail of data transformations, model training, and evaluation processes. DVC works alongside Git to provide end-to-end lineage tracking from raw data through preprocessing, training, and deployment, enabling teams to reproduce any model version and understand exactly how datasets evolved throughout the ML lifecycle.",
    "assurance_goals": [
      "Transparency",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/documentation",
      "expertise-needed/ml-engineering",
      "expertise-needed/software-engineering",
      "lifecycle-stage/data-handling",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/process"
    ],
    "searchText": "data version control data version control (dvc) is a git-like version control system specifically designed for machine learning data, models, and experiments. it tracks changes to large data files, maintains reproducible ml pipelines, and creates a complete audit trail of data transformations, model training, and evaluation processes. dvc works alongside git to provide end-to-end lineage tracking from raw data through preprocessing, training, and deployment, enabling teams to reproduce any model version and understand exactly how datasets evolved throughout the ml lifecycle. transparency reliability applicable-models/architecture/model-agnostic applicable-models/requirements/black-box assurance-goal-category/reliability assurance-goal-category/transparency data-requirements/no-special-requirements data-type/any evidence-type/documentation expertise-needed/ml-engineering expertise-needed/software-engineering lifecycle-stage/data-handling lifecycle-stage/model-development lifecycle-stage/system-deployment-and-use technique-type/process"
  },
  {
    "slug": "automated-documentation-generation",
    "name": "Automated Documentation Generation",
    "description": "Automated documentation generation creates and maintains up-to-date documentation using various methods including programmatic scripts, large language models (LLMs), and extraction tools. These approaches can capture model architectures, data schemas, feature importance, performance metrics, API specifications, and lineage information without manual writing. Methods range from traditional code parsing and template-based generation to modern AI-assisted documentation that can understand context and generate human-readable explanations.",
    "assurance_goals": [
      "Transparency",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "assurance-goal-category/transparency/documentation",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/documentation",
      "expertise-needed/software-engineering",
      "lifecycle-stage/deployment",
      "lifecycle-stage/model-development",
      "lifecycle-stage/project-design",
      "technique-type/algorithmic"
    ],
    "searchText": "automated documentation generation automated documentation generation creates and maintains up-to-date documentation using various methods including programmatic scripts, large language models (llms), and extraction tools. these approaches can capture model architectures, data schemas, feature importance, performance metrics, api specifications, and lineage information without manual writing. methods range from traditional code parsing and template-based generation to modern ai-assisted documentation that can understand context and generate human-readable explanations. transparency reliability applicable-models/architecture/model-agnostic applicable-models/requirements/black-box assurance-goal-category/reliability assurance-goal-category/transparency assurance-goal-category/transparency/documentation data-requirements/no-special-requirements data-type/any evidence-type/documentation expertise-needed/software-engineering lifecycle-stage/deployment lifecycle-stage/model-development lifecycle-stage/project-design technique-type/algorithmic"
  },
  {
    "slug": "model-distillation",
    "name": "Model Distillation",
    "description": "Model distillation transfers knowledge from a large, complex model (teacher) to a smaller, more efficient model (student) by training the student to mimic the teacher's behaviour. The student learns from the teacher's soft predictions and intermediate representations rather than just hard labels, capturing nuanced decision boundaries and uncertainty. This produces models that are faster, require less memory, and are often more interpretable whilst maintaining much of the original performance. Beyond compression, distillation can improve model reliability by regularising training and enable deployment in resource-constrained environments.",
    "assurance_goals": [
      "Explainability",
      "Reliability",
      "Safety"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks",
      "applicable-models/paradigm/parametric",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/training-data",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/explains/decision-boundaries",
      "assurance-goal-category/explainability/explains/internal-mechanisms",
      "assurance-goal-category/explainability/model-simplification/knowledge-transfer",
      "assurance-goal-category/explainability/property/comprehensibility",
      "assurance-goal-category/explainability/property/fidelity",
      "assurance-goal-category/explainability/surrogate-models/global-surrogates",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/access-to-training-data",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/deployment",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "searchText": "model distillation model distillation transfers knowledge from a large, complex model (teacher) to a smaller, more efficient model (student) by training the student to mimic the teacher's behaviour. the student learns from the teacher's soft predictions and intermediate representations rather than just hard labels, capturing nuanced decision boundaries and uncertainty. this produces models that are faster, require less memory, and are often more interpretable whilst maintaining much of the original performance. beyond compression, distillation can improve model reliability by regularising training and enable deployment in resource-constrained environments. explainability reliability safety applicable-models/architecture/neural-networks applicable-models/paradigm/parametric applicable-models/paradigm/supervised applicable-models/requirements/training-data applicable-models/requirements/white-box assurance-goal-category/explainability assurance-goal-category/explainability/explains/decision-boundaries assurance-goal-category/explainability/explains/internal-mechanisms assurance-goal-category/explainability/model-simplification/knowledge-transfer assurance-goal-category/explainability/property/comprehensibility assurance-goal-category/explainability/property/fidelity assurance-goal-category/explainability/surrogate-models/global-surrogates assurance-goal-category/reliability assurance-goal-category/safety data-requirements/access-to-training-data data-type/any evidence-type/quantitative-metric expertise-needed/ml-engineering lifecycle-stage/deployment lifecycle-stage/model-development technique-type/algorithmic"
  },
  {
    "slug": "monotonicity-constraints",
    "name": "Monotonicity Constraints",
    "description": "Monotonicity constraints enforce consistent directional relationships between input features and model predictions, ensuring that increasing a feature value either always increases, always decreases, or has no effect on the output. These constraints integrate domain knowledge into model training, preventing counterintuitive relationships that may arise from spurious correlations in data. By maintaining logical feature relationships (e.g., experience always positively influences salary), monotonicity constraints enhance model trustworthiness, interpretability, and alignment with business logic whilst often improving generalisation to new data.",
    "assurance_goals": [
      "Transparency",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/probabilistic/gaussian-processes",
      "applicable-models/architecture/tree-based",
      "applicable-models/requirements/model-internals",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/labelled-data",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/domain-knowledge",
      "expertise-needed/statistics",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "searchText": "monotonicity constraints monotonicity constraints enforce consistent directional relationships between input features and model predictions, ensuring that increasing a feature value either always increases, always decreases, or has no effect on the output. these constraints integrate domain knowledge into model training, preventing counterintuitive relationships that may arise from spurious correlations in data. by maintaining logical feature relationships (e.g., experience always positively influences salary), monotonicity constraints enhance model trustworthiness, interpretability, and alignment with business logic whilst often improving generalisation to new data. transparency reliability applicable-models/architecture/probabilistic/gaussian-processes applicable-models/architecture/tree-based applicable-models/requirements/model-internals applicable-models/requirements/white-box assurance-goal-category/reliability assurance-goal-category/transparency data-requirements/labelled-data data-type/tabular evidence-type/quantitative-metric expertise-needed/domain-knowledge expertise-needed/statistics lifecycle-stage/model-development technique-type/algorithmic"
  },
  {
    "slug": "intrinsically-interpretable-models",
    "name": "Intrinsically Interpretable Models",
    "description": "Intrinsically interpretable models are machine learning algorithms that are transparent by design, allowing users to understand their decision-making process without requiring additional explanation techniques. This category includes decision trees and rule lists (which use if-then logic), linear and logistic regression models (which use weighted feature combinations), and other simple algorithms where the model structure itself provides interpretability. These models prioritise transparency over complexity, making them ideal when stakeholder understanding and regulatory compliance are paramount.",
    "assurance_goals": [
      "Transparency",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/linear-models",
      "applicable-models/architecture/tree-based",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "evidence-type/structured-output",
      "expertise-needed/low",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "lifecycle-stage/project-design",
      "technique-type/algorithmic"
    ],
    "searchText": "intrinsically interpretable models intrinsically interpretable models are machine learning algorithms that are transparent by design, allowing users to understand their decision-making process without requiring additional explanation techniques. this category includes decision trees and rule lists (which use if-then logic), linear and logistic regression models (which use weighted feature combinations), and other simple algorithms where the model structure itself provides interpretability. these models prioritise transparency over complexity, making them ideal when stakeholder understanding and regulatory compliance are paramount. transparency reliability applicable-models/architecture/linear-models applicable-models/architecture/tree-based applicable-models/paradigm/supervised applicable-models/requirements/white-box assurance-goal-category/reliability assurance-goal-category/transparency data-requirements/no-special-requirements data-type/any evidence-type/quantitative-metric evidence-type/structured-output expertise-needed/low explanatory-scope/global lifecycle-stage/model-development lifecycle-stage/project-design technique-type/algorithmic"
  },
  {
    "slug": "generalized-additive-models",
    "name": "Generalized Additive Models",
    "description": "An intrinsically interpretable modelling technique that extends linear models by allowing flexible, nonlinear relationships between individual features and the target whilst maintaining the additive structure that preserves transparency. Each feature's effect is modelled separately as a smooth function, visualised as a curve showing how the feature influences predictions across its range. GAMs achieve this through spline functions or other smoothing techniques that capture complex patterns in individual variables without interactions, making them particularly valuable for domains requiring both predictive accuracy and model interpretability.",
    "assurance_goals": [
      "Transparency",
      "Explainability",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/linear-models/gam",
      "applicable-models/paradigm/parametric",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/explains/data-patterns",
      "assurance-goal-category/explainability/explains/feature-importance",
      "assurance-goal-category/explainability/property/comprehensibility",
      "assurance-goal-category/explainability/property/fidelity",
      "assurance-goal-category/explainability/property/sparsity",
      "assurance-goal-category/explainability/surrogate-models/global-surrogates",
      "assurance-goal-category/explainability/visualization-methods/feature-relationships",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/labelled-data",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "evidence-type/visualisation",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "lifecycle-stage/project-design",
      "technique-type/algorithmic"
    ],
    "searchText": "generalized additive models an intrinsically interpretable modelling technique that extends linear models by allowing flexible, nonlinear relationships between individual features and the target whilst maintaining the additive structure that preserves transparency. each feature's effect is modelled separately as a smooth function, visualised as a curve showing how the feature influences predictions across its range. gams achieve this through spline functions or other smoothing techniques that capture complex patterns in individual variables without interactions, making them particularly valuable for domains requiring both predictive accuracy and model interpretability. transparency explainability reliability applicable-models/architecture/linear-models/gam applicable-models/paradigm/parametric applicable-models/paradigm/supervised applicable-models/requirements/white-box assurance-goal-category/explainability assurance-goal-category/explainability/explains/data-patterns assurance-goal-category/explainability/explains/feature-importance assurance-goal-category/explainability/property/comprehensibility assurance-goal-category/explainability/property/fidelity assurance-goal-category/explainability/property/sparsity assurance-goal-category/explainability/surrogate-models/global-surrogates assurance-goal-category/explainability/visualization-methods/feature-relationships assurance-goal-category/reliability assurance-goal-category/transparency data-requirements/labelled-data data-type/tabular evidence-type/quantitative-metric evidence-type/visualisation expertise-needed/statistics explanatory-scope/global lifecycle-stage/model-development lifecycle-stage/project-design technique-type/algorithmic"
  },
  {
    "slug": "model-pruning",
    "name": "Model Pruning",
    "description": "Model pruning systematically removes less important weights, neurons, or entire layers from neural networks to create smaller, more efficient models whilst maintaining performance. This process involves iterative removal based on importance criteria (weight magnitudes, gradient information, activation patterns) followed by fine-tuning. Pruning can be structured (removing entire neurons/channels) or unstructured (removing individual weights), with structured pruning providing greater computational benefits and interpretability through simplified architectures.",
    "assurance_goals": [
      "Explainability",
      "Reliability",
      "Safety"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks",
      "applicable-models/paradigm/parametric",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/gradient-access",
      "applicable-models/requirements/model-internals",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/explains/feature-importance",
      "assurance-goal-category/explainability/explains/internal-mechanisms",
      "assurance-goal-category/explainability/model-simplification/pruning",
      "assurance-goal-category/explainability/property/comprehensibility",
      "assurance-goal-category/explainability/property/sparsity",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/access-to-model-internals",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/model-development",
      "lifecycle-stage/model-optimization",
      "technique-type/algorithmic"
    ],
    "searchText": "model pruning model pruning systematically removes less important weights, neurons, or entire layers from neural networks to create smaller, more efficient models whilst maintaining performance. this process involves iterative removal based on importance criteria (weight magnitudes, gradient information, activation patterns) followed by fine-tuning. pruning can be structured (removing entire neurons/channels) or unstructured (removing individual weights), with structured pruning providing greater computational benefits and interpretability through simplified architectures. explainability reliability safety applicable-models/architecture/neural-networks applicable-models/paradigm/parametric applicable-models/paradigm/supervised applicable-models/requirements/gradient-access applicable-models/requirements/model-internals applicable-models/requirements/white-box assurance-goal-category/explainability assurance-goal-category/explainability/explains/feature-importance assurance-goal-category/explainability/explains/internal-mechanisms assurance-goal-category/explainability/model-simplification/pruning assurance-goal-category/explainability/property/comprehensibility assurance-goal-category/explainability/property/sparsity assurance-goal-category/reliability assurance-goal-category/safety data-requirements/access-to-model-internals data-type/any evidence-type/quantitative-metric expertise-needed/ml-engineering lifecycle-stage/model-development lifecycle-stage/model-optimization technique-type/algorithmic"
  },
  {
    "slug": "prompt-sensitivity-analysis",
    "name": "Prompt Sensitivity Analysis",
    "description": "Prompt Sensitivity Analysis systematically evaluates how variations in input prompts affect large language model outputs, providing insights into model robustness, consistency, and interpretability. This technique involves creating controlled perturbations of prompts whilst maintaining semantic meaning, then measuring how these changes influence model responses. It encompasses various types of prompt modifications including lexical substitutions, syntactic restructuring, formatting changes, and contextual variations. The analysis typically quantifies sensitivity through metrics such as output consistency, semantic similarity, and statistical measures of variance across prompt variations.",
    "assurance_goals": [
      "Explainability",
      "Reliability",
      "Safety"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks/transformer/llm",
      "applicable-models/paradigm/generative",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/attribution-methods/perturbation-based",
      "assurance-goal-category/explainability/explains/internal-mechanisms",
      "assurance-goal-category/explainability/explains/prediction-confidence",
      "assurance-goal-category/explainability/property/consistency",
      "assurance-goal-category/explainability/property/fidelity",
      "assurance-goal-category/explainability/uncertainty-analysis/sensitivity-testing",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/no-special-requirements",
      "data-type/text",
      "evidence-type/quantitative-metric",
      "expertise-needed/experimental-design",
      "expertise-needed/linguistics",
      "expertise-needed/statistics",
      "lifecycle-stage/model-development",
      "lifecycle-stage/model-evaluation",
      "technique-type/experimental"
    ],
    "searchText": "prompt sensitivity analysis prompt sensitivity analysis systematically evaluates how variations in input prompts affect large language model outputs, providing insights into model robustness, consistency, and interpretability. this technique involves creating controlled perturbations of prompts whilst maintaining semantic meaning, then measuring how these changes influence model responses. it encompasses various types of prompt modifications including lexical substitutions, syntactic restructuring, formatting changes, and contextual variations. the analysis typically quantifies sensitivity through metrics such as output consistency, semantic similarity, and statistical measures of variance across prompt variations. explainability reliability safety applicable-models/architecture/neural-networks/transformer/llm applicable-models/paradigm/generative applicable-models/requirements/black-box assurance-goal-category/explainability assurance-goal-category/explainability/attribution-methods/perturbation-based assurance-goal-category/explainability/explains/internal-mechanisms assurance-goal-category/explainability/explains/prediction-confidence assurance-goal-category/explainability/property/consistency assurance-goal-category/explainability/property/fidelity assurance-goal-category/explainability/uncertainty-analysis/sensitivity-testing assurance-goal-category/reliability assurance-goal-category/safety data-requirements/no-special-requirements data-type/text evidence-type/quantitative-metric expertise-needed/experimental-design expertise-needed/linguistics expertise-needed/statistics lifecycle-stage/model-development lifecycle-stage/model-evaluation technique-type/experimental"
  },
  {
    "slug": "causal-mediation-analysis-in-language-models",
    "name": "Causal Mediation Analysis in Language Models",
    "description": "Causal mediation analysis in language models is a mechanistic interpretability technique that systematically investigates how specific internal components (neurons, attention heads, or layers) causally contribute to model outputs. By performing controlled interventions—such as activating, deactivating, or modifying specific components—researchers can trace the causal pathways through which information flows and transforms within the model. This approach goes beyond correlation to establish causal relationships, enabling researchers to understand not just what features influence outputs, but how and why they do so through specific computational pathways.",
    "assurance_goals": [
      "Explainability",
      "Reliability",
      "Safety"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks/transformer",
      "applicable-models/architecture/neural-networks/transformer/llm",
      "applicable-models/paradigm/generative",
      "applicable-models/requirements/model-internals",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/causal-analysis/mediation-analysis",
      "assurance-goal-category/explainability/explains/causal-pathways",
      "assurance-goal-category/explainability/explains/internal-mechanisms",
      "assurance-goal-category/explainability/property/causality",
      "assurance-goal-category/explainability/property/fidelity",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/access-to-model-internals",
      "data-type/text",
      "evidence-type/causal-analysis",
      "expertise-needed/causal-inference",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/model-development",
      "lifecycle-stage/post-deployment",
      "technique-type/mechanistic-interpretability"
    ],
    "searchText": "causal mediation analysis in language models causal mediation analysis in language models is a mechanistic interpretability technique that systematically investigates how specific internal components (neurons, attention heads, or layers) causally contribute to model outputs. by performing controlled interventions—such as activating, deactivating, or modifying specific components—researchers can trace the causal pathways through which information flows and transforms within the model. this approach goes beyond correlation to establish causal relationships, enabling researchers to understand not just what features influence outputs, but how and why they do so through specific computational pathways. explainability reliability safety applicable-models/architecture/neural-networks/transformer applicable-models/architecture/neural-networks/transformer/llm applicable-models/paradigm/generative applicable-models/requirements/model-internals applicable-models/requirements/white-box assurance-goal-category/explainability assurance-goal-category/explainability/causal-analysis/mediation-analysis assurance-goal-category/explainability/explains/causal-pathways assurance-goal-category/explainability/explains/internal-mechanisms assurance-goal-category/explainability/property/causality assurance-goal-category/explainability/property/fidelity assurance-goal-category/reliability assurance-goal-category/safety data-requirements/access-to-model-internals data-type/text evidence-type/causal-analysis expertise-needed/causal-inference expertise-needed/ml-engineering lifecycle-stage/model-development lifecycle-stage/post-deployment technique-type/mechanistic-interpretability"
  },
  {
    "slug": "reweighing",
    "name": "Reweighing",
    "description": "Reweighing is a pre-processing technique that mitigates bias by assigning different weights to training examples based on their group membership and class label. The weights are calculated to ensure that privileged and unprivileged groups have equal influence on the model's training process, effectively balancing the dataset without altering the feature values themselves. This helps to train fairer models by correcting for historical imbalances in how different groups are represented in the data.",
    "assurance_goals": [
      "Fairness",
      "Transparency",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/black-box",
      "applicable-models/requirements/training-data",
      "assurance-goal-category/fairness",
      "assurance-goal-category/fairness/group/statistical-parity",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/access-to-training-data",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/fairness-metric",
      "expertise-needed/low",
      "fairness-approach/group",
      "lifecycle-stage/data-handling/preprocessing",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "searchText": "reweighing reweighing is a pre-processing technique that mitigates bias by assigning different weights to training examples based on their group membership and class label. the weights are calculated to ensure that privileged and unprivileged groups have equal influence on the model's training process, effectively balancing the dataset without altering the feature values themselves. this helps to train fairer models by correcting for historical imbalances in how different groups are represented in the data. fairness transparency reliability applicable-models/architecture/model-agnostic applicable-models/paradigm/supervised applicable-models/requirements/black-box applicable-models/requirements/training-data assurance-goal-category/fairness assurance-goal-category/fairness/group/statistical-parity assurance-goal-category/reliability assurance-goal-category/transparency data-requirements/access-to-training-data data-requirements/sensitive-attributes data-type/any evidence-type/fairness-metric expertise-needed/low fairness-approach/group lifecycle-stage/data-handling/preprocessing lifecycle-stage/model-development technique-type/algorithmic"
  },
  {
    "slug": "disparate-impact-remover",
    "name": "Disparate Impact Remover",
    "description": "Disparate Impact Remover is a preprocessing technique that transforms feature values in a dataset to reduce statistical dependence between features and protected attributes (like race or gender). The method modifies non-protected features through mathematical transformations that preserve the utility of the data whilst reducing correlations that could lead to discriminatory outcomes. This approach specifically targets the '80% rule' disparate impact threshold by adjusting feature distributions to ensure more equitable treatment across demographic groups in downstream model predictions.",
    "assurance_goals": [
      "Fairness",
      "Transparency",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/black-box",
      "applicable-models/requirements/training-data",
      "assurance-goal-category/fairness",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/sensitive-attributes",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "searchText": "disparate impact remover disparate impact remover is a preprocessing technique that transforms feature values in a dataset to reduce statistical dependence between features and protected attributes (like race or gender). the method modifies non-protected features through mathematical transformations that preserve the utility of the data whilst reducing correlations that could lead to discriminatory outcomes. this approach specifically targets the '80% rule' disparate impact threshold by adjusting feature distributions to ensure more equitable treatment across demographic groups in downstream model predictions. fairness transparency reliability applicable-models/architecture/model-agnostic applicable-models/paradigm/supervised applicable-models/requirements/black-box applicable-models/requirements/training-data assurance-goal-category/fairness assurance-goal-category/reliability assurance-goal-category/transparency data-requirements/sensitive-attributes data-type/tabular evidence-type/quantitative-metric expertise-needed/statistics fairness-approach/group lifecycle-stage/model-development technique-type/algorithmic"
  },
  {
    "slug": "fairness-gan",
    "name": "Fairness GAN",
    "description": "A data generation technique that employs Generative Adversarial Networks (GANs) to create fair synthetic datasets by learning to generate data representations that preserve utility whilst obfuscating protected attributes. Unlike traditional GANs, Fairness GANs incorporate fairness constraints into the training objective, ensuring that the generated data maintains statistical parity across demographic groups. The technique can be used for data augmentation to balance underrepresented groups or to create privacy-preserving synthetic datasets that remove demographic bias from training data.",
    "assurance_goals": [
      "Fairness",
      "Privacy",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks/generative/gan",
      "applicable-models/paradigm/generative",
      "applicable-models/paradigm/unsupervised",
      "applicable-models/requirements/gradient-access",
      "applicable-models/requirements/training-data",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/fairness/group",
      "assurance-goal-category/privacy",
      "assurance-goal-category/reliability",
      "data-requirements/labelled-data",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/fairness-metric",
      "evidence-type/quantitative-metric",
      "evidence-type/synthetic-data",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/data-collection",
      "lifecycle-stage/data-collection/data-augmentation",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "searchText": "fairness gan a data generation technique that employs generative adversarial networks (gans) to create fair synthetic datasets by learning to generate data representations that preserve utility whilst obfuscating protected attributes. unlike traditional gans, fairness gans incorporate fairness constraints into the training objective, ensuring that the generated data maintains statistical parity across demographic groups. the technique can be used for data augmentation to balance underrepresented groups or to create privacy-preserving synthetic datasets that remove demographic bias from training data. fairness privacy reliability applicable-models/architecture/neural-networks/generative/gan applicable-models/paradigm/generative applicable-models/paradigm/unsupervised applicable-models/requirements/gradient-access applicable-models/requirements/training-data applicable-models/requirements/white-box assurance-goal-category/fairness assurance-goal-category/fairness/group assurance-goal-category/privacy assurance-goal-category/reliability data-requirements/labelled-data data-requirements/sensitive-attributes data-type/any evidence-type/fairness-metric evidence-type/quantitative-metric evidence-type/synthetic-data expertise-needed/ml-engineering expertise-needed/statistics fairness-approach/group lifecycle-stage/data-collection lifecycle-stage/data-collection/data-augmentation lifecycle-stage/model-development technique-type/algorithmic"
  },
  {
    "slug": "relabelling",
    "name": "Relabelling",
    "description": "A preprocessing fairness technique that modifies class labels in training data to achieve equal positive outcome rates across protected groups. Also known as 'data massaging', this method identifies instances that contribute to discriminatory patterns and flips their labels (from positive to negative or vice versa) to balance the proportion of positive outcomes between demographic groups. The technique aims to remove historical bias from training data whilst preserving the overall class distribution, enabling standard classifiers to learn from discrimination-free datasets.",
    "assurance_goals": [
      "Fairness",
      "Transparency",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/black-box",
      "applicable-models/requirements/training-data",
      "assurance-goal-category/fairness",
      "assurance-goal-category/fairness/group",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/access-to-training-data",
      "data-requirements/labelled-data",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/dataset-analysis",
      "evidence-type/quantitative-metric",
      "expertise-needed/domain-knowledge",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/data-collection",
      "lifecycle-stage/data-collection/data-preprocessing",
      "lifecycle-stage/model-development",
      "technique-type/procedural"
    ],
    "searchText": "relabelling a preprocessing fairness technique that modifies class labels in training data to achieve equal positive outcome rates across protected groups. also known as 'data massaging', this method identifies instances that contribute to discriminatory patterns and flips their labels (from positive to negative or vice versa) to balance the proportion of positive outcomes between demographic groups. the technique aims to remove historical bias from training data whilst preserving the overall class distribution, enabling standard classifiers to learn from discrimination-free datasets. fairness transparency reliability applicable-models/architecture/model-agnostic applicable-models/paradigm/supervised applicable-models/requirements/black-box applicable-models/requirements/training-data assurance-goal-category/fairness assurance-goal-category/fairness/group assurance-goal-category/reliability assurance-goal-category/transparency data-requirements/access-to-training-data data-requirements/labelled-data data-requirements/sensitive-attributes data-type/any evidence-type/dataset-analysis evidence-type/quantitative-metric expertise-needed/domain-knowledge expertise-needed/statistics fairness-approach/group lifecycle-stage/data-collection lifecycle-stage/data-collection/data-preprocessing lifecycle-stage/model-development technique-type/procedural"
  },
  {
    "slug": "preferential-sampling",
    "name": "Preferential Sampling",
    "description": "A preprocessing fairness technique developed by Kamiran and Calders that addresses dataset imbalances by re-sampling training data with preference for underrepresented groups to achieve discrimination-free classification. This method modifies the training distribution by prioritising borderline objects (instances near decision boundaries) from underrepresented groups for duplication whilst potentially removing instances from overrepresented groups. Unlike relabelling approaches, preferential sampling maintains original class labels whilst creating a more balanced dataset that prevents models from learning biased patterns due to skewed group representation.",
    "assurance_goals": [
      "Fairness",
      "Reliability",
      "Transparency"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/black-box",
      "applicable-models/requirements/training-data",
      "assurance-goal-category/fairness",
      "assurance-goal-category/fairness/group",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/labelled-data",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/dataset-analysis",
      "evidence-type/quantitative-metric",
      "expertise-needed/low",
      "fairness-approach/group",
      "lifecycle-stage/data-collection",
      "lifecycle-stage/data-collection/data-preprocessing",
      "lifecycle-stage/model-development",
      "technique-type/procedural"
    ],
    "searchText": "preferential sampling a preprocessing fairness technique developed by kamiran and calders that addresses dataset imbalances by re-sampling training data with preference for underrepresented groups to achieve discrimination-free classification. this method modifies the training distribution by prioritising borderline objects (instances near decision boundaries) from underrepresented groups for duplication whilst potentially removing instances from overrepresented groups. unlike relabelling approaches, preferential sampling maintains original class labels whilst creating a more balanced dataset that prevents models from learning biased patterns due to skewed group representation. fairness reliability transparency applicable-models/architecture/model-agnostic applicable-models/paradigm/supervised applicable-models/requirements/black-box applicable-models/requirements/training-data assurance-goal-category/fairness assurance-goal-category/fairness/group assurance-goal-category/reliability assurance-goal-category/transparency data-requirements/labelled-data data-requirements/sensitive-attributes data-type/any evidence-type/dataset-analysis evidence-type/quantitative-metric expertise-needed/low fairness-approach/group lifecycle-stage/data-collection lifecycle-stage/data-collection/data-preprocessing lifecycle-stage/model-development technique-type/procedural"
  },
  {
    "slug": "fair-adversarial-networks",
    "name": "Fair Adversarial Networks",
    "description": "An in-processing fairness technique that employs adversarial training with dual neural networks to learn fair representations. The method consists of a predictor network that learns the main task whilst an adversarial discriminator network simultaneously attempts to predict sensitive attributes from the predictor's hidden representations. Through this adversarial min-max game, the predictor is incentivised to learn features that are informative for the task but statistically independent of protected attributes, effectively removing bias at the representation level in deep learning models.",
    "assurance_goals": [
      "Fairness",
      "Transparency",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks",
      "applicable-models/paradigm/discriminative",
      "applicable-models/paradigm/parametric",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/gradient-access",
      "applicable-models/requirements/model-internals",
      "applicable-models/requirements/training-data",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/fairness/group",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/labelled-data",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/fairness-metric",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "lifecycle-stage/model-development/training",
      "technique-type/algorithmic"
    ],
    "searchText": "fair adversarial networks an in-processing fairness technique that employs adversarial training with dual neural networks to learn fair representations. the method consists of a predictor network that learns the main task whilst an adversarial discriminator network simultaneously attempts to predict sensitive attributes from the predictor's hidden representations. through this adversarial min-max game, the predictor is incentivised to learn features that are informative for the task but statistically independent of protected attributes, effectively removing bias at the representation level in deep learning models. fairness transparency reliability applicable-models/architecture/neural-networks applicable-models/paradigm/discriminative applicable-models/paradigm/parametric applicable-models/paradigm/supervised applicable-models/requirements/gradient-access applicable-models/requirements/model-internals applicable-models/requirements/training-data applicable-models/requirements/white-box assurance-goal-category/fairness assurance-goal-category/fairness/group assurance-goal-category/reliability assurance-goal-category/transparency data-requirements/labelled-data data-requirements/sensitive-attributes data-type/any evidence-type/fairness-metric evidence-type/quantitative-metric expertise-needed/ml-engineering expertise-needed/statistics fairness-approach/group lifecycle-stage/model-development lifecycle-stage/model-development/training technique-type/algorithmic"
  },
  {
    "slug": "prejudice-remover-regulariser",
    "name": "Prejudice Remover Regulariser",
    "description": "An in-processing fairness technique that adds a fairness penalty to machine learning models to reduce bias against protected groups. The method works by minimising 'mutual information' - essentially reducing how much the model's predictions reveal about sensitive attributes like race or gender. By adding this penalty term to the learning objective (typically in logistic regression), the technique ensures predictions become less dependent on protected features. This addresses not only direct discrimination but also indirect bias through correlated features. Practitioners can adjust a tuning parameter to balance between maintaining accuracy and removing prejudice from the model.",
    "assurance_goals": [
      "Fairness",
      "Transparency",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/linear-models/logistic",
      "applicable-models/architecture/probabilistic",
      "applicable-models/paradigm/discriminative",
      "applicable-models/paradigm/probabilistic",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/training-data",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/fairness/group",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/labelled-data",
      "data-requirements/sensitive-attributes",
      "data-type/tabular",
      "evidence-type/fairness-metric",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "lifecycle-stage/model-development/training",
      "technique-type/algorithmic"
    ],
    "searchText": "prejudice remover regulariser an in-processing fairness technique that adds a fairness penalty to machine learning models to reduce bias against protected groups. the method works by minimising 'mutual information' - essentially reducing how much the model's predictions reveal about sensitive attributes like race or gender. by adding this penalty term to the learning objective (typically in logistic regression), the technique ensures predictions become less dependent on protected features. this addresses not only direct discrimination but also indirect bias through correlated features. practitioners can adjust a tuning parameter to balance between maintaining accuracy and removing prejudice from the model. fairness transparency reliability applicable-models/architecture/linear-models/logistic applicable-models/architecture/probabilistic applicable-models/paradigm/discriminative applicable-models/paradigm/probabilistic applicable-models/paradigm/supervised applicable-models/requirements/training-data applicable-models/requirements/white-box assurance-goal-category/fairness assurance-goal-category/fairness/group assurance-goal-category/reliability assurance-goal-category/transparency data-requirements/labelled-data data-requirements/sensitive-attributes data-type/tabular evidence-type/fairness-metric evidence-type/quantitative-metric expertise-needed/ml-engineering expertise-needed/statistics fairness-approach/group lifecycle-stage/model-development lifecycle-stage/model-development/training technique-type/algorithmic"
  },
  {
    "slug": "meta-fair-classifier",
    "name": "Meta Fair Classifier",
    "description": "An in-processing fairness technique that employs meta-learning to modify any existing classifier for optimising fairness metrics whilst maintaining predictive performance. The method learns how to adjust model parameters or decision boundaries to satisfy fairness constraints such as demographic parity or equalised odds through iterative optimisation. This approach is particularly valuable when retrofitting fairness to pre-trained models that perform well but exhibit bias, as it can incorporate fairness without requiring complete retraining from scratch.",
    "assurance_goals": [
      "Fairness",
      "Transparency",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/gradient-access",
      "applicable-models/requirements/gray-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/fairness/group",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/labelled-data",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/fairness-metric",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "lifecycle-stage/model-development/training",
      "lifecycle-stage/model-optimization",
      "technique-type/algorithmic"
    ],
    "searchText": "meta fair classifier an in-processing fairness technique that employs meta-learning to modify any existing classifier for optimising fairness metrics whilst maintaining predictive performance. the method learns how to adjust model parameters or decision boundaries to satisfy fairness constraints such as demographic parity or equalised odds through iterative optimisation. this approach is particularly valuable when retrofitting fairness to pre-trained models that perform well but exhibit bias, as it can incorporate fairness without requiring complete retraining from scratch. fairness transparency reliability applicable-models/architecture/model-agnostic applicable-models/paradigm/supervised applicable-models/requirements/gradient-access applicable-models/requirements/gray-box assurance-goal-category/fairness assurance-goal-category/fairness/group assurance-goal-category/reliability assurance-goal-category/transparency data-requirements/labelled-data data-requirements/sensitive-attributes data-type/any evidence-type/fairness-metric evidence-type/quantitative-metric expertise-needed/ml-engineering expertise-needed/statistics fairness-approach/group lifecycle-stage/model-development lifecycle-stage/model-development/training lifecycle-stage/model-optimization technique-type/algorithmic"
  },
  {
    "slug": "exponentiated-gradient-reduction",
    "name": "Exponentiated Gradient Reduction",
    "description": "An in-processing fairness technique based on Agarwal et al.'s reductions approach that transforms fair classification into a sequence of cost-sensitive classification problems. The method uses an exponentiated gradient algorithm to iteratively reweight training data, returning a randomised classifier that achieves the lowest empirical error whilst satisfying fairness constraints. This reduction-based framework provides theoretical guarantees about both accuracy and constraint violation, making it suitable for various fairness criteria including demographic parity and equalised odds.",
    "assurance_goals": [
      "Fairness",
      "Transparency",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/discriminative",
      "applicable-models/paradigm/parametric",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/gradient-access",
      "applicable-models/requirements/training-data",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/fairness/group",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/labelled-data",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/fairness-metric",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "lifecycle-stage/model-development/training",
      "technique-type/algorithmic"
    ],
    "searchText": "exponentiated gradient reduction an in-processing fairness technique based on agarwal et al.'s reductions approach that transforms fair classification into a sequence of cost-sensitive classification problems. the method uses an exponentiated gradient algorithm to iteratively reweight training data, returning a randomised classifier that achieves the lowest empirical error whilst satisfying fairness constraints. this reduction-based framework provides theoretical guarantees about both accuracy and constraint violation, making it suitable for various fairness criteria including demographic parity and equalised odds. fairness transparency reliability applicable-models/architecture/model-agnostic applicable-models/paradigm/discriminative applicable-models/paradigm/parametric applicable-models/paradigm/supervised applicable-models/requirements/gradient-access applicable-models/requirements/training-data applicable-models/requirements/white-box assurance-goal-category/fairness assurance-goal-category/fairness/group assurance-goal-category/reliability assurance-goal-category/transparency data-requirements/labelled-data data-requirements/sensitive-attributes data-type/any evidence-type/fairness-metric evidence-type/quantitative-metric expertise-needed/ml-engineering expertise-needed/statistics fairness-approach/group lifecycle-stage/model-development lifecycle-stage/model-development/training technique-type/algorithmic"
  },
  {
    "slug": "fair-transfer-learning",
    "name": "Fair Transfer Learning",
    "description": "An in-processing fairness technique that adapts pre-trained models from one domain to another whilst explicitly preserving fairness constraints across different contexts or populations. The method addresses the challenge that fairness properties may not transfer when adapting models to new domains with different demographic compositions or data distributions. Fair transfer learning typically involves constraint-aware fine-tuning, domain adaptation techniques, or adversarial training that maintains equitable performance across groups in the target domain, ensuring that bias mitigation efforts carry over from source to target domains.",
    "assurance_goals": [
      "Fairness",
      "Transparency",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks",
      "applicable-models/paradigm/parametric",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/gradient-access",
      "applicable-models/requirements/model-internals",
      "applicable-models/requirements/training-data",
      "assurance-goal-category/fairness",
      "assurance-goal-category/fairness/group",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/pre-trained-model",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/fairness-metric",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "lifecycle-stage/model-development/fine-tuning",
      "lifecycle-stage/model-development/training",
      "technique-type/algorithmic"
    ],
    "searchText": "fair transfer learning an in-processing fairness technique that adapts pre-trained models from one domain to another whilst explicitly preserving fairness constraints across different contexts or populations. the method addresses the challenge that fairness properties may not transfer when adapting models to new domains with different demographic compositions or data distributions. fair transfer learning typically involves constraint-aware fine-tuning, domain adaptation techniques, or adversarial training that maintains equitable performance across groups in the target domain, ensuring that bias mitigation efforts carry over from source to target domains. fairness transparency reliability applicable-models/architecture/neural-networks applicable-models/paradigm/parametric applicable-models/paradigm/supervised applicable-models/requirements/gradient-access applicable-models/requirements/model-internals applicable-models/requirements/training-data assurance-goal-category/fairness assurance-goal-category/fairness/group assurance-goal-category/reliability assurance-goal-category/transparency data-requirements/pre-trained-model data-requirements/sensitive-attributes data-type/any evidence-type/fairness-metric evidence-type/quantitative-metric expertise-needed/ml-engineering expertise-needed/statistics fairness-approach/group lifecycle-stage/model-development lifecycle-stage/model-development/fine-tuning lifecycle-stage/model-development/training technique-type/algorithmic"
  },
  {
    "slug": "adaptive-sensitive-reweighting",
    "name": "Adaptive Sensitive Reweighting",
    "description": "Adaptive Sensitive Reweighting dynamically adjusts the importance of training examples during model training based on real-time performance across different demographic groups. Unlike traditional static reweighting that fixes weights at the start, this technique continuously monitors fairness metrics and automatically increases the weight of examples from underperforming groups whilst decreasing weights for overrepresented groups. The adaptive mechanism prevents models from perpetuating historical biases by ensuring balanced learning across all demographics throughout the training process.",
    "assurance_goals": [
      "Fairness",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/parametric",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/gray-box",
      "applicable-models/requirements/training-data",
      "assurance-goal-category/fairness",
      "assurance-goal-category/reliability",
      "data-requirements/access-to-model-internals",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "searchText": "adaptive sensitive reweighting adaptive sensitive reweighting dynamically adjusts the importance of training examples during model training based on real-time performance across different demographic groups. unlike traditional static reweighting that fixes weights at the start, this technique continuously monitors fairness metrics and automatically increases the weight of examples from underperforming groups whilst decreasing weights for overrepresented groups. the adaptive mechanism prevents models from perpetuating historical biases by ensuring balanced learning across all demographics throughout the training process. fairness reliability applicable-models/architecture/model-agnostic applicable-models/paradigm/parametric applicable-models/paradigm/supervised applicable-models/requirements/gray-box applicable-models/requirements/training-data assurance-goal-category/fairness assurance-goal-category/reliability data-requirements/access-to-model-internals data-requirements/sensitive-attributes data-type/any evidence-type/quantitative-metric expertise-needed/ml-engineering fairness-approach/group lifecycle-stage/model-development technique-type/algorithmic"
  },
  {
    "slug": "multi-accuracy-boosting",
    "name": "Multi-Accuracy Boosting",
    "description": "An in-processing fairness technique that employs boosting algorithms to improve accuracy uniformly across demographic groups by iteratively correcting errors where the model performs poorly for certain subgroups. The method uses a multi-calibration approach that trains weak learners to focus on prediction errors for underperforming groups, ensuring that no group experiences systematically worse accuracy. This iterative boosting process continues until accuracy parity is achieved across all groups whilst maintaining overall model performance.",
    "assurance_goals": [
      "Fairness",
      "Reliability",
      "Transparency"
    ],
    "tags": [
      "applicable-models/architecture/ensemble",
      "applicable-models/paradigm/discriminative",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/training-data",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/fairness/group",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/labelled-data",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/fairness-metric",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "lifecycle-stage/model-development/training",
      "technique-type/algorithmic"
    ],
    "searchText": "multi-accuracy boosting an in-processing fairness technique that employs boosting algorithms to improve accuracy uniformly across demographic groups by iteratively correcting errors where the model performs poorly for certain subgroups. the method uses a multi-calibration approach that trains weak learners to focus on prediction errors for underperforming groups, ensuring that no group experiences systematically worse accuracy. this iterative boosting process continues until accuracy parity is achieved across all groups whilst maintaining overall model performance. fairness reliability transparency applicable-models/architecture/ensemble applicable-models/paradigm/discriminative applicable-models/paradigm/supervised applicable-models/requirements/training-data applicable-models/requirements/white-box assurance-goal-category/fairness assurance-goal-category/fairness/group assurance-goal-category/reliability assurance-goal-category/transparency data-requirements/labelled-data data-requirements/sensitive-attributes data-type/any evidence-type/fairness-metric evidence-type/quantitative-metric expertise-needed/ml-engineering expertise-needed/statistics fairness-approach/group lifecycle-stage/model-development lifecycle-stage/model-development/training technique-type/algorithmic"
  },
  {
    "slug": "equalised-odds-post-processing",
    "name": "Equalised Odds Post-Processing",
    "description": "A post-processing fairness technique based on Hardt et al.'s seminal work that adjusts classification thresholds after model training to achieve equal true positive rates and false positive rates across demographic groups. The method uses group-specific decision thresholds, potentially with randomisation, to satisfy the equalised odds constraint whilst preserving model utility. This approach enables fairness mitigation without retraining, making it applicable to existing deployed models or when training data access is restricted.",
    "assurance_goals": [
      "Fairness",
      "Transparency",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/discriminative",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/black-box",
      "applicable-models/requirements/probabilistic-output",
      "assurance-goal-category/fairness",
      "assurance-goal-category/fairness/group",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/labelled-data",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/fairness-metric",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "lifecycle-stage/model-development/testing",
      "lifecycle-stage/system-deployment-and-use/monitoring",
      "technique-type/algorithmic"
    ],
    "searchText": "equalised odds post-processing a post-processing fairness technique based on hardt et al.'s seminal work that adjusts classification thresholds after model training to achieve equal true positive rates and false positive rates across demographic groups. the method uses group-specific decision thresholds, potentially with randomisation, to satisfy the equalised odds constraint whilst preserving model utility. this approach enables fairness mitigation without retraining, making it applicable to existing deployed models or when training data access is restricted. fairness transparency reliability applicable-models/architecture/model-agnostic applicable-models/paradigm/discriminative applicable-models/paradigm/supervised applicable-models/requirements/black-box applicable-models/requirements/probabilistic-output assurance-goal-category/fairness assurance-goal-category/fairness/group assurance-goal-category/reliability assurance-goal-category/transparency data-requirements/labelled-data data-requirements/sensitive-attributes data-type/any evidence-type/fairness-metric evidence-type/quantitative-metric expertise-needed/ml-engineering expertise-needed/statistics fairness-approach/group lifecycle-stage/model-development lifecycle-stage/model-development/testing lifecycle-stage/system-deployment-and-use/monitoring technique-type/algorithmic"
  },
  {
    "slug": "reject-option-classification",
    "name": "Reject Option Classification",
    "description": "A post-processing fairness technique that modifies predictions in regions of high uncertainty to favour disadvantaged groups and achieve fairness objectives. The method identifies a 'rejection region' where the model's confidence is low (typically near the decision boundary) and reassigns predictions within this region to benefit underrepresented groups. By leveraging model uncertainty, this approach can improve fairness metrics like demographic parity or equalised odds whilst minimising changes to confident predictions, thus preserving overall accuracy for cases where the model is certain.",
    "assurance_goals": [
      "Fairness",
      "Reliability",
      "Transparency"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/black-box",
      "applicable-models/requirements/probabilistic-output",
      "assurance-goal-category/fairness",
      "assurance-goal-category/fairness/group",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/labelled-data",
      "data-requirements/prediction-probabilities",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/fairness-metric",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/post-processing",
      "technique-type/algorithmic"
    ],
    "searchText": "reject option classification a post-processing fairness technique that modifies predictions in regions of high uncertainty to favour disadvantaged groups and achieve fairness objectives. the method identifies a 'rejection region' where the model's confidence is low (typically near the decision boundary) and reassigns predictions within this region to benefit underrepresented groups. by leveraging model uncertainty, this approach can improve fairness metrics like demographic parity or equalised odds whilst minimising changes to confident predictions, thus preserving overall accuracy for cases where the model is certain. fairness reliability transparency applicable-models/architecture/model-agnostic applicable-models/paradigm/supervised applicable-models/requirements/black-box applicable-models/requirements/probabilistic-output assurance-goal-category/fairness assurance-goal-category/fairness/group assurance-goal-category/reliability assurance-goal-category/transparency data-requirements/labelled-data data-requirements/prediction-probabilities data-requirements/sensitive-attributes data-type/any evidence-type/fairness-metric evidence-type/quantitative-metric expertise-needed/ml-engineering expertise-needed/statistics fairness-approach/group lifecycle-stage/post-processing technique-type/algorithmic"
  },
  {
    "slug": "calibration-with-equality-of-opportunity",
    "name": "Calibration with Equality of Opportunity",
    "description": "A post-processing fairness technique that adjusts model predictions to achieve equal true positive rates across protected groups whilst maintaining calibration within each group. The method addresses fairness by ensuring that qualified individuals from different demographic groups have equal chances of receiving positive predictions, whilst preserving the meaning of probability scores within each group. This technique attempts to balance the competing objectives of group fairness and accurate probability estimation.",
    "assurance_goals": [
      "Fairness",
      "Transparency",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/black-box",
      "applicable-models/requirements/probabilistic-output",
      "assurance-goal-category/fairness",
      "assurance-goal-category/fairness/group",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/calibration-set",
      "data-requirements/labelled-data",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/fairness-metric",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "lifecycle-stage/model-development/testing",
      "technique-type/algorithmic"
    ],
    "searchText": "calibration with equality of opportunity a post-processing fairness technique that adjusts model predictions to achieve equal true positive rates across protected groups whilst maintaining calibration within each group. the method addresses fairness by ensuring that qualified individuals from different demographic groups have equal chances of receiving positive predictions, whilst preserving the meaning of probability scores within each group. this technique attempts to balance the competing objectives of group fairness and accurate probability estimation. fairness transparency reliability applicable-models/architecture/model-agnostic applicable-models/paradigm/supervised applicable-models/requirements/black-box applicable-models/requirements/probabilistic-output assurance-goal-category/fairness assurance-goal-category/fairness/group assurance-goal-category/reliability assurance-goal-category/transparency data-requirements/calibration-set data-requirements/labelled-data data-requirements/sensitive-attributes data-type/any evidence-type/fairness-metric evidence-type/quantitative-metric expertise-needed/ml-engineering expertise-needed/statistics fairness-approach/group lifecycle-stage/model-development lifecycle-stage/model-development/testing technique-type/algorithmic"
  },
  {
    "slug": "equal-opportunity-difference",
    "name": "Equal Opportunity Difference",
    "description": "A fairness metric that quantifies discrimination by measuring the difference in true positive rates (recall) between protected and privileged groups. Based on Hardt et al.'s equality of opportunity framework, this metric computes the maximum difference in TPR across demographic groups, with a value of 0 indicating perfect fairness. The technique provides a mathematical measure of whether qualified individuals from different groups have equal chances of receiving positive predictions.",
    "assurance_goals": [
      "Fairness",
      "Transparency",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/fairness/group",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/labelled-data",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/fairness-metric",
      "evidence-type/quantitative-metric",
      "expertise-needed/low",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "lifecycle-stage/model-development/testing",
      "lifecycle-stage/system-deployment-and-use/monitoring",
      "technique-type/metric"
    ],
    "searchText": "equal opportunity difference a fairness metric that quantifies discrimination by measuring the difference in true positive rates (recall) between protected and privileged groups. based on hardt et al.'s equality of opportunity framework, this metric computes the maximum difference in tpr across demographic groups, with a value of 0 indicating perfect fairness. the technique provides a mathematical measure of whether qualified individuals from different groups have equal chances of receiving positive predictions. fairness transparency reliability applicable-models/architecture/model-agnostic applicable-models/paradigm/supervised applicable-models/requirements/black-box assurance-goal-category/fairness assurance-goal-category/fairness/group assurance-goal-category/reliability assurance-goal-category/transparency data-requirements/labelled-data data-requirements/sensitive-attributes data-type/any evidence-type/fairness-metric evidence-type/quantitative-metric expertise-needed/low fairness-approach/group lifecycle-stage/model-development lifecycle-stage/model-development/testing lifecycle-stage/system-deployment-and-use/monitoring technique-type/metric"
  },
  {
    "slug": "average-odds-difference",
    "name": "Average Odds Difference",
    "description": "Average Odds Difference measures fairness by calculating the average difference in both false positive rates and true positive rates between different demographic groups. This metric captures how consistently a model performs across groups for both positive and negative predictions. A value of 0 indicates perfect fairness under the equalized odds criterion, while larger absolute values indicate greater disparities in model performance between groups.",
    "assurance_goals": [
      "Fairness",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/reliability",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/low",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "lifecycle-stage/testing",
      "technique-type/metric"
    ],
    "searchText": "average odds difference average odds difference measures fairness by calculating the average difference in both false positive rates and true positive rates between different demographic groups. this metric captures how consistently a model performs across groups for both positive and negative predictions. a value of 0 indicates perfect fairness under the equalized odds criterion, while larger absolute values indicate greater disparities in model performance between groups. fairness reliability applicable-models/architecture/model-agnostic applicable-models/paradigm/supervised applicable-models/requirements/black-box assurance-goal-category/fairness assurance-goal-category/reliability data-requirements/sensitive-attributes data-type/any evidence-type/quantitative-metric expertise-needed/low fairness-approach/group lifecycle-stage/model-development lifecycle-stage/testing technique-type/metric"
  },
  {
    "slug": "path-specific-counterfactual-fairness-assessment",
    "name": "Path-Specific Counterfactual Fairness Assessment",
    "description": "A causal fairness evaluation technique that assesses algorithmic discrimination by examining specific causal pathways in a model's decision-making process. Unlike general counterfactual fairness, this approach enables practitioners to identify and intervene on particular causal paths that may introduce bias whilst preserving other legitimate pathways. The method uses causal graphs to distinguish between direct discrimination (through protected attributes) and indirect discrimination (through seemingly neutral factors that correlate with protected attributes), allowing for more nuanced fairness assessments in complex causal settings.",
    "assurance_goals": [
      "Fairness",
      "Transparency",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/fairness/causal",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/causal-graph",
      "data-requirements/sensitive-attributes",
      "data-type/tabular",
      "evidence-type/causal-analysis",
      "evidence-type/quantitative-metric",
      "expertise-needed/causal-inference",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "fairness-approach/causal",
      "lifecycle-stage/model-development",
      "lifecycle-stage/model-evaluation",
      "technique-type/metric"
    ],
    "searchText": "path-specific counterfactual fairness assessment a causal fairness evaluation technique that assesses algorithmic discrimination by examining specific causal pathways in a model's decision-making process. unlike general counterfactual fairness, this approach enables practitioners to identify and intervene on particular causal paths that may introduce bias whilst preserving other legitimate pathways. the method uses causal graphs to distinguish between direct discrimination (through protected attributes) and indirect discrimination (through seemingly neutral factors that correlate with protected attributes), allowing for more nuanced fairness assessments in complex causal settings. fairness transparency reliability applicable-models/architecture/model-agnostic applicable-models/paradigm/supervised applicable-models/requirements/black-box assurance-goal-category/fairness assurance-goal-category/fairness/causal assurance-goal-category/reliability assurance-goal-category/transparency data-requirements/causal-graph data-requirements/sensitive-attributes data-type/tabular evidence-type/causal-analysis evidence-type/quantitative-metric expertise-needed/causal-inference expertise-needed/ml-engineering expertise-needed/statistics fairness-approach/causal lifecycle-stage/model-development lifecycle-stage/model-evaluation technique-type/metric"
  },
  {
    "slug": "bayesian-fairness-regularization",
    "name": "Bayesian Fairness Regularization",
    "description": "Bayesian Fairness Regularization incorporates fairness constraints into machine learning models through Bayesian methods, treating fairness as a prior distribution or regularization term. This approach includes techniques like Fair Bayesian Optimization that use constrained optimization to tune model hyperparameters whilst enforcing fairness constraints, and methods that add regularization terms to objective functions to penalize discriminatory predictions. The technique allows for probabilistic interpretation of fairness constraints and can account for uncertainty in both model parameters and fairness requirements.",
    "assurance_goals": [
      "Fairness",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/parametric",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/gradient-access",
      "applicable-models/requirements/training-data",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/reliability",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "fairness-approach/group",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "searchText": "bayesian fairness regularization bayesian fairness regularization incorporates fairness constraints into machine learning models through bayesian methods, treating fairness as a prior distribution or regularization term. this approach includes techniques like fair bayesian optimization that use constrained optimization to tune model hyperparameters whilst enforcing fairness constraints, and methods that add regularization terms to objective functions to penalize discriminatory predictions. the technique allows for probabilistic interpretation of fairness constraints and can account for uncertainty in both model parameters and fairness requirements. fairness reliability applicable-models/architecture/model-agnostic applicable-models/paradigm/parametric applicable-models/paradigm/supervised applicable-models/requirements/gradient-access applicable-models/requirements/training-data applicable-models/requirements/white-box assurance-goal-category/fairness assurance-goal-category/reliability data-requirements/sensitive-attributes data-type/any evidence-type/quantitative-metric expertise-needed/ml-engineering expertise-needed/statistics fairness-approach/group lifecycle-stage/model-development technique-type/algorithmic"
  },
  {
    "slug": "model-development-audit-trails",
    "name": "Model Development Audit Trails",
    "description": "Model development audit trails create comprehensive, immutable records of all decisions, experiments, and changes throughout the ML lifecycle. This technique captures dataset versions, training configurations, hyperparameter searches, model architecture changes, evaluation results, and deployment decisions in tamper-evident logs. Audit trails enable reproducibility, accountability, and regulatory compliance by documenting who made what changes when and why, supporting post-hoc investigation of model failures or unexpected behaviours.",
    "assurance_goals": [
      "Transparency",
      "Reliability",
      "Safety"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/transparency",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-type/any",
      "lifecycle-stage/project-planning",
      "lifecycle-stage/data-collection",
      "lifecycle-stage/model-development",
      "lifecycle-stage/model-evaluation",
      "lifecycle-stage/deployment",
      "lifecycle-stage/post-deployment",
      "technique-type/procedural",
      "technique-type/governance-framework",
      "evidence-type/documentation",
      "expertise-needed/software-engineering",
      "expertise-needed/ml-engineering",
      "data-requirements/no-special-requirements"
    ],
    "searchText": "model development audit trails model development audit trails create comprehensive, immutable records of all decisions, experiments, and changes throughout the ml lifecycle. this technique captures dataset versions, training configurations, hyperparameter searches, model architecture changes, evaluation results, and deployment decisions in tamper-evident logs. audit trails enable reproducibility, accountability, and regulatory compliance by documenting who made what changes when and why, supporting post-hoc investigation of model failures or unexpected behaviours. transparency reliability safety applicable-models/architecture/model-agnostic applicable-models/requirements/black-box assurance-goal-category/transparency assurance-goal-category/reliability assurance-goal-category/safety data-type/any lifecycle-stage/project-planning lifecycle-stage/data-collection lifecycle-stage/model-development lifecycle-stage/model-evaluation lifecycle-stage/deployment lifecycle-stage/post-deployment technique-type/procedural technique-type/governance-framework evidence-type/documentation expertise-needed/software-engineering expertise-needed/ml-engineering data-requirements/no-special-requirements"
  },
  {
    "slug": "adversarial-robustness-testing",
    "name": "Adversarial Robustness Testing",
    "description": "Adversarial robustness testing evaluates model resilience against intentionally crafted inputs designed to cause misclassification or malfunction. This technique generates adversarial examples through methods like FGSM, PGD, Carlini & Wagner (C&W) attacks, and AutoAttack to measure model vulnerability. Testing assesses both white-box scenarios (where attackers have full model access) and black-box scenarios (where attackers only observe inputs and outputs), measuring metrics like robust accuracy, attack success rates, and perturbation budgets required for successful attacks.",
    "assurance_goals": [
      "Security",
      "Reliability",
      "Safety"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/architecture/neural-networks",
      "applicable-models/requirements/black-box",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/security",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/no-special-requirements",
      "data-type/image",
      "data-type/text",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "evidence-type/test-results",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise",
      "lifecycle-stage/model-evaluation",
      "lifecycle-stage/post-deployment",
      "technique-type/testing"
    ],
    "searchText": "adversarial robustness testing adversarial robustness testing evaluates model resilience against intentionally crafted inputs designed to cause misclassification or malfunction. this technique generates adversarial examples through methods like fgsm, pgd, carlini & wagner (c&w) attacks, and autoattack to measure model vulnerability. testing assesses both white-box scenarios (where attackers have full model access) and black-box scenarios (where attackers only observe inputs and outputs), measuring metrics like robust accuracy, attack success rates, and perturbation budgets required for successful attacks. security reliability safety applicable-models/architecture/model-agnostic applicable-models/architecture/neural-networks applicable-models/requirements/black-box applicable-models/requirements/white-box assurance-goal-category/security assurance-goal-category/reliability assurance-goal-category/safety data-requirements/no-special-requirements data-type/image data-type/text data-type/tabular evidence-type/quantitative-metric evidence-type/test-results expertise-needed/ml-engineering expertise-needed/domain-expertise lifecycle-stage/model-evaluation lifecycle-stage/post-deployment technique-type/testing"
  },
  {
    "slug": "adversarial-training-evaluation",
    "name": "Adversarial Training Evaluation",
    "description": "Adversarial training evaluation assesses whether models trained with adversarial examples have genuinely improved robustness rather than merely overfitting to specific attack methods. This technique tests robustness against diverse attack algorithms including those not used during training, measures certified robustness bounds, and evaluates whether adversarial training creates exploitable trade-offs in clean accuracy or introduces new vulnerabilities. Evaluation ensures adversarial training provides genuine security benefits rather than superficial improvements.",
    "assurance_goals": [
      "Security",
      "Reliability",
      "Transparency"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/architecture/neural-networks",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/security",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-type/any",
      "data-requirements/labeled-data-required",
      "lifecycle-stage/model-evaluation",
      "lifecycle-stage/post-deployment",
      "technique-type/testing",
      "technique-type/analytical",
      "evidence-type/quantitative-metric",
      "evidence-type/test-results",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise"
    ],
    "searchText": "adversarial training evaluation adversarial training evaluation assesses whether models trained with adversarial examples have genuinely improved robustness rather than merely overfitting to specific attack methods. this technique tests robustness against diverse attack algorithms including those not used during training, measures certified robustness bounds, and evaluates whether adversarial training creates exploitable trade-offs in clean accuracy or introduces new vulnerabilities. evaluation ensures adversarial training provides genuine security benefits rather than superficial improvements. security reliability transparency applicable-models/architecture/model-agnostic applicable-models/architecture/neural-networks applicable-models/requirements/black-box assurance-goal-category/security assurance-goal-category/reliability assurance-goal-category/transparency data-type/any data-requirements/labeled-data-required lifecycle-stage/model-evaluation lifecycle-stage/post-deployment technique-type/testing technique-type/analytical evidence-type/quantitative-metric evidence-type/test-results expertise-needed/ml-engineering expertise-needed/domain-expertise"
  },
  {
    "slug": "agent-goal-misalignment-testing",
    "name": "Agent Goal Misalignment Testing",
    "description": "Agent goal misalignment testing identifies scenarios where AI agents pursue objectives in unintended ways or develop proxy goals that diverge from true human intent. This technique tests for specification gaming (achieving stated metrics while violating intent), instrumental goals (agents developing problematic sub-goals to achieve main objectives), and reward hacking (exploiting loopholes in reward functions). Testing uses diverse scenarios to probe whether agents truly understand task intent or merely optimise narrow specified metrics.",
    "assurance_goals": [
      "Safety",
      "Reliability",
      "Fairness"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/reinforcement",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/safety",
      "assurance-goal-category/reliability",
      "assurance-goal-category/fairness",
      "data-type/any",
      "data-requirements/no-special-requirements",
      "lifecycle-stage/model-evaluation",
      "lifecycle-stage/post-deployment",
      "technique-type/testing",
      "evidence-type/test-results",
      "evidence-type/qualitative-assessment",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise"
    ],
    "searchText": "agent goal misalignment testing agent goal misalignment testing identifies scenarios where ai agents pursue objectives in unintended ways or develop proxy goals that diverge from true human intent. this technique tests for specification gaming (achieving stated metrics while violating intent), instrumental goals (agents developing problematic sub-goals to achieve main objectives), and reward hacking (exploiting loopholes in reward functions). testing uses diverse scenarios to probe whether agents truly understand task intent or merely optimise narrow specified metrics. safety reliability fairness applicable-models/architecture/model-agnostic applicable-models/paradigm/reinforcement applicable-models/requirements/black-box assurance-goal-category/safety assurance-goal-category/reliability assurance-goal-category/fairness data-type/any data-requirements/no-special-requirements lifecycle-stage/model-evaluation lifecycle-stage/post-deployment technique-type/testing evidence-type/test-results evidence-type/qualitative-assessment expertise-needed/ml-engineering expertise-needed/domain-expertise"
  },
  {
    "slug": "chain-of-thought-faithfulness-evaluation",
    "name": "Chain-of-Thought Faithfulness Evaluation",
    "description": "Chain-of-thought faithfulness evaluation assesses the quality and faithfulness of step-by-step reasoning produced by language models. This technique evaluates whether intermediate reasoning steps are logically valid, factually accurate, and actually responsible for final answers (rather than post-hoc rationalisations). Evaluation methods include consistency checking (whether altered reasoning changes answers), counterfactual testing (injecting errors in reasoning chains), and comparison between reasoning paths for equivalent problems to ensure systematic rather than spurious reasoning.",
    "assurance_goals": [
      "Explainability",
      "Reliability",
      "Transparency"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks/transformer/llm",
      "applicable-models/paradigm/generative",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/explains/internal-mechanisms",
      "assurance-goal-category/explainability/property/fidelity",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-type/text",
      "data-requirements/labeled-data-required",
      "evidence-type/qualitative-assessment",
      "evidence-type/test-results",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise",
      "expertise-needed/statistical-knowledge",
      "explanatory-scope/local",
      "lifecycle-stage/model-evaluation",
      "lifecycle-stage/post-deployment",
      "technique-type/testing",
      "technique-type/analytical"
    ],
    "searchText": "chain-of-thought faithfulness evaluation chain-of-thought faithfulness evaluation assesses the quality and faithfulness of step-by-step reasoning produced by language models. this technique evaluates whether intermediate reasoning steps are logically valid, factually accurate, and actually responsible for final answers (rather than post-hoc rationalisations). evaluation methods include consistency checking (whether altered reasoning changes answers), counterfactual testing (injecting errors in reasoning chains), and comparison between reasoning paths for equivalent problems to ensure systematic rather than spurious reasoning. explainability reliability transparency applicable-models/architecture/neural-networks/transformer/llm applicable-models/paradigm/generative applicable-models/requirements/black-box assurance-goal-category/explainability assurance-goal-category/explainability/explains/internal-mechanisms assurance-goal-category/explainability/property/fidelity assurance-goal-category/reliability assurance-goal-category/transparency data-type/text data-requirements/labeled-data-required evidence-type/qualitative-assessment evidence-type/test-results evidence-type/quantitative-metric expertise-needed/ml-engineering expertise-needed/domain-expertise expertise-needed/statistical-knowledge explanatory-scope/local lifecycle-stage/model-evaluation lifecycle-stage/post-deployment technique-type/testing technique-type/analytical"
  },
  {
    "slug": "constitutional-ai-evaluation",
    "name": "Constitutional AI Evaluation",
    "description": "Constitutional AI evaluation assesses models trained to follow explicit behavioural principles or 'constitutions' that specify desired values and constraints. This technique verifies that models consistently adhere to specified principles across diverse scenarios, measuring both principle compliance and handling of principle conflicts. Evaluation includes testing boundary cases where principles might conflict, measuring robustness to adversarial attempts to violate principles, and assessing whether models can explain their reasoning in terms of constitutional principles.",
    "assurance_goals": [
      "Safety",
      "Transparency",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks/transformer/llm",
      "applicable-models/requirements/white-box",
      "applicable-models/paradigm/supervised",
      "assurance-goal-category/safety",
      "assurance-goal-category/transparency",
      "assurance-goal-category/reliability",
      "data-type/text",
      "data-requirements/labeled-data-required",
      "data-requirements/training-data-required",
      "lifecycle-stage/model-evaluation",
      "lifecycle-stage/post-deployment",
      "technique-type/testing",
      "technique-type/analytical",
      "evidence-type/quantitative-metric",
      "evidence-type/qualitative-assessment",
      "evidence-type/test-results",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise"
    ],
    "searchText": "constitutional ai evaluation constitutional ai evaluation assesses models trained to follow explicit behavioural principles or 'constitutions' that specify desired values and constraints. this technique verifies that models consistently adhere to specified principles across diverse scenarios, measuring both principle compliance and handling of principle conflicts. evaluation includes testing boundary cases where principles might conflict, measuring robustness to adversarial attempts to violate principles, and assessing whether models can explain their reasoning in terms of constitutional principles. safety transparency reliability applicable-models/architecture/neural-networks/transformer/llm applicable-models/requirements/white-box applicable-models/paradigm/supervised assurance-goal-category/safety assurance-goal-category/transparency assurance-goal-category/reliability data-type/text data-requirements/labeled-data-required data-requirements/training-data-required lifecycle-stage/model-evaluation lifecycle-stage/post-deployment technique-type/testing technique-type/analytical evidence-type/quantitative-metric evidence-type/qualitative-assessment evidence-type/test-results expertise-needed/ml-engineering expertise-needed/domain-expertise"
  },
  {
    "slug": "continual-learning-stability-testing",
    "name": "Continual Learning Stability Testing",
    "description": "Continual learning stability testing evaluates whether models that learn from streaming data maintain performance on previously learned tasks while acquiring new capabilities. This technique measures catastrophic forgetting (performance degradation on old tasks), forward transfer (whether old knowledge helps new learning), and backward transfer (whether new learning damages old performance). Testing includes challenging scenarios where data distributions shift significantly and evaluates whether stability techniques like experience replay or regularisation effectively preserve knowledge.",
    "assurance_goals": [
      "Reliability",
      "Safety",
      "Fairness"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/architecture/neural-networks",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "assurance-goal-category/fairness",
      "data-requirements/training-data-required",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "evidence-type/test-results",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise",
      "lifecycle-stage/model-evaluation",
      "lifecycle-stage/post-deployment",
      "lifecycle-stage/system-deployment-and-use-monitoring",
      "technique-type/testing"
    ],
    "searchText": "continual learning stability testing continual learning stability testing evaluates whether models that learn from streaming data maintain performance on previously learned tasks while acquiring new capabilities. this technique measures catastrophic forgetting (performance degradation on old tasks), forward transfer (whether old knowledge helps new learning), and backward transfer (whether new learning damages old performance). testing includes challenging scenarios where data distributions shift significantly and evaluates whether stability techniques like experience replay or regularisation effectively preserve knowledge. reliability safety fairness applicable-models/architecture/model-agnostic applicable-models/architecture/neural-networks applicable-models/requirements/black-box assurance-goal-category/reliability assurance-goal-category/safety assurance-goal-category/fairness data-requirements/training-data-required data-type/any evidence-type/quantitative-metric evidence-type/test-results expertise-needed/ml-engineering expertise-needed/domain-expertise lifecycle-stage/model-evaluation lifecycle-stage/post-deployment lifecycle-stage/system-deployment-and-use-monitoring technique-type/testing"
  },
  {
    "slug": "data-poisoning-detection",
    "name": "Data Poisoning Detection",
    "description": "Data poisoning detection identifies malicious training data designed to compromise model behaviour. This technique detects various poisoning attacks including backdoor injection (triggers causing specific behaviours), availability attacks (degrading overall performance), and targeted attacks (causing errors on specific inputs). Detection methods include statistical outlier analysis, influence function analysis to identify high-impact training points, and validation-based approaches that test for suspicious model behaviours indicative of poisoning.",
    "assurance_goals": [
      "Security",
      "Reliability",
      "Safety"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/white-box",
      "applicable-models/requirements/gradient-access",
      "assurance-goal-category/security",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/training-data-required",
      "data-type/any",
      "lifecycle-stage/data-collection",
      "lifecycle-stage/model-development",
      "lifecycle-stage/model-evaluation",
      "technique-type/analytical",
      "technique-type/testing",
      "evidence-type/quantitative-metric",
      "evidence-type/test-results",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise"
    ],
    "searchText": "data poisoning detection data poisoning detection identifies malicious training data designed to compromise model behaviour. this technique detects various poisoning attacks including backdoor injection (triggers causing specific behaviours), availability attacks (degrading overall performance), and targeted attacks (causing errors on specific inputs). detection methods include statistical outlier analysis, influence function analysis to identify high-impact training points, and validation-based approaches that test for suspicious model behaviours indicative of poisoning. security reliability safety applicable-models/architecture/model-agnostic applicable-models/requirements/white-box applicable-models/requirements/gradient-access assurance-goal-category/security assurance-goal-category/reliability assurance-goal-category/safety data-requirements/training-data-required data-type/any lifecycle-stage/data-collection lifecycle-stage/model-development lifecycle-stage/model-evaluation technique-type/analytical technique-type/testing evidence-type/quantitative-metric evidence-type/test-results expertise-needed/ml-engineering expertise-needed/domain-expertise"
  },
  {
    "slug": "few-shot-fairness-evaluation",
    "name": "Few-Shot Fairness Evaluation",
    "description": "Few-shot fairness evaluation assesses whether in-context learning with few-shot examples introduces or amplifies biases in model predictions. This technique systematically varies demographic characteristics in few-shot examples and measures how these variations affect model outputs for different groups. Evaluation includes testing prompt sensitivity (how example selection impacts fairness), stereotype amplification (whether biased examples disproportionately affect outputs), and consistency (whether similar inputs receive equitable treatment regardless of example composition).",
    "assurance_goals": [
      "Fairness",
      "Reliability",
      "Transparency"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks/transformer/llm",
      "applicable-models/paradigm/generative",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/fairness/fairness-metrics",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-requirements/labeled-data-required",
      "data-type/text",
      "evidence-type/quantitative-metric",
      "evidence-type/test-results",
      "evidence-type/documentation",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistical-knowledge",
      "expertise-needed/domain-expertise",
      "lifecycle-stage/model-evaluation",
      "lifecycle-stage/post-deployment",
      "technique-type/testing",
      "technique-type/analytical"
    ],
    "searchText": "few-shot fairness evaluation few-shot fairness evaluation assesses whether in-context learning with few-shot examples introduces or amplifies biases in model predictions. this technique systematically varies demographic characteristics in few-shot examples and measures how these variations affect model outputs for different groups. evaluation includes testing prompt sensitivity (how example selection impacts fairness), stereotype amplification (whether biased examples disproportionately affect outputs), and consistency (whether similar inputs receive equitable treatment regardless of example composition). fairness reliability transparency applicable-models/architecture/neural-networks/transformer/llm applicable-models/paradigm/generative applicable-models/requirements/black-box assurance-goal-category/fairness assurance-goal-category/fairness/fairness-metrics assurance-goal-category/reliability assurance-goal-category/transparency data-requirements/labeled-data-required data-type/text evidence-type/quantitative-metric evidence-type/test-results evidence-type/documentation expertise-needed/ml-engineering expertise-needed/statistical-knowledge expertise-needed/domain-expertise lifecycle-stage/model-evaluation lifecycle-stage/post-deployment technique-type/testing technique-type/analytical"
  },
  {
    "slug": "hallucination-detection",
    "name": "Hallucination Detection",
    "description": "Hallucination detection identifies when generative models produce factually incorrect, fabricated, or ungrounded outputs. This technique combines automated consistency checking, self-consistency methods, uncertainty quantification, and human evaluation. It produces detection scores distinguishing intrinsic hallucinations (contradicting sources) from extrinsic hallucinations (unverifiable claims), enabling filtering or user warnings.",
    "assurance_goals": [
      "Reliability",
      "Transparency",
      "Safety"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks/transformer",
      "applicable-models/architecture/neural-networks/transformer/llm",
      "applicable-models/paradigm/generative",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "assurance-goal-category/safety",
      "data-type/text",
      "data-requirements/training-data-required",
      "evidence-type/quantitative-metric",
      "evidence-type/test-results",
      "evidence-type/qualitative-assessment",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise",
      "lifecycle-stage/model-evaluation",
      "lifecycle-stage/post-deployment",
      "technique-type/testing",
      "technique-type/analytical"
    ],
    "searchText": "hallucination detection hallucination detection identifies when generative models produce factually incorrect, fabricated, or ungrounded outputs. this technique combines automated consistency checking, self-consistency methods, uncertainty quantification, and human evaluation. it produces detection scores distinguishing intrinsic hallucinations (contradicting sources) from extrinsic hallucinations (unverifiable claims), enabling filtering or user warnings. reliability transparency safety applicable-models/architecture/neural-networks/transformer applicable-models/architecture/neural-networks/transformer/llm applicable-models/paradigm/generative applicable-models/requirements/black-box assurance-goal-category/reliability assurance-goal-category/transparency assurance-goal-category/safety data-type/text data-requirements/training-data-required evidence-type/quantitative-metric evidence-type/test-results evidence-type/qualitative-assessment expertise-needed/ml-engineering expertise-needed/domain-expertise lifecycle-stage/model-evaluation lifecycle-stage/post-deployment technique-type/testing technique-type/analytical"
  },
  {
    "slug": "safety-guardrails",
    "name": "Safety Guardrails",
    "description": "Safety guardrails apply real-time content moderation and safety constraints during deployment, generating evidence of safety violations and protection effectiveness. This technique produces detailed logs of blocked content, safety incident reports, violation pattern analysis, and guardrail performance metrics. Evidence includes categorised threat taxonomies, false positive/negative rates, and temporal trends in attack patterns, supporting assurance claims about deployed system safety.",
    "assurance_goals": [
      "Safety",
      "Security",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/safety",
      "assurance-goal-category/security",
      "assurance-goal-category/reliability",
      "data-type/text",
      "data-requirements/no-special-requirements",
      "lifecycle-stage/deployment",
      "lifecycle-stage/post-deployment",
      "lifecycle-stage/system-deployment-and-use-monitoring",
      "technique-type/algorithmic",
      "technique-type/procedural",
      "evidence-type/quantitative-metric",
      "evidence-type/test-results",
      "evidence-type/documentation",
      "evidence-type/visual-artifact",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise",
      "expertise-needed/basic-technical"
    ],
    "searchText": "safety guardrails safety guardrails apply real-time content moderation and safety constraints during deployment, generating evidence of safety violations and protection effectiveness. this technique produces detailed logs of blocked content, safety incident reports, violation pattern analysis, and guardrail performance metrics. evidence includes categorised threat taxonomies, false positive/negative rates, and temporal trends in attack patterns, supporting assurance claims about deployed system safety. safety security reliability applicable-models/architecture/model-agnostic applicable-models/requirements/black-box assurance-goal-category/safety assurance-goal-category/security assurance-goal-category/reliability data-type/text data-requirements/no-special-requirements lifecycle-stage/deployment lifecycle-stage/post-deployment lifecycle-stage/system-deployment-and-use-monitoring technique-type/algorithmic technique-type/procedural evidence-type/quantitative-metric evidence-type/test-results evidence-type/documentation evidence-type/visual-artifact expertise-needed/ml-engineering expertise-needed/domain-expertise expertise-needed/basic-technical"
  },
  {
    "slug": "jailbreak-resistance-testing",
    "name": "Jailbreak Resistance Testing",
    "description": "Jailbreak resistance testing evaluates LLM defences against techniques that bypass safety constraints. This involves testing role-playing attacks, hypothetical framing, encoded instructions, and multi-turn manipulation. Testing measures immediate resistance to jailbreaks and the system's ability to detect and block evolving attack patterns, producing reports on vulnerability severity and exploit paths.",
    "assurance_goals": [
      "Safety",
      "Security",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks/transformer/llm",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/safety",
      "assurance-goal-category/security",
      "assurance-goal-category/reliability",
      "data-type/text",
      "lifecycle-stage/model-evaluation",
      "lifecycle-stage/post-deployment",
      "technique-type/testing",
      "evidence-type/test-results",
      "evidence-type/quantitative-metric",
      "evidence-type/documentation",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise",
      "data-requirements/no-special-requirements"
    ],
    "searchText": "jailbreak resistance testing jailbreak resistance testing evaluates llm defences against techniques that bypass safety constraints. this involves testing role-playing attacks, hypothetical framing, encoded instructions, and multi-turn manipulation. testing measures immediate resistance to jailbreaks and the system's ability to detect and block evolving attack patterns, producing reports on vulnerability severity and exploit paths. safety security reliability applicable-models/architecture/neural-networks/transformer/llm applicable-models/requirements/black-box assurance-goal-category/safety assurance-goal-category/security assurance-goal-category/reliability data-type/text lifecycle-stage/model-evaluation lifecycle-stage/post-deployment technique-type/testing evidence-type/test-results evidence-type/quantitative-metric evidence-type/documentation expertise-needed/ml-engineering expertise-needed/domain-expertise data-requirements/no-special-requirements"
  },
  {
    "slug": "epistemic-uncertainty-quantification",
    "name": "Epistemic Uncertainty Quantification",
    "description": "Epistemic uncertainty quantification systematically measures model uncertainty about what it knows, partially knows, and doesn't know across different domains and topics. This technique uses probing datasets, confidence calibration analysis, and consistency testing to quantify epistemic uncertainty (uncertainty due to lack of knowledge) as distinct from aleatoric uncertainty (inherent data uncertainty). Uncertainty quantification enables appropriate deployment scoping, communicating model limitations, and identifying knowledge gaps requiring additional training or human oversight.",
    "assurance_goals": [
      "Transparency",
      "Reliability",
      "Safety"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/transparency",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/labeled-data-required",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "evidence-type/documentation",
      "evidence-type/visual-artifact",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise",
      "expertise-needed/statistical-knowledge",
      "lifecycle-stage/model-evaluation",
      "lifecycle-stage/post-deployment",
      "technique-type/analytical",
      "technique-type/testing"
    ],
    "searchText": "epistemic uncertainty quantification epistemic uncertainty quantification systematically measures model uncertainty about what it knows, partially knows, and doesn't know across different domains and topics. this technique uses probing datasets, confidence calibration analysis, and consistency testing to quantify epistemic uncertainty (uncertainty due to lack of knowledge) as distinct from aleatoric uncertainty (inherent data uncertainty). uncertainty quantification enables appropriate deployment scoping, communicating model limitations, and identifying knowledge gaps requiring additional training or human oversight. transparency reliability safety applicable-models/architecture/model-agnostic applicable-models/requirements/black-box assurance-goal-category/transparency assurance-goal-category/reliability assurance-goal-category/safety data-requirements/labeled-data-required data-type/any evidence-type/quantitative-metric evidence-type/documentation evidence-type/visual-artifact expertise-needed/ml-engineering expertise-needed/domain-expertise expertise-needed/statistical-knowledge lifecycle-stage/model-evaluation lifecycle-stage/post-deployment technique-type/analytical technique-type/testing"
  },
  {
    "slug": "multi-agent-system-testing",
    "name": "Multi-Agent System Testing",
    "description": "Multi-agent system testing evaluates safety and reliability of systems where multiple AI agents interact, coordinate, or compete. This technique assesses emergent behaviours, communication protocols, conflict resolution, and whether agents maintain objectives appropriately. Testing produces reports on agent interaction patterns, coalition formation, failure cascade analysis, and safety property violations in multi-agent scenarios.",
    "assurance_goals": [
      "Safety",
      "Reliability",
      "Security"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/safety",
      "assurance-goal-category/reliability",
      "assurance-goal-category/security",
      "data-type/any",
      "lifecycle-stage/model-evaluation",
      "lifecycle-stage/post-deployment",
      "lifecycle-stage/system-deployment-and-use-monitoring",
      "technique-type/testing",
      "technique-type/analytical",
      "evidence-type/test-results",
      "evidence-type/documentation",
      "evidence-type/qualitative-assessment",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise",
      "expertise-needed/software-engineering",
      "data-requirements/no-special-requirements"
    ],
    "searchText": "multi-agent system testing multi-agent system testing evaluates safety and reliability of systems where multiple ai agents interact, coordinate, or compete. this technique assesses emergent behaviours, communication protocols, conflict resolution, and whether agents maintain objectives appropriately. testing produces reports on agent interaction patterns, coalition formation, failure cascade analysis, and safety property violations in multi-agent scenarios. safety reliability security applicable-models/architecture/model-agnostic applicable-models/requirements/black-box assurance-goal-category/safety assurance-goal-category/reliability assurance-goal-category/security data-type/any lifecycle-stage/model-evaluation lifecycle-stage/post-deployment lifecycle-stage/system-deployment-and-use-monitoring technique-type/testing technique-type/analytical evidence-type/test-results evidence-type/documentation evidence-type/qualitative-assessment expertise-needed/ml-engineering expertise-needed/domain-expertise expertise-needed/software-engineering data-requirements/no-special-requirements"
  },
  {
    "slug": "multimodal-alignment-evaluation",
    "name": "Multimodal Alignment Evaluation",
    "description": "Multimodal alignment evaluation assesses whether different modalities (vision, language, audio) are synchronised and consistent in multimodal AI systems. This technique tests whether descriptions match content, sounds associate with correct sources, and cross-modal reasoning maintains accuracy. It produces alignment scores, grounding quality metrics, and reports on modality-specific failure patterns.",
    "assurance_goals": [
      "Reliability",
      "Explainability",
      "Safety"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks",
      "applicable-models/architecture/neural-networks/transformer",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/reliability",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/explains/internal-mechanisms",
      "assurance-goal-category/safety",
      "data-type/image",
      "data-type/text",
      "data-requirements/labeled-data-required",
      "lifecycle-stage/model-evaluation",
      "lifecycle-stage/post-deployment",
      "technique-type/testing",
      "technique-type/analytical",
      "evidence-type/quantitative-metric",
      "evidence-type/qualitative-assessment",
      "evidence-type/test-results",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise"
    ],
    "searchText": "multimodal alignment evaluation multimodal alignment evaluation assesses whether different modalities (vision, language, audio) are synchronised and consistent in multimodal ai systems. this technique tests whether descriptions match content, sounds associate with correct sources, and cross-modal reasoning maintains accuracy. it produces alignment scores, grounding quality metrics, and reports on modality-specific failure patterns. reliability explainability safety applicable-models/architecture/neural-networks applicable-models/architecture/neural-networks/transformer applicable-models/requirements/black-box assurance-goal-category/reliability assurance-goal-category/explainability assurance-goal-category/explainability/explains/internal-mechanisms assurance-goal-category/safety data-type/image data-type/text data-requirements/labeled-data-required lifecycle-stage/model-evaluation lifecycle-stage/post-deployment technique-type/testing technique-type/analytical evidence-type/quantitative-metric evidence-type/qualitative-assessment evidence-type/test-results expertise-needed/ml-engineering expertise-needed/domain-expertise"
  },
  {
    "slug": "out-of-domain-detection",
    "name": "Out-of-Domain Detection",
    "description": "Out-of-domain (OOD) detection identifies user inputs that fall outside an AI system's intended domain or capabilities, enabling graceful handling rather than generating unreliable responses. This technique trains classifiers or uses embedding-based methods to recognise when queries require knowledge or capabilities the system doesn't possess. OOD detection enables systems to explicitly decline, redirect, or request clarification rather than hallucinating answers or applying inappropriate reasoning to unfamiliar domains.",
    "assurance_goals": [
      "Reliability",
      "Safety",
      "Transparency"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/architecture/neural-networks/transformer/llm",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "assurance-goal-category/transparency",
      "data-requirements/training-data-required",
      "data-requirements/labeled-data-required",
      "data-type/text",
      "evidence-type/quantitative-metric",
      "evidence-type/test-results",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise",
      "lifecycle-stage/model-development",
      "lifecycle-stage/deployment",
      "lifecycle-stage/post-deployment",
      "technique-type/algorithmic",
      "technique-type/procedural"
    ],
    "searchText": "out-of-domain detection out-of-domain (ood) detection identifies user inputs that fall outside an ai system's intended domain or capabilities, enabling graceful handling rather than generating unreliable responses. this technique trains classifiers or uses embedding-based methods to recognise when queries require knowledge or capabilities the system doesn't possess. ood detection enables systems to explicitly decline, redirect, or request clarification rather than hallucinating answers or applying inappropriate reasoning to unfamiliar domains. reliability safety transparency applicable-models/architecture/model-agnostic applicable-models/architecture/neural-networks/transformer/llm applicable-models/paradigm/supervised applicable-models/requirements/black-box assurance-goal-category/reliability assurance-goal-category/safety assurance-goal-category/transparency data-requirements/training-data-required data-requirements/labeled-data-required data-type/text evidence-type/quantitative-metric evidence-type/test-results expertise-needed/ml-engineering expertise-needed/domain-expertise lifecycle-stage/model-development lifecycle-stage/deployment lifecycle-stage/post-deployment technique-type/algorithmic technique-type/procedural"
  },
  {
    "slug": "synthetic-data-evaluation",
    "name": "Synthetic Data Evaluation",
    "description": "Synthetic data evaluation assesses whether synthetic datasets protect individual privacy while maintaining statistical utility and fidelity to real data. This technique evaluates three key dimensions: privacy (through disclosure risk metrics and re-identification attack success rates), utility (by comparing statistical properties and model performance), and fidelity (measuring distributional similarity to real data). It produces evaluation reports quantifying the privacy-utility-fidelity trade-offs.",
    "assurance_goals": [
      "Privacy",
      "Transparency",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/privacy",
      "assurance-goal-category/transparency",
      "assurance-goal-category/reliability",
      "data-requirements/training-data-required",
      "data-type/tabular",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "evidence-type/documentation",
      "evidence-type/test-results",
      "expertise-needed/statistical-knowledge",
      "expertise-needed/domain-expertise",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/data-collection",
      "lifecycle-stage/model-evaluation",
      "technique-type/analytical",
      "technique-type/testing"
    ],
    "searchText": "synthetic data evaluation synthetic data evaluation assesses whether synthetic datasets protect individual privacy while maintaining statistical utility and fidelity to real data. this technique evaluates three key dimensions: privacy (through disclosure risk metrics and re-identification attack success rates), utility (by comparing statistical properties and model performance), and fidelity (measuring distributional similarity to real data). it produces evaluation reports quantifying the privacy-utility-fidelity trade-offs. privacy transparency reliability applicable-models/architecture/model-agnostic applicable-models/requirements/black-box assurance-goal-category/privacy assurance-goal-category/transparency assurance-goal-category/reliability data-requirements/training-data-required data-type/tabular data-type/any evidence-type/quantitative-metric evidence-type/documentation evidence-type/test-results expertise-needed/statistical-knowledge expertise-needed/domain-expertise expertise-needed/ml-engineering lifecycle-stage/data-collection lifecycle-stage/model-evaluation technique-type/analytical technique-type/testing"
  },
  {
    "slug": "prompt-robustness-testing",
    "name": "Prompt Robustness Testing",
    "description": "Prompt robustness testing evaluates how consistently models perform when prompts undergo minor variations in wording, formatting, or structure. This technique systematically paraphrases prompts, reorders elements, changes formatting (capitalisation, punctuation), and tests semantically equivalent variations to measure output consistency. Testing assesses robustness by identifying when superficial prompt changes cause dramatic performance swings, helping developers create robust prompt templates and understand model reliability boundaries.",
    "assurance_goals": [
      "Reliability",
      "Fairness",
      "Explainability"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks/transformer/llm",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/reliability",
      "assurance-goal-category/fairness",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/explains/decision-boundaries",
      "data-type/text",
      "data-requirements/no-special-requirements",
      "lifecycle-stage/model-evaluation",
      "lifecycle-stage/post-deployment",
      "technique-type/testing",
      "evidence-type/quantitative-metric",
      "evidence-type/test-results",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise",
      "explanatory-scope/global"
    ],
    "searchText": "prompt robustness testing prompt robustness testing evaluates how consistently models perform when prompts undergo minor variations in wording, formatting, or structure. this technique systematically paraphrases prompts, reorders elements, changes formatting (capitalisation, punctuation), and tests semantically equivalent variations to measure output consistency. testing assesses robustness by identifying when superficial prompt changes cause dramatic performance swings, helping developers create robust prompt templates and understand model reliability boundaries. reliability fairness explainability applicable-models/architecture/neural-networks/transformer/llm applicable-models/requirements/black-box assurance-goal-category/reliability assurance-goal-category/fairness assurance-goal-category/explainability assurance-goal-category/explainability/explains/decision-boundaries data-type/text data-requirements/no-special-requirements lifecycle-stage/model-evaluation lifecycle-stage/post-deployment technique-type/testing evidence-type/quantitative-metric evidence-type/test-results expertise-needed/ml-engineering expertise-needed/domain-expertise explanatory-scope/global"
  },
  {
    "slug": "prompt-injection-testing",
    "name": "Prompt Injection Testing",
    "description": "Prompt injection testing systematically evaluates LLMs and generative AI systems for vulnerabilities where malicious users can override system instructions through carefully crafted inputs. This technique encompasses both offensive security testing (proactively attempting injection attacks to identify vulnerabilities) and defensive detection mechanisms (monitoring for suspicious patterns in prompts and context). Testing involves evaluating various injection patterns including direct instruction manipulation, context confusion, role-playing exploits, delimiter attacks, and context window poisoning. Detection methods analyse context provenance, validate formatting consistency, and identify semantic anomalies that indicate manipulation attempts rather than legitimate user interaction.",
    "assurance_goals": [
      "Security",
      "Safety",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks/transformer/llm",
      "applicable-models/paradigm/generative",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/security",
      "assurance-goal-category/safety",
      "assurance-goal-category/reliability",
      "data-requirements/no-special-requirements",
      "data-type/text",
      "evidence-type/test-results",
      "evidence-type/qualitative-assessment",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise",
      "lifecycle-stage/model-evaluation",
      "lifecycle-stage/deployment",
      "lifecycle-stage/post-deployment",
      "lifecycle-stage/system-deployment-and-use-monitoring",
      "technique-type/testing",
      "technique-type/analytical"
    ],
    "searchText": "prompt injection testing prompt injection testing systematically evaluates llms and generative ai systems for vulnerabilities where malicious users can override system instructions through carefully crafted inputs. this technique encompasses both offensive security testing (proactively attempting injection attacks to identify vulnerabilities) and defensive detection mechanisms (monitoring for suspicious patterns in prompts and context). testing involves evaluating various injection patterns including direct instruction manipulation, context confusion, role-playing exploits, delimiter attacks, and context window poisoning. detection methods analyse context provenance, validate formatting consistency, and identify semantic anomalies that indicate manipulation attempts rather than legitimate user interaction. security safety reliability applicable-models/architecture/neural-networks/transformer/llm applicable-models/paradigm/generative applicable-models/requirements/black-box assurance-goal-category/security assurance-goal-category/safety assurance-goal-category/reliability data-requirements/no-special-requirements data-type/text evidence-type/test-results evidence-type/qualitative-assessment evidence-type/quantitative-metric expertise-needed/ml-engineering expertise-needed/domain-expertise lifecycle-stage/model-evaluation lifecycle-stage/deployment lifecycle-stage/post-deployment lifecycle-stage/system-deployment-and-use-monitoring technique-type/testing technique-type/analytical"
  },
  {
    "slug": "retrieval-augmented-generation-evaluation",
    "name": "Retrieval-Augmented Generation Evaluation",
    "description": "RAG evaluation assesses systems combining retrieval and generation by measuring retrieval quality, generation faithfulness, and overall performance. This technique evaluates whether retrieved context is relevant, whether responses faithfully represent information without hallucination, and how systems handle insufficient context. Key metrics include retrieval precision/recall, answer relevance, faithfulness scores, and citation accuracy.",
    "assurance_goals": [
      "Reliability",
      "Transparency",
      "Explainability"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks/transformer/llm",
      "applicable-models/paradigm/generative",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/explains/internal-mechanisms",
      "assurance-goal-category/explainability/property/fidelity",
      "data-requirements/labeled-data-required",
      "data-type/text",
      "evidence-type/quantitative-metric",
      "evidence-type/qualitative-assessment",
      "evidence-type/test-results",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise",
      "lifecycle-stage/model-evaluation",
      "lifecycle-stage/post-deployment",
      "technique-type/testing",
      "technique-type/analytical"
    ],
    "searchText": "retrieval-augmented generation evaluation rag evaluation assesses systems combining retrieval and generation by measuring retrieval quality, generation faithfulness, and overall performance. this technique evaluates whether retrieved context is relevant, whether responses faithfully represent information without hallucination, and how systems handle insufficient context. key metrics include retrieval precision/recall, answer relevance, faithfulness scores, and citation accuracy. reliability transparency explainability applicable-models/architecture/neural-networks/transformer/llm applicable-models/paradigm/generative applicable-models/requirements/black-box assurance-goal-category/reliability assurance-goal-category/transparency assurance-goal-category/explainability assurance-goal-category/explainability/explains/internal-mechanisms assurance-goal-category/explainability/property/fidelity data-requirements/labeled-data-required data-type/text evidence-type/quantitative-metric evidence-type/qualitative-assessment evidence-type/test-results expertise-needed/ml-engineering expertise-needed/domain-expertise lifecycle-stage/model-evaluation lifecycle-stage/post-deployment technique-type/testing technique-type/analytical"
  },
  {
    "slug": "reward-hacking-detection",
    "name": "Reward Hacking Detection",
    "description": "Reward hacking detection identifies when AI systems achieve stated objectives through unintended shortcuts or loopholes rather than genuine task mastery. This technique analyses whether systems exploit reward specification ambiguities, reward function bugs, or simulator artifacts to maximise rewards without actually solving the intended problem. Detection involves adversarial specification testing, human evaluation of solution quality, transfer testing to slightly modified tasks, and analysis of unexpected strategy patterns that suggest reward hacking rather than genuine learning.",
    "assurance_goals": [
      "Reliability",
      "Safety",
      "Transparency"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/reinforcement",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "assurance-goal-category/transparency",
      "data-type/any",
      "data-requirements/no-special-requirements",
      "lifecycle-stage/model-evaluation",
      "lifecycle-stage/post-deployment",
      "technique-type/testing",
      "technique-type/analytical",
      "evidence-type/qualitative-assessment",
      "evidence-type/test-results",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise"
    ],
    "searchText": "reward hacking detection reward hacking detection identifies when ai systems achieve stated objectives through unintended shortcuts or loopholes rather than genuine task mastery. this technique analyses whether systems exploit reward specification ambiguities, reward function bugs, or simulator artifacts to maximise rewards without actually solving the intended problem. detection involves adversarial specification testing, human evaluation of solution quality, transfer testing to slightly modified tasks, and analysis of unexpected strategy patterns that suggest reward hacking rather than genuine learning. reliability safety transparency applicable-models/architecture/model-agnostic applicable-models/paradigm/reinforcement applicable-models/requirements/black-box assurance-goal-category/reliability assurance-goal-category/safety assurance-goal-category/transparency data-type/any data-requirements/no-special-requirements lifecycle-stage/model-evaluation lifecycle-stage/post-deployment technique-type/testing technique-type/analytical evidence-type/qualitative-assessment evidence-type/test-results expertise-needed/ml-engineering expertise-needed/domain-expertise"
  },
  {
    "slug": "ai-agent-safety-testing",
    "name": "AI Agent Safety Testing",
    "description": "AI agent safety testing evaluates autonomous AI agents that interact with external tools, APIs, and systems to ensure they operate safely and as intended. This technique assesses whether agents correctly understand tool specifications, respect usage constraints, handle errors gracefully, avoid unintended side effects, and make safe decisions during multi-step reasoning. Testing includes boundary condition validation (extreme parameters, edge cases), permission verification (ensuring agents don't exceed authorized actions), and cascade failure analysis (how tool errors propagate through agent workflows).",
    "assurance_goals": [
      "Safety",
      "Security",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks/transformer/llm",
      "applicable-models/paradigm/generative",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/safety",
      "assurance-goal-category/security",
      "assurance-goal-category/reliability",
      "data-type/any",
      "data-requirements/no-special-requirements",
      "lifecycle-stage/model-evaluation",
      "lifecycle-stage/post-deployment",
      "technique-type/testing",
      "evidence-type/test-results",
      "evidence-type/quantitative-metric",
      "evidence-type/documentation",
      "expertise-needed/ml-engineering",
      "expertise-needed/software-engineering",
      "expertise-needed/domain-expertise"
    ],
    "searchText": "ai agent safety testing ai agent safety testing evaluates autonomous ai agents that interact with external tools, apis, and systems to ensure they operate safely and as intended. this technique assesses whether agents correctly understand tool specifications, respect usage constraints, handle errors gracefully, avoid unintended side effects, and make safe decisions during multi-step reasoning. testing includes boundary condition validation (extreme parameters, edge cases), permission verification (ensuring agents don't exceed authorized actions), and cascade failure analysis (how tool errors propagate through agent workflows). safety security reliability applicable-models/architecture/neural-networks/transformer/llm applicable-models/paradigm/generative applicable-models/requirements/black-box assurance-goal-category/safety assurance-goal-category/security assurance-goal-category/reliability data-type/any data-requirements/no-special-requirements lifecycle-stage/model-evaluation lifecycle-stage/post-deployment technique-type/testing evidence-type/test-results evidence-type/quantitative-metric evidence-type/documentation expertise-needed/ml-engineering expertise-needed/software-engineering expertise-needed/domain-expertise"
  },
  {
    "slug": "toxicity-and-bias-detection",
    "name": "Toxicity and Bias Detection",
    "description": "Toxicity and bias detection uses automated classifiers and human review to identify harmful, offensive, or biased content in model outputs. This technique employs tools trained to detect toxic language, hate speech, stereotypes, and demographic biases through targeted testing with adversarial prompts. Detection covers explicit toxicity, implicit bias, and distributional unfairness across demographic groups.",
    "assurance_goals": [
      "Safety",
      "Fairness",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/architecture/neural-networks/transformer/llm",
      "applicable-models/requirements/black-box",
      "applicable-models/paradigm/generative",
      "assurance-goal-category/safety",
      "assurance-goal-category/fairness",
      "assurance-goal-category/fairness-metrics",
      "assurance-goal-category/reliability",
      "data-type/text",
      "data-requirements/labeled-data-required",
      "lifecycle-stage/model-evaluation",
      "lifecycle-stage/post-deployment",
      "lifecycle-stage/system-deployment-and-use-monitoring",
      "technique-type/testing",
      "technique-type/analytical",
      "evidence-type/quantitative-metric",
      "evidence-type/qualitative-assessment",
      "evidence-type/test-results",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise"
    ],
    "searchText": "toxicity and bias detection toxicity and bias detection uses automated classifiers and human review to identify harmful, offensive, or biased content in model outputs. this technique employs tools trained to detect toxic language, hate speech, stereotypes, and demographic biases through targeted testing with adversarial prompts. detection covers explicit toxicity, implicit bias, and distributional unfairness across demographic groups. safety fairness reliability applicable-models/architecture/model-agnostic applicable-models/architecture/neural-networks/transformer/llm applicable-models/requirements/black-box applicable-models/paradigm/generative assurance-goal-category/safety assurance-goal-category/fairness assurance-goal-category/fairness-metrics assurance-goal-category/reliability data-type/text data-requirements/labeled-data-required lifecycle-stage/model-evaluation lifecycle-stage/post-deployment lifecycle-stage/system-deployment-and-use-monitoring technique-type/testing technique-type/analytical evidence-type/quantitative-metric evidence-type/qualitative-assessment evidence-type/test-results expertise-needed/ml-engineering expertise-needed/domain-expertise"
  }
]