[
  {
    "slug": "homomorphic-encryption",
    "name": "Homomorphic Encryption",
    "description": "Homomorphic encryption allows computation on encrypted data without decrypting it first, producing encrypted results that, when decrypted, match the results of performing the same operations on the plaintext. This enables secure outsourced computation where sensitive data remains encrypted throughout processing. By allowing ML operations on encrypted data, it provides strong privacy guarantees for applications involving highly sensitive information.",
    "assurance_goals": [
      "Privacy",
      "Safety",
      "Transparency",
      "Security"
    ],
    "tags": [
      "applicable-models/architecture/linear-models",
      "applicable-models/architecture/neural-networks/feedforward",
      "applicable-models/paradigm/discriminative",
      "applicable-models/paradigm/parametric",
      "applicable-models/requirements/differentiable",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/privacy",
      "assurance-goal-category/privacy/formal-guarantee",
      "assurance-goal-category/safety",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/privacy-guarantee",
      "evidence-type/quantitative-metric",
      "expertise-needed/cryptography",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "searchText": "homomorphic encryption homomorphic encryption allows computation on encrypted data without decrypting it first, producing encrypted results that, when decrypted, match the results of performing the same operations on the plaintext. this enables secure outsourced computation where sensitive data remains encrypted throughout processing. by allowing ml operations on encrypted data, it provides strong privacy guarantees for applications involving highly sensitive information. privacy safety transparency security applicable-models/architecture/linear-models applicable-models/architecture/neural-networks/feedforward applicable-models/paradigm/discriminative applicable-models/paradigm/parametric applicable-models/requirements/differentiable applicable-models/requirements/white-box assurance-goal-category/privacy assurance-goal-category/privacy/formal-guarantee assurance-goal-category/safety assurance-goal-category/transparency data-requirements/no-special-requirements data-type/any evidence-type/privacy-guarantee evidence-type/quantitative-metric expertise-needed/cryptography expertise-needed/ml-engineering lifecycle-stage/model-development lifecycle-stage/system-deployment-and-use technique-type/algorithmic"
  },
  {
    "slug": "red-teaming",
    "name": "Red Teaming",
    "description": "Red teaming involves systematic adversarial testing of AI/ML systems by dedicated specialists who attempt to identify flaws, vulnerabilities, harmful outputs, and ways to circumvent safety measures. Drawing from cybersecurity practices, red teams employ diverse attack vectors including prompt injection, adversarial examples, edge case exploitation, social engineering scenarios, and goal misalignment probes. Unlike standard testing that validates expected behaviour, red teaming specifically seeks to break systems through creative and adversarial approaches, revealing non-obvious risks and failure modes that could be exploited maliciously or cause harm in deployment.",
    "assurance_goals": [
      "Safety",
      "Reliability",
      "Fairness",
      "Security"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/qualitative-report",
      "expertise-needed/ml-engineering",
      "expertise-needed/security",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/procedural"
    ],
    "searchText": "red teaming red teaming involves systematic adversarial testing of ai/ml systems by dedicated specialists who attempt to identify flaws, vulnerabilities, harmful outputs, and ways to circumvent safety measures. drawing from cybersecurity practices, red teams employ diverse attack vectors including prompt injection, adversarial examples, edge case exploitation, social engineering scenarios, and goal misalignment probes. unlike standard testing that validates expected behaviour, red teaming specifically seeks to break systems through creative and adversarial approaches, revealing non-obvious risks and failure modes that could be exploited maliciously or cause harm in deployment. safety reliability fairness security applicable-models/architecture/model-agnostic applicable-models/requirements/black-box assurance-goal-category/fairness assurance-goal-category/reliability assurance-goal-category/safety data-requirements/no-special-requirements data-type/any evidence-type/qualitative-report expertise-needed/ml-engineering expertise-needed/security lifecycle-stage/system-deployment-and-use technique-type/procedural"
  },
  {
    "slug": "anomaly-detection",
    "name": "Anomaly Detection",
    "description": "Anomaly detection identifies unusual behaviours, inputs, or outputs that deviate significantly from established normal patterns using statistical, machine learning, or rule-based methods. Applied to AI/ML systems, it serves as a continuous monitoring mechanism that can flag unexpected model predictions, suspicious input patterns, data drift, adversarial attacks, or operational malfunctions. By establishing baselines of normal system behaviour and alerting when deviations exceed predefined thresholds, organisations can detect potential security threats, model degradation, fairness violations, or system failures before they cause significant harm.",
    "assurance_goals": [
      "Safety",
      "Reliability",
      "Fairness",
      "Security"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "applicable-models/requirements/training-data",
      "assurance-goal-category/fairness",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "assurance-goal-category/safety/monitoring/anomaly-detection",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "lifecycle-stage/system-deployment-and-use",
      "lifecycle-stage/system-deployment-and-use/monitoring",
      "technique-type/algorithmic"
    ],
    "searchText": "anomaly detection anomaly detection identifies unusual behaviours, inputs, or outputs that deviate significantly from established normal patterns using statistical, machine learning, or rule-based methods. applied to ai/ml systems, it serves as a continuous monitoring mechanism that can flag unexpected model predictions, suspicious input patterns, data drift, adversarial attacks, or operational malfunctions. by establishing baselines of normal system behaviour and alerting when deviations exceed predefined thresholds, organisations can detect potential security threats, model degradation, fairness violations, or system failures before they cause significant harm. safety reliability fairness security applicable-models/architecture/model-agnostic applicable-models/requirements/black-box applicable-models/requirements/training-data assurance-goal-category/fairness assurance-goal-category/reliability assurance-goal-category/safety assurance-goal-category/safety/monitoring/anomaly-detection data-requirements/no-special-requirements data-type/any evidence-type/quantitative-metric expertise-needed/ml-engineering expertise-needed/statistics lifecycle-stage/system-deployment-and-use lifecycle-stage/system-deployment-and-use/monitoring technique-type/algorithmic"
  },
  {
    "slug": "adversarial-robustness-testing",
    "name": "Adversarial Robustness Testing",
    "description": "Adversarial robustness testing evaluates model resilience against intentionally crafted inputs designed to cause misclassification or malfunction. This technique generates adversarial examples through methods like FGSM, PGD, Carlini & Wagner (C&W) attacks, and AutoAttack to measure model vulnerability. Testing assesses both white-box scenarios (where attackers have full model access) and black-box scenarios (where attackers only observe inputs and outputs), measuring metrics like robust accuracy, attack success rates, and perturbation budgets required for successful attacks.",
    "assurance_goals": [
      "Security",
      "Reliability",
      "Safety"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/architecture/neural-networks",
      "applicable-models/requirements/black-box",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/security",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/no-special-requirements",
      "data-type/image",
      "data-type/text",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "evidence-type/test-results",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise",
      "lifecycle-stage/model-evaluation",
      "lifecycle-stage/post-deployment",
      "technique-type/testing"
    ],
    "searchText": "adversarial robustness testing adversarial robustness testing evaluates model resilience against intentionally crafted inputs designed to cause misclassification or malfunction. this technique generates adversarial examples through methods like fgsm, pgd, carlini & wagner (c&w) attacks, and autoattack to measure model vulnerability. testing assesses both white-box scenarios (where attackers have full model access) and black-box scenarios (where attackers only observe inputs and outputs), measuring metrics like robust accuracy, attack success rates, and perturbation budgets required for successful attacks. security reliability safety applicable-models/architecture/model-agnostic applicable-models/architecture/neural-networks applicable-models/requirements/black-box applicable-models/requirements/white-box assurance-goal-category/security assurance-goal-category/reliability assurance-goal-category/safety data-requirements/no-special-requirements data-type/image data-type/text data-type/tabular evidence-type/quantitative-metric evidence-type/test-results expertise-needed/ml-engineering expertise-needed/domain-expertise lifecycle-stage/model-evaluation lifecycle-stage/post-deployment technique-type/testing"
  },
  {
    "slug": "adversarial-training-evaluation",
    "name": "Adversarial Training Evaluation",
    "description": "Adversarial training evaluation assesses whether models trained with adversarial examples have genuinely improved robustness rather than merely overfitting to specific attack methods. This technique tests robustness against diverse attack algorithms including those not used during training, measures certified robustness bounds, and evaluates whether adversarial training creates exploitable trade-offs in clean accuracy or introduces new vulnerabilities. Evaluation ensures adversarial training provides genuine security benefits rather than superficial improvements.",
    "assurance_goals": [
      "Security",
      "Reliability",
      "Transparency"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/architecture/neural-networks",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/security",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "data-type/any",
      "data-requirements/labeled-data-required",
      "lifecycle-stage/model-evaluation",
      "lifecycle-stage/post-deployment",
      "technique-type/testing",
      "technique-type/analytical",
      "evidence-type/quantitative-metric",
      "evidence-type/test-results",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise"
    ],
    "searchText": "adversarial training evaluation adversarial training evaluation assesses whether models trained with adversarial examples have genuinely improved robustness rather than merely overfitting to specific attack methods. this technique tests robustness against diverse attack algorithms including those not used during training, measures certified robustness bounds, and evaluates whether adversarial training creates exploitable trade-offs in clean accuracy or introduces new vulnerabilities. evaluation ensures adversarial training provides genuine security benefits rather than superficial improvements. security reliability transparency applicable-models/architecture/model-agnostic applicable-models/architecture/neural-networks applicable-models/requirements/black-box assurance-goal-category/security assurance-goal-category/reliability assurance-goal-category/transparency data-type/any data-requirements/labeled-data-required lifecycle-stage/model-evaluation lifecycle-stage/post-deployment technique-type/testing technique-type/analytical evidence-type/quantitative-metric evidence-type/test-results expertise-needed/ml-engineering expertise-needed/domain-expertise"
  },
  {
    "slug": "api-usage-pattern-monitoring",
    "name": "API Usage Pattern Monitoring",
    "description": "API usage pattern monitoring analyses AI model API usage to detect anomalies and generate evidence of secure operation. This technique tracks request patterns, input distributions, and usage velocity to produce security reports, anomaly detection evidence, and usage compliance documentation. Monitoring generates quantitative metrics on extraction attempt frequency, adversarial probing patterns, and deviation from intended use, creating auditable evidence for assurance cases.",
    "assurance_goals": [
      "Security",
      "Safety",
      "Transparency"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/security",
      "assurance-goal-category/safety",
      "assurance-goal-category/transparency",
      "data-type/any",
      "data-requirements/no-special-requirements",
      "lifecycle-stage/post-deployment",
      "lifecycle-stage/system-deployment-and-use-monitoring",
      "technique-type/analytical",
      "technique-type/procedural",
      "evidence-type/quantitative-metric",
      "evidence-type/documentation",
      "evidence-type/test-results",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise",
      "expertise-needed/statistical-knowledge"
    ],
    "searchText": "api usage pattern monitoring api usage pattern monitoring analyses ai model api usage to detect anomalies and generate evidence of secure operation. this technique tracks request patterns, input distributions, and usage velocity to produce security reports, anomaly detection evidence, and usage compliance documentation. monitoring generates quantitative metrics on extraction attempt frequency, adversarial probing patterns, and deviation from intended use, creating auditable evidence for assurance cases. security safety transparency applicable-models/architecture/model-agnostic applicable-models/requirements/black-box assurance-goal-category/security assurance-goal-category/safety assurance-goal-category/transparency data-type/any data-requirements/no-special-requirements lifecycle-stage/post-deployment lifecycle-stage/system-deployment-and-use-monitoring technique-type/analytical technique-type/procedural evidence-type/quantitative-metric evidence-type/documentation evidence-type/test-results expertise-needed/ml-engineering expertise-needed/domain-expertise expertise-needed/statistical-knowledge"
  },
  {
    "slug": "data-poisoning-detection",
    "name": "Data Poisoning Detection",
    "description": "Data poisoning detection identifies malicious training data designed to compromise model behaviour. This technique detects various poisoning attacks including backdoor injection (triggers causing specific behaviours), availability attacks (degrading overall performance), and targeted attacks (causing errors on specific inputs). Detection methods include statistical outlier analysis, influence function analysis to identify high-impact training points, and validation-based approaches that test for suspicious model behaviours indicative of poisoning.",
    "assurance_goals": [
      "Security",
      "Reliability",
      "Safety"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/white-box",
      "applicable-models/requirements/gradient-access",
      "assurance-goal-category/security",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/training-data-required",
      "data-type/any",
      "lifecycle-stage/data-collection",
      "lifecycle-stage/model-development",
      "lifecycle-stage/model-evaluation",
      "technique-type/analytical",
      "technique-type/testing",
      "evidence-type/quantitative-metric",
      "evidence-type/test-results",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise"
    ],
    "searchText": "data poisoning detection data poisoning detection identifies malicious training data designed to compromise model behaviour. this technique detects various poisoning attacks including backdoor injection (triggers causing specific behaviours), availability attacks (degrading overall performance), and targeted attacks (causing errors on specific inputs). detection methods include statistical outlier analysis, influence function analysis to identify high-impact training points, and validation-based approaches that test for suspicious model behaviours indicative of poisoning. security reliability safety applicable-models/architecture/model-agnostic applicable-models/requirements/white-box applicable-models/requirements/gradient-access assurance-goal-category/security assurance-goal-category/reliability assurance-goal-category/safety data-requirements/training-data-required data-type/any lifecycle-stage/data-collection lifecycle-stage/model-development lifecycle-stage/model-evaluation technique-type/analytical technique-type/testing evidence-type/quantitative-metric evidence-type/test-results expertise-needed/ml-engineering expertise-needed/domain-expertise"
  },
  {
    "slug": "safety-guardrails",
    "name": "Safety Guardrails",
    "description": "Safety guardrails apply real-time content moderation and safety constraints during deployment, generating evidence of safety violations and protection effectiveness. This technique produces detailed logs of blocked content, safety incident reports, violation pattern analysis, and guardrail performance metrics. Evidence includes categorised threat taxonomies, false positive/negative rates, and temporal trends in attack patterns, supporting assurance claims about deployed system safety.",
    "assurance_goals": [
      "Safety",
      "Security",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/safety",
      "assurance-goal-category/security",
      "assurance-goal-category/reliability",
      "data-type/text",
      "data-requirements/no-special-requirements",
      "lifecycle-stage/deployment",
      "lifecycle-stage/post-deployment",
      "lifecycle-stage/system-deployment-and-use-monitoring",
      "technique-type/algorithmic",
      "technique-type/procedural",
      "evidence-type/quantitative-metric",
      "evidence-type/test-results",
      "evidence-type/documentation",
      "evidence-type/visual-artifact",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise",
      "expertise-needed/basic-technical"
    ],
    "searchText": "safety guardrails safety guardrails apply real-time content moderation and safety constraints during deployment, generating evidence of safety violations and protection effectiveness. this technique produces detailed logs of blocked content, safety incident reports, violation pattern analysis, and guardrail performance metrics. evidence includes categorised threat taxonomies, false positive/negative rates, and temporal trends in attack patterns, supporting assurance claims about deployed system safety. safety security reliability applicable-models/architecture/model-agnostic applicable-models/requirements/black-box assurance-goal-category/safety assurance-goal-category/security assurance-goal-category/reliability data-type/text data-requirements/no-special-requirements lifecycle-stage/deployment lifecycle-stage/post-deployment lifecycle-stage/system-deployment-and-use-monitoring technique-type/algorithmic technique-type/procedural evidence-type/quantitative-metric evidence-type/test-results evidence-type/documentation evidence-type/visual-artifact expertise-needed/ml-engineering expertise-needed/domain-expertise expertise-needed/basic-technical"
  },
  {
    "slug": "jailbreak-resistance-testing",
    "name": "Jailbreak Resistance Testing",
    "description": "Jailbreak resistance testing evaluates LLM defences against techniques that bypass safety constraints. This involves testing role-playing attacks, hypothetical framing, encoded instructions, and multi-turn manipulation. Testing measures immediate resistance to jailbreaks and the system's ability to detect and block evolving attack patterns, producing reports on vulnerability severity and exploit paths.",
    "assurance_goals": [
      "Safety",
      "Security",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks/transformer/llm",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/safety",
      "assurance-goal-category/security",
      "assurance-goal-category/reliability",
      "data-type/text",
      "lifecycle-stage/model-evaluation",
      "lifecycle-stage/post-deployment",
      "technique-type/testing",
      "evidence-type/test-results",
      "evidence-type/quantitative-metric",
      "evidence-type/documentation",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise",
      "data-requirements/no-special-requirements"
    ],
    "searchText": "jailbreak resistance testing jailbreak resistance testing evaluates llm defences against techniques that bypass safety constraints. this involves testing role-playing attacks, hypothetical framing, encoded instructions, and multi-turn manipulation. testing measures immediate resistance to jailbreaks and the system's ability to detect and block evolving attack patterns, producing reports on vulnerability severity and exploit paths. safety security reliability applicable-models/architecture/neural-networks/transformer/llm applicable-models/requirements/black-box assurance-goal-category/safety assurance-goal-category/security assurance-goal-category/reliability data-type/text lifecycle-stage/model-evaluation lifecycle-stage/post-deployment technique-type/testing evidence-type/test-results evidence-type/quantitative-metric evidence-type/documentation expertise-needed/ml-engineering expertise-needed/domain-expertise data-requirements/no-special-requirements"
  },
  {
    "slug": "model-extraction-defence-testing",
    "name": "Model Extraction Defence Testing",
    "description": "Model extraction defence testing evaluates protections against attackers who attempt to steal model functionality by querying it and training surrogate models. This technique assesses defences like query limiting, output perturbation, watermarking, and fingerprinting by simulating extraction attacks and measuring how much model functionality can be replicated. Testing evaluates both the effectiveness of defences in preventing extraction and their impact on legitimate use cases, ensuring security measures don't excessively degrade user experience.",
    "assurance_goals": [
      "Security",
      "Privacy",
      "Transparency"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/security",
      "assurance-goal-category/privacy",
      "assurance-goal-category/transparency",
      "data-type/any",
      "data-requirements/training-data-required",
      "lifecycle-stage/model-evaluation",
      "lifecycle-stage/post-deployment",
      "technique-type/testing",
      "evidence-type/quantitative-metric",
      "evidence-type/test-results",
      "expertise-needed/ml-engineering",
      "expertise-needed/software-engineering"
    ],
    "searchText": "model extraction defence testing model extraction defence testing evaluates protections against attackers who attempt to steal model functionality by querying it and training surrogate models. this technique assesses defences like query limiting, output perturbation, watermarking, and fingerprinting by simulating extraction attacks and measuring how much model functionality can be replicated. testing evaluates both the effectiveness of defences in preventing extraction and their impact on legitimate use cases, ensuring security measures don't excessively degrade user experience. security privacy transparency applicable-models/architecture/model-agnostic applicable-models/requirements/black-box assurance-goal-category/security assurance-goal-category/privacy assurance-goal-category/transparency data-type/any data-requirements/training-data-required lifecycle-stage/model-evaluation lifecycle-stage/post-deployment technique-type/testing evidence-type/quantitative-metric evidence-type/test-results expertise-needed/ml-engineering expertise-needed/software-engineering"
  },
  {
    "slug": "multi-agent-system-testing",
    "name": "Multi-Agent System Testing",
    "description": "Multi-agent system testing evaluates safety and reliability of systems where multiple AI agents interact, coordinate, or compete. This technique assesses emergent behaviours, communication protocols, conflict resolution, and whether agents maintain objectives appropriately. Testing produces reports on agent interaction patterns, coalition formation, failure cascade analysis, and safety property violations in multi-agent scenarios.",
    "assurance_goals": [
      "Safety",
      "Reliability",
      "Security"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/safety",
      "assurance-goal-category/reliability",
      "assurance-goal-category/security",
      "data-type/any",
      "lifecycle-stage/model-evaluation",
      "lifecycle-stage/post-deployment",
      "lifecycle-stage/system-deployment-and-use-monitoring",
      "technique-type/testing",
      "technique-type/analytical",
      "evidence-type/test-results",
      "evidence-type/documentation",
      "evidence-type/qualitative-assessment",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise",
      "expertise-needed/software-engineering",
      "data-requirements/no-special-requirements"
    ],
    "searchText": "multi-agent system testing multi-agent system testing evaluates safety and reliability of systems where multiple ai agents interact, coordinate, or compete. this technique assesses emergent behaviours, communication protocols, conflict resolution, and whether agents maintain objectives appropriately. testing produces reports on agent interaction patterns, coalition formation, failure cascade analysis, and safety property violations in multi-agent scenarios. safety reliability security applicable-models/architecture/model-agnostic applicable-models/requirements/black-box assurance-goal-category/safety assurance-goal-category/reliability assurance-goal-category/security data-type/any lifecycle-stage/model-evaluation lifecycle-stage/post-deployment lifecycle-stage/system-deployment-and-use-monitoring technique-type/testing technique-type/analytical evidence-type/test-results evidence-type/documentation evidence-type/qualitative-assessment expertise-needed/ml-engineering expertise-needed/domain-expertise expertise-needed/software-engineering data-requirements/no-special-requirements"
  },
  {
    "slug": "membership-inference-attack-testing",
    "name": "Membership Inference Attack Testing",
    "description": "Membership inference attack testing evaluates whether adversaries can determine if specific data points were included in a model's training set. This technique simulates attacks where adversaries use model confidence scores, prediction patterns, or loss values to distinguish training data from non-training data. Testing measures privacy leakage by calculating attack success rates, precision-recall trade-offs, and advantage over random guessing. Results inform decisions about privacy-enhancing techniques like differential privacy or regularisation.",
    "assurance_goals": [
      "Privacy",
      "Security",
      "Transparency"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/privacy",
      "assurance-goal-category/security",
      "assurance-goal-category/transparency",
      "data-requirements/training-data-required",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "evidence-type/test-results",
      "expertise-needed/ml-engineering",
      "expertise-needed/security-expertise",
      "lifecycle-stage/model-evaluation",
      "lifecycle-stage/post-deployment",
      "technique-type/testing",
      "technique-type/analytical"
    ],
    "searchText": "membership inference attack testing membership inference attack testing evaluates whether adversaries can determine if specific data points were included in a model's training set. this technique simulates attacks where adversaries use model confidence scores, prediction patterns, or loss values to distinguish training data from non-training data. testing measures privacy leakage by calculating attack success rates, precision-recall trade-offs, and advantage over random guessing. results inform decisions about privacy-enhancing techniques like differential privacy or regularisation. privacy security transparency applicable-models/architecture/model-agnostic applicable-models/requirements/black-box assurance-goal-category/privacy assurance-goal-category/security assurance-goal-category/transparency data-requirements/training-data-required data-type/any evidence-type/quantitative-metric evidence-type/test-results expertise-needed/ml-engineering expertise-needed/security-expertise lifecycle-stage/model-evaluation lifecycle-stage/post-deployment technique-type/testing technique-type/analytical"
  },
  {
    "slug": "model-watermarking-and-theft-detection",
    "name": "Model Watermarking and Theft Detection",
    "description": "Model watermarking and theft detection techniques protect AI systems from unauthorised replication by embedding detectable signatures in model outputs and identifying suspiciously similar prediction patterns. This includes watermarking schemes that survive knowledge distillation, fingerprinting methods that create unique statistical signatures, and detection methods that identify when a model has been stolen or replicated through model extraction, distillation, or imitation. These techniques enable model owners to prove intellectual property theft and protect proprietary AI systems.",
    "assurance_goals": [
      "Security",
      "Transparency",
      "Fairness"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/security",
      "assurance-goal-category/transparency",
      "assurance-goal-category/fairness",
      "data-type/any",
      "data-requirements/no-special-requirements",
      "lifecycle-stage/model-development",
      "lifecycle-stage/deployment",
      "lifecycle-stage/post-deployment",
      "lifecycle-stage/system-deployment-and-use-monitoring",
      "technique-type/algorithmic",
      "technique-type/testing",
      "evidence-type/quantitative-metric",
      "evidence-type/test-results",
      "evidence-type/documentation",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise"
    ],
    "searchText": "model watermarking and theft detection model watermarking and theft detection techniques protect ai systems from unauthorised replication by embedding detectable signatures in model outputs and identifying suspiciously similar prediction patterns. this includes watermarking schemes that survive knowledge distillation, fingerprinting methods that create unique statistical signatures, and detection methods that identify when a model has been stolen or replicated through model extraction, distillation, or imitation. these techniques enable model owners to prove intellectual property theft and protect proprietary ai systems. security transparency fairness applicable-models/architecture/model-agnostic applicable-models/requirements/black-box assurance-goal-category/security assurance-goal-category/transparency assurance-goal-category/fairness data-type/any data-requirements/no-special-requirements lifecycle-stage/model-development lifecycle-stage/deployment lifecycle-stage/post-deployment lifecycle-stage/system-deployment-and-use-monitoring technique-type/algorithmic technique-type/testing evidence-type/quantitative-metric evidence-type/test-results evidence-type/documentation expertise-needed/ml-engineering expertise-needed/domain-expertise"
  },
  {
    "slug": "prompt-injection-testing",
    "name": "Prompt Injection Testing",
    "description": "Prompt injection testing systematically evaluates LLMs and generative AI systems for vulnerabilities where malicious users can override system instructions through carefully crafted inputs. This technique encompasses both offensive security testing (proactively attempting injection attacks to identify vulnerabilities) and defensive detection mechanisms (monitoring for suspicious patterns in prompts and context). Testing involves evaluating various injection patterns including direct instruction manipulation, context confusion, role-playing exploits, delimiter attacks, and context window poisoning. Detection methods analyse context provenance, validate formatting consistency, and identify semantic anomalies that indicate manipulation attempts rather than legitimate user interaction.",
    "assurance_goals": [
      "Security",
      "Safety",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks/transformer/llm",
      "applicable-models/paradigm/generative",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/security",
      "assurance-goal-category/safety",
      "assurance-goal-category/reliability",
      "data-requirements/no-special-requirements",
      "data-type/text",
      "evidence-type/test-results",
      "evidence-type/qualitative-assessment",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise",
      "lifecycle-stage/model-evaluation",
      "lifecycle-stage/deployment",
      "lifecycle-stage/post-deployment",
      "lifecycle-stage/system-deployment-and-use-monitoring",
      "technique-type/testing",
      "technique-type/analytical"
    ],
    "searchText": "prompt injection testing prompt injection testing systematically evaluates llms and generative ai systems for vulnerabilities where malicious users can override system instructions through carefully crafted inputs. this technique encompasses both offensive security testing (proactively attempting injection attacks to identify vulnerabilities) and defensive detection mechanisms (monitoring for suspicious patterns in prompts and context). testing involves evaluating various injection patterns including direct instruction manipulation, context confusion, role-playing exploits, delimiter attacks, and context window poisoning. detection methods analyse context provenance, validate formatting consistency, and identify semantic anomalies that indicate manipulation attempts rather than legitimate user interaction. security safety reliability applicable-models/architecture/neural-networks/transformer/llm applicable-models/paradigm/generative applicable-models/requirements/black-box assurance-goal-category/security assurance-goal-category/safety assurance-goal-category/reliability data-requirements/no-special-requirements data-type/text evidence-type/test-results evidence-type/qualitative-assessment evidence-type/quantitative-metric expertise-needed/ml-engineering expertise-needed/domain-expertise lifecycle-stage/model-evaluation lifecycle-stage/deployment lifecycle-stage/post-deployment lifecycle-stage/system-deployment-and-use-monitoring technique-type/testing technique-type/analytical"
  },
  {
    "slug": "ai-agent-safety-testing",
    "name": "AI Agent Safety Testing",
    "description": "AI agent safety testing evaluates autonomous AI agents that interact with external tools, APIs, and systems to ensure they operate safely and as intended. This technique assesses whether agents correctly understand tool specifications, respect usage constraints, handle errors gracefully, avoid unintended side effects, and make safe decisions during multi-step reasoning. Testing includes boundary condition validation (extreme parameters, edge cases), permission verification (ensuring agents don't exceed authorized actions), and cascade failure analysis (how tool errors propagate through agent workflows).",
    "assurance_goals": [
      "Safety",
      "Security",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks/transformer/llm",
      "applicable-models/paradigm/generative",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/safety",
      "assurance-goal-category/security",
      "assurance-goal-category/reliability",
      "data-type/any",
      "data-requirements/no-special-requirements",
      "lifecycle-stage/model-evaluation",
      "lifecycle-stage/post-deployment",
      "technique-type/testing",
      "evidence-type/test-results",
      "evidence-type/quantitative-metric",
      "evidence-type/documentation",
      "expertise-needed/ml-engineering",
      "expertise-needed/software-engineering",
      "expertise-needed/domain-expertise"
    ],
    "searchText": "ai agent safety testing ai agent safety testing evaluates autonomous ai agents that interact with external tools, apis, and systems to ensure they operate safely and as intended. this technique assesses whether agents correctly understand tool specifications, respect usage constraints, handle errors gracefully, avoid unintended side effects, and make safe decisions during multi-step reasoning. testing includes boundary condition validation (extreme parameters, edge cases), permission verification (ensuring agents don't exceed authorized actions), and cascade failure analysis (how tool errors propagate through agent workflows). safety security reliability applicable-models/architecture/neural-networks/transformer/llm applicable-models/paradigm/generative applicable-models/requirements/black-box assurance-goal-category/safety assurance-goal-category/security assurance-goal-category/reliability data-type/any data-requirements/no-special-requirements lifecycle-stage/model-evaluation lifecycle-stage/post-deployment technique-type/testing evidence-type/test-results evidence-type/quantitative-metric evidence-type/documentation expertise-needed/ml-engineering expertise-needed/software-engineering expertise-needed/domain-expertise"
  }
]