[
  {
    "slug": "out-of-distribution-detector-for-neural-networks",
    "name": "Out-of-Distribution Detector for Neural Networks",
    "description": "ODIN (Out-of-Distribution Detector for Neural Networks) identifies when a neural network encounters inputs significantly different from its training distribution. It enhances detection by applying temperature scaling to soften the model's output distribution and adding small, carefully calibrated perturbations to the input that push in-distribution samples towards higher confidence predictions. By measuring the maximum softmax probability after these adjustments, ODIN can effectively distinguish between in-distribution and out-of-distribution inputs, flagging potentially unreliable predictions before they cause downstream errors.",
    "assurance_goals": [
      "Explainability",
      "Reliability",
      "Safety"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks",
      "applicable-models/paradigm/discriminative",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/probabilistic-output",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/attribution-methods/perturbation-based",
      "assurance-goal-category/explainability/explains/decision-boundaries",
      "assurance-goal-category/explainability/explains/prediction-confidence",
      "assurance-goal-category/explainability/property/consistency",
      "assurance-goal-category/explainability/property/fidelity",
      "assurance-goal-category/explainability/uncertainty-analysis/prediction-uncertainty",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "explanatory-scope/global",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "searchText": "out-of-distribution detector for neural networks odin (out-of-distribution detector for neural networks) identifies when a neural network encounters inputs significantly different from its training distribution. it enhances detection by applying temperature scaling to soften the model's output distribution and adding small, carefully calibrated perturbations to the input that push in-distribution samples towards higher confidence predictions. by measuring the maximum softmax probability after these adjustments, odin can effectively distinguish between in-distribution and out-of-distribution inputs, flagging potentially unreliable predictions before they cause downstream errors. explainability reliability safety applicable-models/architecture/neural-networks applicable-models/paradigm/discriminative applicable-models/paradigm/supervised applicable-models/requirements/probabilistic-output applicable-models/requirements/white-box assurance-goal-category/explainability assurance-goal-category/explainability/attribution-methods/perturbation-based assurance-goal-category/explainability/explains/decision-boundaries assurance-goal-category/explainability/explains/prediction-confidence assurance-goal-category/explainability/property/consistency assurance-goal-category/explainability/property/fidelity assurance-goal-category/explainability/uncertainty-analysis/prediction-uncertainty assurance-goal-category/reliability assurance-goal-category/safety data-requirements/no-special-requirements data-type/any evidence-type/quantitative-metric expertise-needed/ml-engineering expertise-needed/statistics explanatory-scope/global lifecycle-stage/model-development technique-type/algorithmic"
  },
  {
    "slug": "synthetic-data-generation",
    "name": "Synthetic Data Generation",
    "description": "Synthetic data generation creates artificial datasets that aim to preserve the statistical properties, distributions, and relationships of real data whilst containing no actual records from real individuals. The technique encompasses various approaches including generative adversarial networks (GANs), variational autoencoders (VAEs), statistical sampling methods, and privacy-preserving techniques like differential privacy. Beyond privacy protection, synthetic data serves multiple purposes: augmenting limited datasets, balancing class distributions, testing model robustness, enabling data sharing across organisations, and supporting fairness assessments by generating representative samples for underrepresented groups.",
    "assurance_goals": [
      "Privacy",
      "Fairness",
      "Reliability",
      "Safety"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks/generative/gan",
      "applicable-models/architecture/neural-networks/generative/vae",
      "applicable-models/architecture/probabilistic",
      "applicable-models/paradigm/generative",
      "applicable-models/paradigm/unsupervised",
      "applicable-models/requirements/training-data",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/privacy",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/structured-output",
      "expertise-needed/cryptography",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "lifecycle-stage/data-handling",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "searchText": "synthetic data generation synthetic data generation creates artificial datasets that aim to preserve the statistical properties, distributions, and relationships of real data whilst containing no actual records from real individuals. the technique encompasses various approaches including generative adversarial networks (gans), variational autoencoders (vaes), statistical sampling methods, and privacy-preserving techniques like differential privacy. beyond privacy protection, synthetic data serves multiple purposes: augmenting limited datasets, balancing class distributions, testing model robustness, enabling data sharing across organisations, and supporting fairness assessments by generating representative samples for underrepresented groups. privacy fairness reliability safety applicable-models/architecture/neural-networks/generative/gan applicable-models/architecture/neural-networks/generative/vae applicable-models/architecture/probabilistic applicable-models/paradigm/generative applicable-models/paradigm/unsupervised applicable-models/requirements/training-data applicable-models/requirements/white-box assurance-goal-category/fairness assurance-goal-category/privacy assurance-goal-category/reliability assurance-goal-category/safety data-requirements/no-special-requirements data-type/any evidence-type/structured-output expertise-needed/cryptography expertise-needed/ml-engineering expertise-needed/statistics lifecycle-stage/data-handling lifecycle-stage/model-development technique-type/algorithmic"
  },
  {
    "slug": "federated-learning",
    "name": "Federated Learning",
    "description": "Federated learning enables collaborative model training across multiple distributed parties (devices, organisations, or data centres) without requiring centralised data sharing. Participants train models locally on their private datasets and only share model updates (gradients, weights, or aggregated statistics) with a central coordinator. This distributed approach serves multiple purposes: preserving data privacy and sovereignty, reducing communication costs, enabling learning from diverse data sources, improving model robustness through heterogeneous training, and facilitating compliance with data protection regulations whilst maintaining model performance comparable to centralised training.",
    "assurance_goals": [
      "Privacy",
      "Reliability",
      "Safety",
      "Fairness"
    ],
    "tags": [
      "applicable-models/architecture/linear-models",
      "applicable-models/architecture/neural-networks",
      "applicable-models/paradigm/parametric",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/gradient-access",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/privacy",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/software-engineering",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "searchText": "federated learning federated learning enables collaborative model training across multiple distributed parties (devices, organisations, or data centres) without requiring centralised data sharing. participants train models locally on their private datasets and only share model updates (gradients, weights, or aggregated statistics) with a central coordinator. this distributed approach serves multiple purposes: preserving data privacy and sovereignty, reducing communication costs, enabling learning from diverse data sources, improving model robustness through heterogeneous training, and facilitating compliance with data protection regulations whilst maintaining model performance comparable to centralised training. privacy reliability safety fairness applicable-models/architecture/linear-models applicable-models/architecture/neural-networks applicable-models/paradigm/parametric applicable-models/paradigm/supervised applicable-models/requirements/gradient-access applicable-models/requirements/white-box assurance-goal-category/fairness assurance-goal-category/privacy assurance-goal-category/reliability assurance-goal-category/safety data-requirements/no-special-requirements data-type/any evidence-type/quantitative-metric expertise-needed/ml-engineering expertise-needed/software-engineering lifecycle-stage/model-development lifecycle-stage/system-deployment-and-use technique-type/algorithmic"
  },
  {
    "slug": "homomorphic-encryption",
    "name": "Homomorphic Encryption",
    "description": "Homomorphic encryption allows computation on encrypted data without decrypting it first, producing encrypted results that, when decrypted, match the results of performing the same operations on the plaintext. This enables secure outsourced computation where sensitive data remains encrypted throughout processing. By allowing ML operations on encrypted data, it provides strong privacy guarantees for applications involving highly sensitive information.",
    "assurance_goals": [
      "Privacy",
      "Safety",
      "Transparency",
      "Security"
    ],
    "tags": [
      "applicable-models/architecture/linear-models",
      "applicable-models/architecture/neural-networks/feedforward",
      "applicable-models/paradigm/discriminative",
      "applicable-models/paradigm/parametric",
      "applicable-models/requirements/differentiable",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/privacy",
      "assurance-goal-category/privacy/formal-guarantee",
      "assurance-goal-category/safety",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/privacy-guarantee",
      "evidence-type/quantitative-metric",
      "expertise-needed/cryptography",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "searchText": "homomorphic encryption homomorphic encryption allows computation on encrypted data without decrypting it first, producing encrypted results that, when decrypted, match the results of performing the same operations on the plaintext. this enables secure outsourced computation where sensitive data remains encrypted throughout processing. by allowing ml operations on encrypted data, it provides strong privacy guarantees for applications involving highly sensitive information. privacy safety transparency security applicable-models/architecture/linear-models applicable-models/architecture/neural-networks/feedforward applicable-models/paradigm/discriminative applicable-models/paradigm/parametric applicable-models/requirements/differentiable applicable-models/requirements/white-box assurance-goal-category/privacy assurance-goal-category/privacy/formal-guarantee assurance-goal-category/safety assurance-goal-category/transparency data-requirements/no-special-requirements data-type/any evidence-type/privacy-guarantee evidence-type/quantitative-metric expertise-needed/cryptography expertise-needed/ml-engineering lifecycle-stage/model-development lifecycle-stage/system-deployment-and-use technique-type/algorithmic"
  },
  {
    "slug": "deep-ensembles",
    "name": "Deep Ensembles",
    "description": "Deep ensembles combine predictions from multiple neural networks trained independently with different random initializations to capture epistemic uncertainty (model uncertainty). By training several models on the same data with different starting points, the ensemble reveals how much the model's predictions depend on training randomness. The disagreement between ensemble members naturally indicates prediction uncertainty - when models agree, confidence is high; when they disagree, uncertainty is revealed. This approach provides more reliable uncertainty estimates, better out-of-distribution detection, and improved calibration compared to single models.",
    "assurance_goals": [
      "Reliability",
      "Transparency",
      "Safety"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks",
      "applicable-models/paradigm/parametric",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/training-data",
      "assurance-goal-category/reliability",
      "assurance-goal-category/reliability/uncertainty-quantification",
      "assurance-goal-category/safety",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/prediction-interval",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "searchText": "deep ensembles deep ensembles combine predictions from multiple neural networks trained independently with different random initializations to capture epistemic uncertainty (model uncertainty). by training several models on the same data with different starting points, the ensemble reveals how much the model's predictions depend on training randomness. the disagreement between ensemble members naturally indicates prediction uncertainty - when models agree, confidence is high; when they disagree, uncertainty is revealed. this approach provides more reliable uncertainty estimates, better out-of-distribution detection, and improved calibration compared to single models. reliability transparency safety applicable-models/architecture/neural-networks applicable-models/paradigm/parametric applicable-models/paradigm/supervised applicable-models/requirements/training-data assurance-goal-category/reliability assurance-goal-category/reliability/uncertainty-quantification assurance-goal-category/safety assurance-goal-category/transparency data-requirements/no-special-requirements data-type/any evidence-type/prediction-interval evidence-type/quantitative-metric expertise-needed/ml-engineering expertise-needed/statistics lifecycle-stage/model-development lifecycle-stage/system-deployment-and-use technique-type/algorithmic"
  },
  {
    "slug": "safety-envelope-testing",
    "name": "Safety Envelope Testing",
    "description": "Safety envelope testing systematically evaluates AI system performance at the boundaries of its intended operational domain to identify potential failure modes before deployment. The technique involves defining the system's operational design domain (ODD), creating test scenarios that approach or exceed these boundaries, and measuring performance degradation as conditions become more challenging. By testing edge cases, environmental extremes, and boundary conditions, it reveals where the system transitions from safe to unsafe operation, enabling the establishment of clear operational limits and safety margins for deployment.",
    "assurance_goals": [
      "Safety",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/test-scenarios",
      "data-type/any",
      "evidence-type/boundary-analysis",
      "evidence-type/quantitative-metric",
      "expertise-needed/domain-expertise",
      "expertise-needed/safety-engineering",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/testing"
    ],
    "searchText": "safety envelope testing safety envelope testing systematically evaluates ai system performance at the boundaries of its intended operational domain to identify potential failure modes before deployment. the technique involves defining the system's operational design domain (odd), creating test scenarios that approach or exceed these boundaries, and measuring performance degradation as conditions become more challenging. by testing edge cases, environmental extremes, and boundary conditions, it reveals where the system transitions from safe to unsafe operation, enabling the establishment of clear operational limits and safety margins for deployment. safety reliability applicable-models/architecture/model-agnostic applicable-models/requirements/black-box assurance-goal-category/reliability assurance-goal-category/safety data-requirements/test-scenarios data-type/any evidence-type/boundary-analysis evidence-type/quantitative-metric expertise-needed/domain-expertise expertise-needed/safety-engineering lifecycle-stage/model-development lifecycle-stage/system-deployment-and-use technique-type/testing"
  },
  {
    "slug": "internal-review-boards",
    "name": "Internal Review Boards",
    "description": "Internal Review Boards (IRBs) provide independent, systematic evaluation of AI/ML projects throughout their lifecycle to identify ethical, safety, and societal risks before they materialise. Typically composed of multidisciplinary experts including ethicists, domain specialists, legal counsel, community representatives, and technical staff, IRBs review project proposals, assess potential harms to individuals and communities, evaluate mitigation strategies, and establish ongoing monitoring requirements. Unlike traditional research ethics committees, AI-focused IRBs address algorithmic bias, fairness concerns, privacy implications, and societal impact at scale, providing essential governance for responsible AI development and deployment.",
    "assurance_goals": [
      "Safety",
      "Fairness",
      "Transparency"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/safety",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/governance-framework",
      "evidence-type/qualitative-report",
      "expertise-needed/domain-expertise",
      "expertise-needed/ethics",
      "expertise-needed/legal",
      "lifecycle-stage/model-development",
      "lifecycle-stage/project-planning",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/process"
    ],
    "searchText": "internal review boards internal review boards (irbs) provide independent, systematic evaluation of ai/ml projects throughout their lifecycle to identify ethical, safety, and societal risks before they materialise. typically composed of multidisciplinary experts including ethicists, domain specialists, legal counsel, community representatives, and technical staff, irbs review project proposals, assess potential harms to individuals and communities, evaluate mitigation strategies, and establish ongoing monitoring requirements. unlike traditional research ethics committees, ai-focused irbs address algorithmic bias, fairness concerns, privacy implications, and societal impact at scale, providing essential governance for responsible ai development and deployment. safety fairness transparency applicable-models/architecture/model-agnostic applicable-models/requirements/black-box assurance-goal-category/fairness assurance-goal-category/safety assurance-goal-category/transparency data-requirements/no-special-requirements data-type/any evidence-type/governance-framework evidence-type/qualitative-report expertise-needed/domain-expertise expertise-needed/ethics expertise-needed/legal lifecycle-stage/model-development lifecycle-stage/project-planning lifecycle-stage/system-deployment-and-use technique-type/process"
  },
  {
    "slug": "red-teaming",
    "name": "Red Teaming",
    "description": "Red teaming involves systematic adversarial testing of AI/ML systems by dedicated specialists who attempt to identify flaws, vulnerabilities, harmful outputs, and ways to circumvent safety measures. Drawing from cybersecurity practices, red teams employ diverse attack vectors including prompt injection, adversarial examples, edge case exploitation, social engineering scenarios, and goal misalignment probes. Unlike standard testing that validates expected behaviour, red teaming specifically seeks to break systems through creative and adversarial approaches, revealing non-obvious risks and failure modes that could be exploited maliciously or cause harm in deployment.",
    "assurance_goals": [
      "Safety",
      "Reliability",
      "Fairness",
      "Security"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/qualitative-report",
      "expertise-needed/ml-engineering",
      "expertise-needed/security",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/procedural"
    ],
    "searchText": "red teaming red teaming involves systematic adversarial testing of ai/ml systems by dedicated specialists who attempt to identify flaws, vulnerabilities, harmful outputs, and ways to circumvent safety measures. drawing from cybersecurity practices, red teams employ diverse attack vectors including prompt injection, adversarial examples, edge case exploitation, social engineering scenarios, and goal misalignment probes. unlike standard testing that validates expected behaviour, red teaming specifically seeks to break systems through creative and adversarial approaches, revealing non-obvious risks and failure modes that could be exploited maliciously or cause harm in deployment. safety reliability fairness security applicable-models/architecture/model-agnostic applicable-models/requirements/black-box assurance-goal-category/fairness assurance-goal-category/reliability assurance-goal-category/safety data-requirements/no-special-requirements data-type/any evidence-type/qualitative-report expertise-needed/ml-engineering expertise-needed/security lifecycle-stage/system-deployment-and-use technique-type/procedural"
  },
  {
    "slug": "anomaly-detection",
    "name": "Anomaly Detection",
    "description": "Anomaly detection identifies unusual behaviours, inputs, or outputs that deviate significantly from established normal patterns using statistical, machine learning, or rule-based methods. Applied to AI/ML systems, it serves as a continuous monitoring mechanism that can flag unexpected model predictions, suspicious input patterns, data drift, adversarial attacks, or operational malfunctions. By establishing baselines of normal system behaviour and alerting when deviations exceed predefined thresholds, organisations can detect potential security threats, model degradation, fairness violations, or system failures before they cause significant harm.",
    "assurance_goals": [
      "Safety",
      "Reliability",
      "Fairness",
      "Security"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "applicable-models/requirements/training-data",
      "assurance-goal-category/fairness",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "assurance-goal-category/safety/monitoring/anomaly-detection",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "lifecycle-stage/system-deployment-and-use",
      "lifecycle-stage/system-deployment-and-use/monitoring",
      "technique-type/algorithmic"
    ],
    "searchText": "anomaly detection anomaly detection identifies unusual behaviours, inputs, or outputs that deviate significantly from established normal patterns using statistical, machine learning, or rule-based methods. applied to ai/ml systems, it serves as a continuous monitoring mechanism that can flag unexpected model predictions, suspicious input patterns, data drift, adversarial attacks, or operational malfunctions. by establishing baselines of normal system behaviour and alerting when deviations exceed predefined thresholds, organisations can detect potential security threats, model degradation, fairness violations, or system failures before they cause significant harm. safety reliability fairness security applicable-models/architecture/model-agnostic applicable-models/requirements/black-box applicable-models/requirements/training-data assurance-goal-category/fairness assurance-goal-category/reliability assurance-goal-category/safety assurance-goal-category/safety/monitoring/anomaly-detection data-requirements/no-special-requirements data-type/any evidence-type/quantitative-metric expertise-needed/ml-engineering expertise-needed/statistics lifecycle-stage/system-deployment-and-use lifecycle-stage/system-deployment-and-use/monitoring technique-type/algorithmic"
  },
  {
    "slug": "human-in-the-loop-safeguards",
    "name": "Human-in-the-Loop Safeguards",
    "description": "Human-in-the-loop safeguards establish systematic checkpoints where human experts review, validate, or override AI/ML system decisions before they take effect. This governance approach combines automated efficiency with human judgement by defining clear intervention criteria (such as uncertainty thresholds, risk levels, or sensitive contexts) that trigger mandatory human oversight. By incorporating domain expertise, ethical considerations, and contextual understanding that machines may lack, these safeguards help ensure that critical decisions maintain appropriate human accountability whilst preserving the benefits of automated processing for routine cases.",
    "assurance_goals": [
      "Safety",
      "Transparency",
      "Fairness"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/safety",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/qualitative-report",
      "expertise-needed/domain-knowledge",
      "expertise-needed/stakeholder-engagement",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/process"
    ],
    "searchText": "human-in-the-loop safeguards human-in-the-loop safeguards establish systematic checkpoints where human experts review, validate, or override ai/ml system decisions before they take effect. this governance approach combines automated efficiency with human judgement by defining clear intervention criteria (such as uncertainty thresholds, risk levels, or sensitive contexts) that trigger mandatory human oversight. by incorporating domain expertise, ethical considerations, and contextual understanding that machines may lack, these safeguards help ensure that critical decisions maintain appropriate human accountability whilst preserving the benefits of automated processing for routine cases. safety transparency fairness applicable-models/architecture/model-agnostic applicable-models/requirements/black-box assurance-goal-category/fairness assurance-goal-category/safety assurance-goal-category/transparency data-requirements/no-special-requirements data-type/any evidence-type/qualitative-report expertise-needed/domain-knowledge expertise-needed/stakeholder-engagement lifecycle-stage/system-deployment-and-use technique-type/process"
  },
  {
    "slug": "confidence-thresholding",
    "name": "Confidence Thresholding",
    "description": "Confidence thresholding creates decision boundaries based on model uncertainty scores, routing predictions into different handling workflows depending on their confidence levels. High-confidence predictions (e.g., above 95%) proceed automatically, whilst medium-confidence cases (e.g., 70-95%) may trigger additional validation or human review, and low-confidence predictions (below 70%) receive extensive oversight or default to conservative fallback actions. This technique enables organisations to maintain automated efficiency for clear-cut cases whilst ensuring appropriate human intervention for uncertain decisions, balancing operational speed with risk management across safety-critical applications.",
    "assurance_goals": [
      "Safety",
      "Reliability",
      "Transparency"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "applicable-models/requirements/probabilistic-output",
      "assurance-goal-category/reliability",
      "assurance-goal-category/reliability/uncertainty-quantification",
      "assurance-goal-category/safety",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/statistics",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/algorithmic"
    ],
    "searchText": "confidence thresholding confidence thresholding creates decision boundaries based on model uncertainty scores, routing predictions into different handling workflows depending on their confidence levels. high-confidence predictions (e.g., above 95%) proceed automatically, whilst medium-confidence cases (e.g., 70-95%) may trigger additional validation or human review, and low-confidence predictions (below 70%) receive extensive oversight or default to conservative fallback actions. this technique enables organisations to maintain automated efficiency for clear-cut cases whilst ensuring appropriate human intervention for uncertain decisions, balancing operational speed with risk management across safety-critical applications. safety reliability transparency applicable-models/architecture/model-agnostic applicable-models/requirements/black-box applicable-models/requirements/probabilistic-output assurance-goal-category/reliability assurance-goal-category/reliability/uncertainty-quantification assurance-goal-category/safety assurance-goal-category/transparency data-requirements/no-special-requirements data-type/any evidence-type/quantitative-metric expertise-needed/ml-engineering expertise-needed/statistics lifecycle-stage/system-deployment-and-use technique-type/algorithmic"
  },
  {
    "slug": "runtime-monitoring-and-circuit-breakers",
    "name": "Runtime Monitoring and Circuit Breakers",
    "description": "Runtime monitoring and circuit breakers establish continuous surveillance of AI/ML systems in production, tracking critical metrics such as prediction accuracy, response times, input characteristics, output distributions, and system resource usage. When monitored parameters exceed predefined safety thresholds or exhibit anomalous patterns, automated circuit breakers immediately trigger protective actions including request throttling, service degradation, system shutdown, or failover to backup mechanisms. This approach provides real-time defensive capabilities that prevent cascading failures, ensure consistent service reliability, and maintain transparent operation status for stakeholders monitoring system health.",
    "assurance_goals": [
      "Safety",
      "Reliability",
      "Transparency"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "assurance-goal-category/safety/monitoring/anomaly-detection",
      "assurance-goal-category/transparency",
      "data-requirements/no-special-requirements",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/software-engineering",
      "lifecycle-stage/system-deployment-and-use",
      "lifecycle-stage/system-deployment-and-use/monitoring",
      "technique-type/algorithmic"
    ],
    "searchText": "runtime monitoring and circuit breakers runtime monitoring and circuit breakers establish continuous surveillance of ai/ml systems in production, tracking critical metrics such as prediction accuracy, response times, input characteristics, output distributions, and system resource usage. when monitored parameters exceed predefined safety thresholds or exhibit anomalous patterns, automated circuit breakers immediately trigger protective actions including request throttling, service degradation, system shutdown, or failover to backup mechanisms. this approach provides real-time defensive capabilities that prevent cascading failures, ensure consistent service reliability, and maintain transparent operation status for stakeholders monitoring system health. safety reliability transparency applicable-models/architecture/model-agnostic applicable-models/requirements/black-box assurance-goal-category/reliability assurance-goal-category/safety assurance-goal-category/safety/monitoring/anomaly-detection assurance-goal-category/transparency data-requirements/no-special-requirements data-type/any evidence-type/quantitative-metric expertise-needed/ml-engineering expertise-needed/software-engineering lifecycle-stage/system-deployment-and-use lifecycle-stage/system-deployment-and-use/monitoring technique-type/algorithmic"
  },
  {
    "slug": "model-cards",
    "name": "Model Cards",
    "description": "Model cards are standardised documentation frameworks that systematically document machine learning models through structured templates. The templates cover intended use cases, performance metrics across different demographic groups and operating conditions, training data characteristics, evaluation procedures, limitations, and ethical considerations. They serve as comprehensive technical specifications that enable informed model selection, prevent inappropriate deployment, support regulatory compliance, and facilitate fair assessment by providing transparent reporting of model capabilities and constraints across diverse populations and scenarios.",
    "assurance_goals": [
      "Transparency",
      "Fairness",
      "Safety"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/fairness",
      "assurance-goal-category/safety",
      "assurance-goal-category/transparency",
      "assurance-goal-category/transparency/documentation/model-card",
      "data-requirements/access-to-training-data",
      "data-requirements/sensitive-attributes",
      "data-type/any",
      "evidence-type/documentation",
      "expertise-needed/ml-engineering",
      "expertise-needed/regulatory-compliance",
      "expertise-needed/statistics",
      "lifecycle-stage/model-development",
      "lifecycle-stage/system-deployment-and-use",
      "technique-type/documentation"
    ],
    "searchText": "model cards model cards are standardised documentation frameworks that systematically document machine learning models through structured templates. the templates cover intended use cases, performance metrics across different demographic groups and operating conditions, training data characteristics, evaluation procedures, limitations, and ethical considerations. they serve as comprehensive technical specifications that enable informed model selection, prevent inappropriate deployment, support regulatory compliance, and facilitate fair assessment by providing transparent reporting of model capabilities and constraints across diverse populations and scenarios. transparency fairness safety applicable-models/architecture/model-agnostic applicable-models/requirements/black-box assurance-goal-category/fairness assurance-goal-category/safety assurance-goal-category/transparency assurance-goal-category/transparency/documentation/model-card data-requirements/access-to-training-data data-requirements/sensitive-attributes data-type/any evidence-type/documentation expertise-needed/ml-engineering expertise-needed/regulatory-compliance expertise-needed/statistics lifecycle-stage/model-development lifecycle-stage/system-deployment-and-use technique-type/documentation"
  },
  {
    "slug": "model-distillation",
    "name": "Model Distillation",
    "description": "Model distillation transfers knowledge from a large, complex model (teacher) to a smaller, more efficient model (student) by training the student to mimic the teacher's behaviour. The student learns from the teacher's soft predictions and intermediate representations rather than just hard labels, capturing nuanced decision boundaries and uncertainty. This produces models that are faster, require less memory, and are often more interpretable whilst maintaining much of the original performance. Beyond compression, distillation can improve model reliability by regularising training and enable deployment in resource-constrained environments.",
    "assurance_goals": [
      "Explainability",
      "Reliability",
      "Safety"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks",
      "applicable-models/paradigm/parametric",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/training-data",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/explains/decision-boundaries",
      "assurance-goal-category/explainability/explains/internal-mechanisms",
      "assurance-goal-category/explainability/model-simplification/knowledge-transfer",
      "assurance-goal-category/explainability/property/comprehensibility",
      "assurance-goal-category/explainability/property/fidelity",
      "assurance-goal-category/explainability/surrogate-models/global-surrogates",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/access-to-training-data",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/deployment",
      "lifecycle-stage/model-development",
      "technique-type/algorithmic"
    ],
    "searchText": "model distillation model distillation transfers knowledge from a large, complex model (teacher) to a smaller, more efficient model (student) by training the student to mimic the teacher's behaviour. the student learns from the teacher's soft predictions and intermediate representations rather than just hard labels, capturing nuanced decision boundaries and uncertainty. this produces models that are faster, require less memory, and are often more interpretable whilst maintaining much of the original performance. beyond compression, distillation can improve model reliability by regularising training and enable deployment in resource-constrained environments. explainability reliability safety applicable-models/architecture/neural-networks applicable-models/paradigm/parametric applicable-models/paradigm/supervised applicable-models/requirements/training-data applicable-models/requirements/white-box assurance-goal-category/explainability assurance-goal-category/explainability/explains/decision-boundaries assurance-goal-category/explainability/explains/internal-mechanisms assurance-goal-category/explainability/model-simplification/knowledge-transfer assurance-goal-category/explainability/property/comprehensibility assurance-goal-category/explainability/property/fidelity assurance-goal-category/explainability/surrogate-models/global-surrogates assurance-goal-category/reliability assurance-goal-category/safety data-requirements/access-to-training-data data-type/any evidence-type/quantitative-metric expertise-needed/ml-engineering lifecycle-stage/deployment lifecycle-stage/model-development technique-type/algorithmic"
  },
  {
    "slug": "model-pruning",
    "name": "Model Pruning",
    "description": "Model pruning systematically removes less important weights, neurons, or entire layers from neural networks to create smaller, more efficient models whilst maintaining performance. This process involves iterative removal based on importance criteria (weight magnitudes, gradient information, activation patterns) followed by fine-tuning. Pruning can be structured (removing entire neurons/channels) or unstructured (removing individual weights), with structured pruning providing greater computational benefits and interpretability through simplified architectures.",
    "assurance_goals": [
      "Explainability",
      "Reliability",
      "Safety"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks",
      "applicable-models/paradigm/parametric",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/gradient-access",
      "applicable-models/requirements/model-internals",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/explains/feature-importance",
      "assurance-goal-category/explainability/explains/internal-mechanisms",
      "assurance-goal-category/explainability/model-simplification/pruning",
      "assurance-goal-category/explainability/property/comprehensibility",
      "assurance-goal-category/explainability/property/sparsity",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/access-to-model-internals",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/model-development",
      "lifecycle-stage/model-optimization",
      "technique-type/algorithmic"
    ],
    "searchText": "model pruning model pruning systematically removes less important weights, neurons, or entire layers from neural networks to create smaller, more efficient models whilst maintaining performance. this process involves iterative removal based on importance criteria (weight magnitudes, gradient information, activation patterns) followed by fine-tuning. pruning can be structured (removing entire neurons/channels) or unstructured (removing individual weights), with structured pruning providing greater computational benefits and interpretability through simplified architectures. explainability reliability safety applicable-models/architecture/neural-networks applicable-models/paradigm/parametric applicable-models/paradigm/supervised applicable-models/requirements/gradient-access applicable-models/requirements/model-internals applicable-models/requirements/white-box assurance-goal-category/explainability assurance-goal-category/explainability/explains/feature-importance assurance-goal-category/explainability/explains/internal-mechanisms assurance-goal-category/explainability/model-simplification/pruning assurance-goal-category/explainability/property/comprehensibility assurance-goal-category/explainability/property/sparsity assurance-goal-category/reliability assurance-goal-category/safety data-requirements/access-to-model-internals data-type/any evidence-type/quantitative-metric expertise-needed/ml-engineering lifecycle-stage/model-development lifecycle-stage/model-optimization technique-type/algorithmic"
  },
  {
    "slug": "neuron-activation-analysis",
    "name": "Neuron Activation Analysis",
    "description": "Neuron activation analysis examines the firing patterns of individual neurons in neural networks by probing them with diverse inputs and analysing their activation responses. This technique helps understand what concepts, features, or patterns different neurons have learned to recognise, providing insights into the model's internal representations. For large language models, this can reveal neurons specialised for linguistic concepts, semantic categories, or even potentially harmful patterns, enabling targeted interventions and deeper model understanding.",
    "assurance_goals": [
      "Explainability",
      "Safety",
      "Fairness"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks",
      "applicable-models/requirements/model-internals",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/explains/data-patterns",
      "assurance-goal-category/explainability/explains/internal-mechanisms",
      "assurance-goal-category/explainability/property/comprehensibility",
      "assurance-goal-category/explainability/representation-analysis/concept-identification",
      "assurance-goal-category/fairness",
      "assurance-goal-category/safety",
      "data-requirements/access-to-model-internals",
      "data-type/text",
      "evidence-type/quantitative-metric",
      "evidence-type/visualisation",
      "expertise-needed/ml-engineering",
      "explanatory-scope/global",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "lifecycle-stage/monitoring",
      "lifecycle-stage/testing",
      "technique-type/algorithmic"
    ],
    "searchText": "neuron activation analysis neuron activation analysis examines the firing patterns of individual neurons in neural networks by probing them with diverse inputs and analysing their activation responses. this technique helps understand what concepts, features, or patterns different neurons have learned to recognise, providing insights into the model's internal representations. for large language models, this can reveal neurons specialised for linguistic concepts, semantic categories, or even potentially harmful patterns, enabling targeted interventions and deeper model understanding. explainability safety fairness applicable-models/architecture/neural-networks applicable-models/requirements/model-internals applicable-models/requirements/white-box assurance-goal-category/explainability assurance-goal-category/explainability/explains/data-patterns assurance-goal-category/explainability/explains/internal-mechanisms assurance-goal-category/explainability/property/comprehensibility assurance-goal-category/explainability/representation-analysis/concept-identification assurance-goal-category/fairness assurance-goal-category/safety data-requirements/access-to-model-internals data-type/text evidence-type/quantitative-metric evidence-type/visualisation expertise-needed/ml-engineering explanatory-scope/global explanatory-scope/local lifecycle-stage/model-development lifecycle-stage/monitoring lifecycle-stage/testing technique-type/algorithmic"
  },
  {
    "slug": "prompt-sensitivity-analysis",
    "name": "Prompt Sensitivity Analysis",
    "description": "Prompt Sensitivity Analysis systematically evaluates how variations in input prompts affect large language model outputs, providing insights into model robustness, consistency, and interpretability. This technique involves creating controlled perturbations of prompts whilst maintaining semantic meaning, then measuring how these changes influence model responses. It encompasses various types of prompt modifications including lexical substitutions, syntactic restructuring, formatting changes, and contextual variations. The analysis typically quantifies sensitivity through metrics such as output consistency, semantic similarity, and statistical measures of variance across prompt variations.",
    "assurance_goals": [
      "Explainability",
      "Reliability",
      "Safety"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks/transformer/llm",
      "applicable-models/paradigm/generative",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/attribution-methods/perturbation-based",
      "assurance-goal-category/explainability/explains/internal-mechanisms",
      "assurance-goal-category/explainability/explains/prediction-confidence",
      "assurance-goal-category/explainability/property/consistency",
      "assurance-goal-category/explainability/property/fidelity",
      "assurance-goal-category/explainability/uncertainty-analysis/sensitivity-testing",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/no-special-requirements",
      "data-type/text",
      "evidence-type/quantitative-metric",
      "expertise-needed/experimental-design",
      "expertise-needed/linguistics",
      "expertise-needed/statistics",
      "lifecycle-stage/model-development",
      "lifecycle-stage/model-evaluation",
      "technique-type/experimental"
    ],
    "searchText": "prompt sensitivity analysis prompt sensitivity analysis systematically evaluates how variations in input prompts affect large language model outputs, providing insights into model robustness, consistency, and interpretability. this technique involves creating controlled perturbations of prompts whilst maintaining semantic meaning, then measuring how these changes influence model responses. it encompasses various types of prompt modifications including lexical substitutions, syntactic restructuring, formatting changes, and contextual variations. the analysis typically quantifies sensitivity through metrics such as output consistency, semantic similarity, and statistical measures of variance across prompt variations. explainability reliability safety applicable-models/architecture/neural-networks/transformer/llm applicable-models/paradigm/generative applicable-models/requirements/black-box assurance-goal-category/explainability assurance-goal-category/explainability/attribution-methods/perturbation-based assurance-goal-category/explainability/explains/internal-mechanisms assurance-goal-category/explainability/explains/prediction-confidence assurance-goal-category/explainability/property/consistency assurance-goal-category/explainability/property/fidelity assurance-goal-category/explainability/uncertainty-analysis/sensitivity-testing assurance-goal-category/reliability assurance-goal-category/safety data-requirements/no-special-requirements data-type/text evidence-type/quantitative-metric expertise-needed/experimental-design expertise-needed/linguistics expertise-needed/statistics lifecycle-stage/model-development lifecycle-stage/model-evaluation technique-type/experimental"
  },
  {
    "slug": "causal-mediation-analysis-in-language-models",
    "name": "Causal Mediation Analysis in Language Models",
    "description": "Causal mediation analysis in language models is a mechanistic interpretability technique that systematically investigates how specific internal components (neurons, attention heads, or layers) causally contribute to model outputs. By performing controlled interventionssuch as activating, deactivating, or modifying specific componentsresearchers can trace the causal pathways through which information flows and transforms within the model. This approach goes beyond correlation to establish causal relationships, enabling researchers to understand not just what features influence outputs, but how and why they do so through specific computational pathways.",
    "assurance_goals": [
      "Explainability",
      "Reliability",
      "Safety"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks/transformer",
      "applicable-models/architecture/neural-networks/transformer/llm",
      "applicable-models/paradigm/generative",
      "applicable-models/requirements/model-internals",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/causal-analysis/mediation-analysis",
      "assurance-goal-category/explainability/explains/causal-pathways",
      "assurance-goal-category/explainability/explains/internal-mechanisms",
      "assurance-goal-category/explainability/property/causality",
      "assurance-goal-category/explainability/property/fidelity",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/access-to-model-internals",
      "data-type/text",
      "evidence-type/causal-analysis",
      "expertise-needed/causal-inference",
      "expertise-needed/ml-engineering",
      "lifecycle-stage/model-development",
      "lifecycle-stage/post-deployment",
      "technique-type/mechanistic-interpretability"
    ],
    "searchText": "causal mediation analysis in language models causal mediation analysis in language models is a mechanistic interpretability technique that systematically investigates how specific internal components (neurons, attention heads, or layers) causally contribute to model outputs. by performing controlled interventionssuch as activating, deactivating, or modifying specific componentsresearchers can trace the causal pathways through which information flows and transforms within the model. this approach goes beyond correlation to establish causal relationships, enabling researchers to understand not just what features influence outputs, but how and why they do so through specific computational pathways. explainability reliability safety applicable-models/architecture/neural-networks/transformer applicable-models/architecture/neural-networks/transformer/llm applicable-models/paradigm/generative applicable-models/requirements/model-internals applicable-models/requirements/white-box assurance-goal-category/explainability assurance-goal-category/explainability/causal-analysis/mediation-analysis assurance-goal-category/explainability/explains/causal-pathways assurance-goal-category/explainability/explains/internal-mechanisms assurance-goal-category/explainability/property/causality assurance-goal-category/explainability/property/fidelity assurance-goal-category/reliability assurance-goal-category/safety data-requirements/access-to-model-internals data-type/text evidence-type/causal-analysis expertise-needed/causal-inference expertise-needed/ml-engineering lifecycle-stage/model-development lifecycle-stage/post-deployment technique-type/mechanistic-interpretability"
  },
  {
    "slug": "feature-attribution-with-integrated-gradients-in-nlp",
    "name": "Feature Attribution with Integrated Gradients in NLP",
    "description": "Applies Integrated Gradients to natural language processing models to attribute prediction importance to individual input tokens, words, or subword units. This technique computes gradients along a straight-line path from a baseline input (typically all-zeros, padding tokens, or neutral text) to the actual input, integrating these gradients to obtain attribution scores. Unlike vanilla gradient methods, Integrated Gradients satisfies axioms of sensitivity and implementation invariance, making it particularly valuable for understanding transformer-based language models where token interactions are complex.",
    "assurance_goals": [
      "Explainability",
      "Fairness",
      "Safety"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks/transformer",
      "applicable-models/architecture/neural-networks/transformer/llm",
      "applicable-models/paradigm/discriminative",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/gradient-access",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/attribution-methods/gradient-based",
      "assurance-goal-category/explainability/explains/feature-importance",
      "assurance-goal-category/explainability/property/completeness",
      "assurance-goal-category/fairness",
      "assurance-goal-category/safety",
      "data-requirements/no-special-requirements",
      "data-type/text",
      "evidence-type/quantitative-metric",
      "evidence-type/visualisation",
      "expertise-needed/ml-engineering",
      "explanatory-scope/local",
      "lifecycle-stage/model-development",
      "lifecycle-stage/testing",
      "technique-type/algorithmic"
    ],
    "searchText": "feature attribution with integrated gradients in nlp applies integrated gradients to natural language processing models to attribute prediction importance to individual input tokens, words, or subword units. this technique computes gradients along a straight-line path from a baseline input (typically all-zeros, padding tokens, or neutral text) to the actual input, integrating these gradients to obtain attribution scores. unlike vanilla gradient methods, integrated gradients satisfies axioms of sensitivity and implementation invariance, making it particularly valuable for understanding transformer-based language models where token interactions are complex. explainability fairness safety applicable-models/architecture/neural-networks/transformer applicable-models/architecture/neural-networks/transformer/llm applicable-models/paradigm/discriminative applicable-models/paradigm/supervised applicable-models/requirements/gradient-access applicable-models/requirements/white-box assurance-goal-category/explainability assurance-goal-category/explainability/attribution-methods/gradient-based assurance-goal-category/explainability/explains/feature-importance assurance-goal-category/explainability/property/completeness assurance-goal-category/fairness assurance-goal-category/safety data-requirements/no-special-requirements data-type/text evidence-type/quantitative-metric evidence-type/visualisation expertise-needed/ml-engineering explanatory-scope/local lifecycle-stage/model-development lifecycle-stage/testing technique-type/algorithmic"
  },
  {
    "slug": "model-development-audit-trails",
    "name": "Model Development Audit Trails",
    "description": "Model development audit trails create comprehensive, immutable records of all decisions, experiments, and changes throughout the ML lifecycle. This technique captures dataset versions, training configurations, hyperparameter searches, model architecture changes, evaluation results, and deployment decisions in tamper-evident logs. Audit trails enable reproducibility, accountability, and regulatory compliance by documenting who made what changes when and why, supporting post-hoc investigation of model failures or unexpected behaviours.",
    "assurance_goals": [
      "Transparency",
      "Reliability",
      "Safety"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/transparency",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-type/any",
      "lifecycle-stage/project-planning",
      "lifecycle-stage/data-collection",
      "lifecycle-stage/model-development",
      "lifecycle-stage/model-evaluation",
      "lifecycle-stage/deployment",
      "lifecycle-stage/post-deployment",
      "technique-type/procedural",
      "technique-type/governance-framework",
      "evidence-type/documentation",
      "expertise-needed/software-engineering",
      "expertise-needed/ml-engineering",
      "data-requirements/no-special-requirements"
    ],
    "searchText": "model development audit trails model development audit trails create comprehensive, immutable records of all decisions, experiments, and changes throughout the ml lifecycle. this technique captures dataset versions, training configurations, hyperparameter searches, model architecture changes, evaluation results, and deployment decisions in tamper-evident logs. audit trails enable reproducibility, accountability, and regulatory compliance by documenting who made what changes when and why, supporting post-hoc investigation of model failures or unexpected behaviours. transparency reliability safety applicable-models/architecture/model-agnostic applicable-models/requirements/black-box assurance-goal-category/transparency assurance-goal-category/reliability assurance-goal-category/safety data-type/any lifecycle-stage/project-planning lifecycle-stage/data-collection lifecycle-stage/model-development lifecycle-stage/model-evaluation lifecycle-stage/deployment lifecycle-stage/post-deployment technique-type/procedural technique-type/governance-framework evidence-type/documentation expertise-needed/software-engineering expertise-needed/ml-engineering data-requirements/no-special-requirements"
  },
  {
    "slug": "adversarial-robustness-testing",
    "name": "Adversarial Robustness Testing",
    "description": "Adversarial robustness testing evaluates model resilience against intentionally crafted inputs designed to cause misclassification or malfunction. This technique generates adversarial examples through methods like FGSM, PGD, Carlini & Wagner (C&W) attacks, and AutoAttack to measure model vulnerability. Testing assesses both white-box scenarios (where attackers have full model access) and black-box scenarios (where attackers only observe inputs and outputs), measuring metrics like robust accuracy, attack success rates, and perturbation budgets required for successful attacks.",
    "assurance_goals": [
      "Security",
      "Reliability",
      "Safety"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/architecture/neural-networks",
      "applicable-models/requirements/black-box",
      "applicable-models/requirements/white-box",
      "assurance-goal-category/security",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/no-special-requirements",
      "data-type/image",
      "data-type/text",
      "data-type/tabular",
      "evidence-type/quantitative-metric",
      "evidence-type/test-results",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise",
      "lifecycle-stage/model-evaluation",
      "lifecycle-stage/post-deployment",
      "technique-type/testing"
    ],
    "searchText": "adversarial robustness testing adversarial robustness testing evaluates model resilience against intentionally crafted inputs designed to cause misclassification or malfunction. this technique generates adversarial examples through methods like fgsm, pgd, carlini & wagner (c&w) attacks, and autoattack to measure model vulnerability. testing assesses both white-box scenarios (where attackers have full model access) and black-box scenarios (where attackers only observe inputs and outputs), measuring metrics like robust accuracy, attack success rates, and perturbation budgets required for successful attacks. security reliability safety applicable-models/architecture/model-agnostic applicable-models/architecture/neural-networks applicable-models/requirements/black-box applicable-models/requirements/white-box assurance-goal-category/security assurance-goal-category/reliability assurance-goal-category/safety data-requirements/no-special-requirements data-type/image data-type/text data-type/tabular evidence-type/quantitative-metric evidence-type/test-results expertise-needed/ml-engineering expertise-needed/domain-expertise lifecycle-stage/model-evaluation lifecycle-stage/post-deployment technique-type/testing"
  },
  {
    "slug": "agent-goal-misalignment-testing",
    "name": "Agent Goal Misalignment Testing",
    "description": "Agent goal misalignment testing identifies scenarios where AI agents pursue objectives in unintended ways or develop proxy goals that diverge from true human intent. This technique tests for specification gaming (achieving stated metrics while violating intent), instrumental goals (agents developing problematic sub-goals to achieve main objectives), and reward hacking (exploiting loopholes in reward functions). Testing uses diverse scenarios to probe whether agents truly understand task intent or merely optimise narrow specified metrics.",
    "assurance_goals": [
      "Safety",
      "Reliability",
      "Fairness"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/reinforcement",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/safety",
      "assurance-goal-category/reliability",
      "assurance-goal-category/fairness",
      "data-type/any",
      "data-requirements/no-special-requirements",
      "lifecycle-stage/model-evaluation",
      "lifecycle-stage/post-deployment",
      "technique-type/testing",
      "evidence-type/test-results",
      "evidence-type/qualitative-assessment",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise"
    ],
    "searchText": "agent goal misalignment testing agent goal misalignment testing identifies scenarios where ai agents pursue objectives in unintended ways or develop proxy goals that diverge from true human intent. this technique tests for specification gaming (achieving stated metrics while violating intent), instrumental goals (agents developing problematic sub-goals to achieve main objectives), and reward hacking (exploiting loopholes in reward functions). testing uses diverse scenarios to probe whether agents truly understand task intent or merely optimise narrow specified metrics. safety reliability fairness applicable-models/architecture/model-agnostic applicable-models/paradigm/reinforcement applicable-models/requirements/black-box assurance-goal-category/safety assurance-goal-category/reliability assurance-goal-category/fairness data-type/any data-requirements/no-special-requirements lifecycle-stage/model-evaluation lifecycle-stage/post-deployment technique-type/testing evidence-type/test-results evidence-type/qualitative-assessment expertise-needed/ml-engineering expertise-needed/domain-expertise"
  },
  {
    "slug": "api-usage-pattern-monitoring",
    "name": "API Usage Pattern Monitoring",
    "description": "API usage pattern monitoring analyses AI model API usage to detect anomalies and generate evidence of secure operation. This technique tracks request patterns, input distributions, and usage velocity to produce security reports, anomaly detection evidence, and usage compliance documentation. Monitoring generates quantitative metrics on extraction attempt frequency, adversarial probing patterns, and deviation from intended use, creating auditable evidence for assurance cases.",
    "assurance_goals": [
      "Security",
      "Safety",
      "Transparency"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/security",
      "assurance-goal-category/safety",
      "assurance-goal-category/transparency",
      "data-type/any",
      "data-requirements/no-special-requirements",
      "lifecycle-stage/post-deployment",
      "lifecycle-stage/system-deployment-and-use-monitoring",
      "technique-type/analytical",
      "technique-type/procedural",
      "evidence-type/quantitative-metric",
      "evidence-type/documentation",
      "evidence-type/test-results",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise",
      "expertise-needed/statistical-knowledge"
    ],
    "searchText": "api usage pattern monitoring api usage pattern monitoring analyses ai model api usage to detect anomalies and generate evidence of secure operation. this technique tracks request patterns, input distributions, and usage velocity to produce security reports, anomaly detection evidence, and usage compliance documentation. monitoring generates quantitative metrics on extraction attempt frequency, adversarial probing patterns, and deviation from intended use, creating auditable evidence for assurance cases. security safety transparency applicable-models/architecture/model-agnostic applicable-models/requirements/black-box assurance-goal-category/security assurance-goal-category/safety assurance-goal-category/transparency data-type/any data-requirements/no-special-requirements lifecycle-stage/post-deployment lifecycle-stage/system-deployment-and-use-monitoring technique-type/analytical technique-type/procedural evidence-type/quantitative-metric evidence-type/documentation evidence-type/test-results expertise-needed/ml-engineering expertise-needed/domain-expertise expertise-needed/statistical-knowledge"
  },
  {
    "slug": "constitutional-ai-evaluation",
    "name": "Constitutional AI Evaluation",
    "description": "Constitutional AI evaluation assesses models trained to follow explicit behavioural principles or 'constitutions' that specify desired values and constraints. This technique verifies that models consistently adhere to specified principles across diverse scenarios, measuring both principle compliance and handling of principle conflicts. Evaluation includes testing boundary cases where principles might conflict, measuring robustness to adversarial attempts to violate principles, and assessing whether models can explain their reasoning in terms of constitutional principles.",
    "assurance_goals": [
      "Safety",
      "Transparency",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks/transformer/llm",
      "applicable-models/requirements/white-box",
      "applicable-models/paradigm/supervised",
      "assurance-goal-category/safety",
      "assurance-goal-category/transparency",
      "assurance-goal-category/reliability",
      "data-type/text",
      "data-requirements/labeled-data-required",
      "data-requirements/training-data-required",
      "lifecycle-stage/model-evaluation",
      "lifecycle-stage/post-deployment",
      "technique-type/testing",
      "technique-type/analytical",
      "evidence-type/quantitative-metric",
      "evidence-type/qualitative-assessment",
      "evidence-type/test-results",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise"
    ],
    "searchText": "constitutional ai evaluation constitutional ai evaluation assesses models trained to follow explicit behavioural principles or 'constitutions' that specify desired values and constraints. this technique verifies that models consistently adhere to specified principles across diverse scenarios, measuring both principle compliance and handling of principle conflicts. evaluation includes testing boundary cases where principles might conflict, measuring robustness to adversarial attempts to violate principles, and assessing whether models can explain their reasoning in terms of constitutional principles. safety transparency reliability applicable-models/architecture/neural-networks/transformer/llm applicable-models/requirements/white-box applicable-models/paradigm/supervised assurance-goal-category/safety assurance-goal-category/transparency assurance-goal-category/reliability data-type/text data-requirements/labeled-data-required data-requirements/training-data-required lifecycle-stage/model-evaluation lifecycle-stage/post-deployment technique-type/testing technique-type/analytical evidence-type/quantitative-metric evidence-type/qualitative-assessment evidence-type/test-results expertise-needed/ml-engineering expertise-needed/domain-expertise"
  },
  {
    "slug": "continual-learning-stability-testing",
    "name": "Continual Learning Stability Testing",
    "description": "Continual learning stability testing evaluates whether models that learn from streaming data maintain performance on previously learned tasks while acquiring new capabilities. This technique measures catastrophic forgetting (performance degradation on old tasks), forward transfer (whether old knowledge helps new learning), and backward transfer (whether new learning damages old performance). Testing includes challenging scenarios where data distributions shift significantly and evaluates whether stability techniques like experience replay or regularisation effectively preserve knowledge.",
    "assurance_goals": [
      "Reliability",
      "Safety",
      "Fairness"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/architecture/neural-networks",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "assurance-goal-category/fairness",
      "data-requirements/training-data-required",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "evidence-type/test-results",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise",
      "lifecycle-stage/model-evaluation",
      "lifecycle-stage/post-deployment",
      "lifecycle-stage/system-deployment-and-use-monitoring",
      "technique-type/testing"
    ],
    "searchText": "continual learning stability testing continual learning stability testing evaluates whether models that learn from streaming data maintain performance on previously learned tasks while acquiring new capabilities. this technique measures catastrophic forgetting (performance degradation on old tasks), forward transfer (whether old knowledge helps new learning), and backward transfer (whether new learning damages old performance). testing includes challenging scenarios where data distributions shift significantly and evaluates whether stability techniques like experience replay or regularisation effectively preserve knowledge. reliability safety fairness applicable-models/architecture/model-agnostic applicable-models/architecture/neural-networks applicable-models/requirements/black-box assurance-goal-category/reliability assurance-goal-category/safety assurance-goal-category/fairness data-requirements/training-data-required data-type/any evidence-type/quantitative-metric evidence-type/test-results expertise-needed/ml-engineering expertise-needed/domain-expertise lifecycle-stage/model-evaluation lifecycle-stage/post-deployment lifecycle-stage/system-deployment-and-use-monitoring technique-type/testing"
  },
  {
    "slug": "data-poisoning-detection",
    "name": "Data Poisoning Detection",
    "description": "Data poisoning detection identifies malicious training data designed to compromise model behaviour. This technique detects various poisoning attacks including backdoor injection (triggers causing specific behaviours), availability attacks (degrading overall performance), and targeted attacks (causing errors on specific inputs). Detection methods include statistical outlier analysis, influence function analysis to identify high-impact training points, and validation-based approaches that test for suspicious model behaviours indicative of poisoning.",
    "assurance_goals": [
      "Security",
      "Reliability",
      "Safety"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/white-box",
      "applicable-models/requirements/gradient-access",
      "assurance-goal-category/security",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/training-data-required",
      "data-type/any",
      "lifecycle-stage/data-collection",
      "lifecycle-stage/model-development",
      "lifecycle-stage/model-evaluation",
      "technique-type/analytical",
      "technique-type/testing",
      "evidence-type/quantitative-metric",
      "evidence-type/test-results",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise"
    ],
    "searchText": "data poisoning detection data poisoning detection identifies malicious training data designed to compromise model behaviour. this technique detects various poisoning attacks including backdoor injection (triggers causing specific behaviours), availability attacks (degrading overall performance), and targeted attacks (causing errors on specific inputs). detection methods include statistical outlier analysis, influence function analysis to identify high-impact training points, and validation-based approaches that test for suspicious model behaviours indicative of poisoning. security reliability safety applicable-models/architecture/model-agnostic applicable-models/requirements/white-box applicable-models/requirements/gradient-access assurance-goal-category/security assurance-goal-category/reliability assurance-goal-category/safety data-requirements/training-data-required data-type/any lifecycle-stage/data-collection lifecycle-stage/model-development lifecycle-stage/model-evaluation technique-type/analytical technique-type/testing evidence-type/quantitative-metric evidence-type/test-results expertise-needed/ml-engineering expertise-needed/domain-expertise"
  },
  {
    "slug": "hallucination-detection",
    "name": "Hallucination Detection",
    "description": "Hallucination detection identifies when generative models produce factually incorrect, fabricated, or ungrounded outputs. This technique combines automated consistency checking, self-consistency methods, uncertainty quantification, and human evaluation. It produces detection scores distinguishing intrinsic hallucinations (contradicting sources) from extrinsic hallucinations (unverifiable claims), enabling filtering or user warnings.",
    "assurance_goals": [
      "Reliability",
      "Transparency",
      "Safety"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks/transformer",
      "applicable-models/architecture/neural-networks/transformer/llm",
      "applicable-models/paradigm/generative",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/reliability",
      "assurance-goal-category/transparency",
      "assurance-goal-category/safety",
      "data-type/text",
      "data-requirements/training-data-required",
      "evidence-type/quantitative-metric",
      "evidence-type/test-results",
      "evidence-type/qualitative-assessment",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise",
      "lifecycle-stage/model-evaluation",
      "lifecycle-stage/post-deployment",
      "technique-type/testing",
      "technique-type/analytical"
    ],
    "searchText": "hallucination detection hallucination detection identifies when generative models produce factually incorrect, fabricated, or ungrounded outputs. this technique combines automated consistency checking, self-consistency methods, uncertainty quantification, and human evaluation. it produces detection scores distinguishing intrinsic hallucinations (contradicting sources) from extrinsic hallucinations (unverifiable claims), enabling filtering or user warnings. reliability transparency safety applicable-models/architecture/neural-networks/transformer applicable-models/architecture/neural-networks/transformer/llm applicable-models/paradigm/generative applicable-models/requirements/black-box assurance-goal-category/reliability assurance-goal-category/transparency assurance-goal-category/safety data-type/text data-requirements/training-data-required evidence-type/quantitative-metric evidence-type/test-results evidence-type/qualitative-assessment expertise-needed/ml-engineering expertise-needed/domain-expertise lifecycle-stage/model-evaluation lifecycle-stage/post-deployment technique-type/testing technique-type/analytical"
  },
  {
    "slug": "safety-guardrails",
    "name": "Safety Guardrails",
    "description": "Safety guardrails apply real-time content moderation and safety constraints during deployment, generating evidence of safety violations and protection effectiveness. This technique produces detailed logs of blocked content, safety incident reports, violation pattern analysis, and guardrail performance metrics. Evidence includes categorised threat taxonomies, false positive/negative rates, and temporal trends in attack patterns, supporting assurance claims about deployed system safety.",
    "assurance_goals": [
      "Safety",
      "Security",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/safety",
      "assurance-goal-category/security",
      "assurance-goal-category/reliability",
      "data-type/text",
      "data-requirements/no-special-requirements",
      "lifecycle-stage/deployment",
      "lifecycle-stage/post-deployment",
      "lifecycle-stage/system-deployment-and-use-monitoring",
      "technique-type/algorithmic",
      "technique-type/procedural",
      "evidence-type/quantitative-metric",
      "evidence-type/test-results",
      "evidence-type/documentation",
      "evidence-type/visual-artifact",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise",
      "expertise-needed/basic-technical"
    ],
    "searchText": "safety guardrails safety guardrails apply real-time content moderation and safety constraints during deployment, generating evidence of safety violations and protection effectiveness. this technique produces detailed logs of blocked content, safety incident reports, violation pattern analysis, and guardrail performance metrics. evidence includes categorised threat taxonomies, false positive/negative rates, and temporal trends in attack patterns, supporting assurance claims about deployed system safety. safety security reliability applicable-models/architecture/model-agnostic applicable-models/requirements/black-box assurance-goal-category/safety assurance-goal-category/security assurance-goal-category/reliability data-type/text data-requirements/no-special-requirements lifecycle-stage/deployment lifecycle-stage/post-deployment lifecycle-stage/system-deployment-and-use-monitoring technique-type/algorithmic technique-type/procedural evidence-type/quantitative-metric evidence-type/test-results evidence-type/documentation evidence-type/visual-artifact expertise-needed/ml-engineering expertise-needed/domain-expertise expertise-needed/basic-technical"
  },
  {
    "slug": "jailbreak-resistance-testing",
    "name": "Jailbreak Resistance Testing",
    "description": "Jailbreak resistance testing evaluates LLM defences against techniques that bypass safety constraints. This involves testing role-playing attacks, hypothetical framing, encoded instructions, and multi-turn manipulation. Testing measures immediate resistance to jailbreaks and the system's ability to detect and block evolving attack patterns, producing reports on vulnerability severity and exploit paths.",
    "assurance_goals": [
      "Safety",
      "Security",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks/transformer/llm",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/safety",
      "assurance-goal-category/security",
      "assurance-goal-category/reliability",
      "data-type/text",
      "lifecycle-stage/model-evaluation",
      "lifecycle-stage/post-deployment",
      "technique-type/testing",
      "evidence-type/test-results",
      "evidence-type/quantitative-metric",
      "evidence-type/documentation",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise",
      "data-requirements/no-special-requirements"
    ],
    "searchText": "jailbreak resistance testing jailbreak resistance testing evaluates llm defences against techniques that bypass safety constraints. this involves testing role-playing attacks, hypothetical framing, encoded instructions, and multi-turn manipulation. testing measures immediate resistance to jailbreaks and the system's ability to detect and block evolving attack patterns, producing reports on vulnerability severity and exploit paths. safety security reliability applicable-models/architecture/neural-networks/transformer/llm applicable-models/requirements/black-box assurance-goal-category/safety assurance-goal-category/security assurance-goal-category/reliability data-type/text lifecycle-stage/model-evaluation lifecycle-stage/post-deployment technique-type/testing evidence-type/test-results evidence-type/quantitative-metric evidence-type/documentation expertise-needed/ml-engineering expertise-needed/domain-expertise data-requirements/no-special-requirements"
  },
  {
    "slug": "epistemic-uncertainty-quantification",
    "name": "Epistemic Uncertainty Quantification",
    "description": "Epistemic uncertainty quantification systematically measures model uncertainty about what it knows, partially knows, and doesn't know across different domains and topics. This technique uses probing datasets, confidence calibration analysis, and consistency testing to quantify epistemic uncertainty (uncertainty due to lack of knowledge) as distinct from aleatoric uncertainty (inherent data uncertainty). Uncertainty quantification enables appropriate deployment scoping, communicating model limitations, and identifying knowledge gaps requiring additional training or human oversight.",
    "assurance_goals": [
      "Transparency",
      "Reliability",
      "Safety"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/transparency",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "data-requirements/labeled-data-required",
      "data-type/any",
      "evidence-type/quantitative-metric",
      "evidence-type/documentation",
      "evidence-type/visual-artifact",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise",
      "expertise-needed/statistical-knowledge",
      "lifecycle-stage/model-evaluation",
      "lifecycle-stage/post-deployment",
      "technique-type/analytical",
      "technique-type/testing"
    ],
    "searchText": "epistemic uncertainty quantification epistemic uncertainty quantification systematically measures model uncertainty about what it knows, partially knows, and doesn't know across different domains and topics. this technique uses probing datasets, confidence calibration analysis, and consistency testing to quantify epistemic uncertainty (uncertainty due to lack of knowledge) as distinct from aleatoric uncertainty (inherent data uncertainty). uncertainty quantification enables appropriate deployment scoping, communicating model limitations, and identifying knowledge gaps requiring additional training or human oversight. transparency reliability safety applicable-models/architecture/model-agnostic applicable-models/requirements/black-box assurance-goal-category/transparency assurance-goal-category/reliability assurance-goal-category/safety data-requirements/labeled-data-required data-type/any evidence-type/quantitative-metric evidence-type/documentation evidence-type/visual-artifact expertise-needed/ml-engineering expertise-needed/domain-expertise expertise-needed/statistical-knowledge lifecycle-stage/model-evaluation lifecycle-stage/post-deployment technique-type/analytical technique-type/testing"
  },
  {
    "slug": "multi-agent-system-testing",
    "name": "Multi-Agent System Testing",
    "description": "Multi-agent system testing evaluates safety and reliability of systems where multiple AI agents interact, coordinate, or compete. This technique assesses emergent behaviours, communication protocols, conflict resolution, and whether agents maintain objectives appropriately. Testing produces reports on agent interaction patterns, coalition formation, failure cascade analysis, and safety property violations in multi-agent scenarios.",
    "assurance_goals": [
      "Safety",
      "Reliability",
      "Security"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/safety",
      "assurance-goal-category/reliability",
      "assurance-goal-category/security",
      "data-type/any",
      "lifecycle-stage/model-evaluation",
      "lifecycle-stage/post-deployment",
      "lifecycle-stage/system-deployment-and-use-monitoring",
      "technique-type/testing",
      "technique-type/analytical",
      "evidence-type/test-results",
      "evidence-type/documentation",
      "evidence-type/qualitative-assessment",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise",
      "expertise-needed/software-engineering",
      "data-requirements/no-special-requirements"
    ],
    "searchText": "multi-agent system testing multi-agent system testing evaluates safety and reliability of systems where multiple ai agents interact, coordinate, or compete. this technique assesses emergent behaviours, communication protocols, conflict resolution, and whether agents maintain objectives appropriately. testing produces reports on agent interaction patterns, coalition formation, failure cascade analysis, and safety property violations in multi-agent scenarios. safety reliability security applicable-models/architecture/model-agnostic applicable-models/requirements/black-box assurance-goal-category/safety assurance-goal-category/reliability assurance-goal-category/security data-type/any lifecycle-stage/model-evaluation lifecycle-stage/post-deployment lifecycle-stage/system-deployment-and-use-monitoring technique-type/testing technique-type/analytical evidence-type/test-results evidence-type/documentation evidence-type/qualitative-assessment expertise-needed/ml-engineering expertise-needed/domain-expertise expertise-needed/software-engineering data-requirements/no-special-requirements"
  },
  {
    "slug": "multimodal-alignment-evaluation",
    "name": "Multimodal Alignment Evaluation",
    "description": "Multimodal alignment evaluation assesses whether different modalities (vision, language, audio) are synchronised and consistent in multimodal AI systems. This technique tests whether descriptions match content, sounds associate with correct sources, and cross-modal reasoning maintains accuracy. It produces alignment scores, grounding quality metrics, and reports on modality-specific failure patterns.",
    "assurance_goals": [
      "Reliability",
      "Explainability",
      "Safety"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks",
      "applicable-models/architecture/neural-networks/transformer",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/reliability",
      "assurance-goal-category/explainability",
      "assurance-goal-category/explainability/explains/internal-mechanisms",
      "assurance-goal-category/safety",
      "data-type/image",
      "data-type/text",
      "data-requirements/labeled-data-required",
      "lifecycle-stage/model-evaluation",
      "lifecycle-stage/post-deployment",
      "technique-type/testing",
      "technique-type/analytical",
      "evidence-type/quantitative-metric",
      "evidence-type/qualitative-assessment",
      "evidence-type/test-results",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise"
    ],
    "searchText": "multimodal alignment evaluation multimodal alignment evaluation assesses whether different modalities (vision, language, audio) are synchronised and consistent in multimodal ai systems. this technique tests whether descriptions match content, sounds associate with correct sources, and cross-modal reasoning maintains accuracy. it produces alignment scores, grounding quality metrics, and reports on modality-specific failure patterns. reliability explainability safety applicable-models/architecture/neural-networks applicable-models/architecture/neural-networks/transformer applicable-models/requirements/black-box assurance-goal-category/reliability assurance-goal-category/explainability assurance-goal-category/explainability/explains/internal-mechanisms assurance-goal-category/safety data-type/image data-type/text data-requirements/labeled-data-required lifecycle-stage/model-evaluation lifecycle-stage/post-deployment technique-type/testing technique-type/analytical evidence-type/quantitative-metric evidence-type/qualitative-assessment evidence-type/test-results expertise-needed/ml-engineering expertise-needed/domain-expertise"
  },
  {
    "slug": "out-of-domain-detection",
    "name": "Out-of-Domain Detection",
    "description": "Out-of-domain (OOD) detection identifies user inputs that fall outside an AI system's intended domain or capabilities, enabling graceful handling rather than generating unreliable responses. This technique trains classifiers or uses embedding-based methods to recognise when queries require knowledge or capabilities the system doesn't possess. OOD detection enables systems to explicitly decline, redirect, or request clarification rather than hallucinating answers or applying inappropriate reasoning to unfamiliar domains.",
    "assurance_goals": [
      "Reliability",
      "Safety",
      "Transparency"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/architecture/neural-networks/transformer/llm",
      "applicable-models/paradigm/supervised",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "assurance-goal-category/transparency",
      "data-requirements/training-data-required",
      "data-requirements/labeled-data-required",
      "data-type/text",
      "evidence-type/quantitative-metric",
      "evidence-type/test-results",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise",
      "lifecycle-stage/model-development",
      "lifecycle-stage/deployment",
      "lifecycle-stage/post-deployment",
      "technique-type/algorithmic",
      "technique-type/procedural"
    ],
    "searchText": "out-of-domain detection out-of-domain (ood) detection identifies user inputs that fall outside an ai system's intended domain or capabilities, enabling graceful handling rather than generating unreliable responses. this technique trains classifiers or uses embedding-based methods to recognise when queries require knowledge or capabilities the system doesn't possess. ood detection enables systems to explicitly decline, redirect, or request clarification rather than hallucinating answers or applying inappropriate reasoning to unfamiliar domains. reliability safety transparency applicable-models/architecture/model-agnostic applicable-models/architecture/neural-networks/transformer/llm applicable-models/paradigm/supervised applicable-models/requirements/black-box assurance-goal-category/reliability assurance-goal-category/safety assurance-goal-category/transparency data-requirements/training-data-required data-requirements/labeled-data-required data-type/text evidence-type/quantitative-metric evidence-type/test-results expertise-needed/ml-engineering expertise-needed/domain-expertise lifecycle-stage/model-development lifecycle-stage/deployment lifecycle-stage/post-deployment technique-type/algorithmic technique-type/procedural"
  },
  {
    "slug": "prompt-injection-testing",
    "name": "Prompt Injection Testing",
    "description": "Prompt injection testing systematically evaluates LLMs and generative AI systems for vulnerabilities where malicious users can override system instructions through carefully crafted inputs. This technique encompasses both offensive security testing (proactively attempting injection attacks to identify vulnerabilities) and defensive detection mechanisms (monitoring for suspicious patterns in prompts and context). Testing involves evaluating various injection patterns including direct instruction manipulation, context confusion, role-playing exploits, delimiter attacks, and context window poisoning. Detection methods analyse context provenance, validate formatting consistency, and identify semantic anomalies that indicate manipulation attempts rather than legitimate user interaction.",
    "assurance_goals": [
      "Security",
      "Safety",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks/transformer/llm",
      "applicable-models/paradigm/generative",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/security",
      "assurance-goal-category/safety",
      "assurance-goal-category/reliability",
      "data-requirements/no-special-requirements",
      "data-type/text",
      "evidence-type/test-results",
      "evidence-type/qualitative-assessment",
      "evidence-type/quantitative-metric",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise",
      "lifecycle-stage/model-evaluation",
      "lifecycle-stage/deployment",
      "lifecycle-stage/post-deployment",
      "lifecycle-stage/system-deployment-and-use-monitoring",
      "technique-type/testing",
      "technique-type/analytical"
    ],
    "searchText": "prompt injection testing prompt injection testing systematically evaluates llms and generative ai systems for vulnerabilities where malicious users can override system instructions through carefully crafted inputs. this technique encompasses both offensive security testing (proactively attempting injection attacks to identify vulnerabilities) and defensive detection mechanisms (monitoring for suspicious patterns in prompts and context). testing involves evaluating various injection patterns including direct instruction manipulation, context confusion, role-playing exploits, delimiter attacks, and context window poisoning. detection methods analyse context provenance, validate formatting consistency, and identify semantic anomalies that indicate manipulation attempts rather than legitimate user interaction. security safety reliability applicable-models/architecture/neural-networks/transformer/llm applicable-models/paradigm/generative applicable-models/requirements/black-box assurance-goal-category/security assurance-goal-category/safety assurance-goal-category/reliability data-requirements/no-special-requirements data-type/text evidence-type/test-results evidence-type/qualitative-assessment evidence-type/quantitative-metric expertise-needed/ml-engineering expertise-needed/domain-expertise lifecycle-stage/model-evaluation lifecycle-stage/deployment lifecycle-stage/post-deployment lifecycle-stage/system-deployment-and-use-monitoring technique-type/testing technique-type/analytical"
  },
  {
    "slug": "reward-hacking-detection",
    "name": "Reward Hacking Detection",
    "description": "Reward hacking detection identifies when AI systems achieve stated objectives through unintended shortcuts or loopholes rather than genuine task mastery. This technique analyses whether systems exploit reward specification ambiguities, reward function bugs, or simulator artifacts to maximise rewards without actually solving the intended problem. Detection involves adversarial specification testing, human evaluation of solution quality, transfer testing to slightly modified tasks, and analysis of unexpected strategy patterns that suggest reward hacking rather than genuine learning.",
    "assurance_goals": [
      "Reliability",
      "Safety",
      "Transparency"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/paradigm/reinforcement",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/reliability",
      "assurance-goal-category/safety",
      "assurance-goal-category/transparency",
      "data-type/any",
      "data-requirements/no-special-requirements",
      "lifecycle-stage/model-evaluation",
      "lifecycle-stage/post-deployment",
      "technique-type/testing",
      "technique-type/analytical",
      "evidence-type/qualitative-assessment",
      "evidence-type/test-results",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise"
    ],
    "searchText": "reward hacking detection reward hacking detection identifies when ai systems achieve stated objectives through unintended shortcuts or loopholes rather than genuine task mastery. this technique analyses whether systems exploit reward specification ambiguities, reward function bugs, or simulator artifacts to maximise rewards without actually solving the intended problem. detection involves adversarial specification testing, human evaluation of solution quality, transfer testing to slightly modified tasks, and analysis of unexpected strategy patterns that suggest reward hacking rather than genuine learning. reliability safety transparency applicable-models/architecture/model-agnostic applicable-models/paradigm/reinforcement applicable-models/requirements/black-box assurance-goal-category/reliability assurance-goal-category/safety assurance-goal-category/transparency data-type/any data-requirements/no-special-requirements lifecycle-stage/model-evaluation lifecycle-stage/post-deployment technique-type/testing technique-type/analytical evidence-type/qualitative-assessment evidence-type/test-results expertise-needed/ml-engineering expertise-needed/domain-expertise"
  },
  {
    "slug": "ai-agent-safety-testing",
    "name": "AI Agent Safety Testing",
    "description": "AI agent safety testing evaluates autonomous AI agents that interact with external tools, APIs, and systems to ensure they operate safely and as intended. This technique assesses whether agents correctly understand tool specifications, respect usage constraints, handle errors gracefully, avoid unintended side effects, and make safe decisions during multi-step reasoning. Testing includes boundary condition validation (extreme parameters, edge cases), permission verification (ensuring agents don't exceed authorized actions), and cascade failure analysis (how tool errors propagate through agent workflows).",
    "assurance_goals": [
      "Safety",
      "Security",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/neural-networks/transformer/llm",
      "applicable-models/paradigm/generative",
      "applicable-models/requirements/black-box",
      "assurance-goal-category/safety",
      "assurance-goal-category/security",
      "assurance-goal-category/reliability",
      "data-type/any",
      "data-requirements/no-special-requirements",
      "lifecycle-stage/model-evaluation",
      "lifecycle-stage/post-deployment",
      "technique-type/testing",
      "evidence-type/test-results",
      "evidence-type/quantitative-metric",
      "evidence-type/documentation",
      "expertise-needed/ml-engineering",
      "expertise-needed/software-engineering",
      "expertise-needed/domain-expertise"
    ],
    "searchText": "ai agent safety testing ai agent safety testing evaluates autonomous ai agents that interact with external tools, apis, and systems to ensure they operate safely and as intended. this technique assesses whether agents correctly understand tool specifications, respect usage constraints, handle errors gracefully, avoid unintended side effects, and make safe decisions during multi-step reasoning. testing includes boundary condition validation (extreme parameters, edge cases), permission verification (ensuring agents don't exceed authorized actions), and cascade failure analysis (how tool errors propagate through agent workflows). safety security reliability applicable-models/architecture/neural-networks/transformer/llm applicable-models/paradigm/generative applicable-models/requirements/black-box assurance-goal-category/safety assurance-goal-category/security assurance-goal-category/reliability data-type/any data-requirements/no-special-requirements lifecycle-stage/model-evaluation lifecycle-stage/post-deployment technique-type/testing evidence-type/test-results evidence-type/quantitative-metric evidence-type/documentation expertise-needed/ml-engineering expertise-needed/software-engineering expertise-needed/domain-expertise"
  },
  {
    "slug": "toxicity-and-bias-detection",
    "name": "Toxicity and Bias Detection",
    "description": "Toxicity and bias detection uses automated classifiers and human review to identify harmful, offensive, or biased content in model outputs. This technique employs tools trained to detect toxic language, hate speech, stereotypes, and demographic biases through targeted testing with adversarial prompts. Detection covers explicit toxicity, implicit bias, and distributional unfairness across demographic groups.",
    "assurance_goals": [
      "Safety",
      "Fairness",
      "Reliability"
    ],
    "tags": [
      "applicable-models/architecture/model-agnostic",
      "applicable-models/architecture/neural-networks/transformer/llm",
      "applicable-models/requirements/black-box",
      "applicable-models/paradigm/generative",
      "assurance-goal-category/safety",
      "assurance-goal-category/fairness",
      "assurance-goal-category/fairness-metrics",
      "assurance-goal-category/reliability",
      "data-type/text",
      "data-requirements/labeled-data-required",
      "lifecycle-stage/model-evaluation",
      "lifecycle-stage/post-deployment",
      "lifecycle-stage/system-deployment-and-use-monitoring",
      "technique-type/testing",
      "technique-type/analytical",
      "evidence-type/quantitative-metric",
      "evidence-type/qualitative-assessment",
      "evidence-type/test-results",
      "expertise-needed/ml-engineering",
      "expertise-needed/domain-expertise"
    ],
    "searchText": "toxicity and bias detection toxicity and bias detection uses automated classifiers and human review to identify harmful, offensive, or biased content in model outputs. this technique employs tools trained to detect toxic language, hate speech, stereotypes, and demographic biases through targeted testing with adversarial prompts. detection covers explicit toxicity, implicit bias, and distributional unfairness across demographic groups. safety fairness reliability applicable-models/architecture/model-agnostic applicable-models/architecture/neural-networks/transformer/llm applicable-models/requirements/black-box applicable-models/paradigm/generative assurance-goal-category/safety assurance-goal-category/fairness assurance-goal-category/fairness-metrics assurance-goal-category/reliability data-type/text data-requirements/labeled-data-required lifecycle-stage/model-evaluation lifecycle-stage/post-deployment lifecycle-stage/system-deployment-and-use-monitoring technique-type/testing technique-type/analytical evidence-type/quantitative-metric evidence-type/qualitative-assessment evidence-type/test-results expertise-needed/ml-engineering expertise-needed/domain-expertise"
  }
]