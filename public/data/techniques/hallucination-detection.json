{
  "slug": "hallucination-detection",
  "name": "Hallucination Detection",
  "description": "Hallucination detection identifies when generative models produce factually incorrect, fabricated, or ungrounded outputs. This technique combines automated consistency checking, self-consistency methods, uncertainty quantification, and human evaluation. It produces detection scores distinguishing intrinsic hallucinations (contradicting sources) from extrinsic hallucinations (unverifiable claims), enabling filtering or user warnings.",
  "assurance_goals": [
    "Reliability",
    "Transparency",
    "Safety"
  ],
  "tags": [
    "applicable-models/architecture/neural-networks/transformer",
    "applicable-models/architecture/neural-networks/transformer/llm",
    "applicable-models/paradigm/generative",
    "applicable-models/requirements/black-box",
    "assurance-goal-category/reliability",
    "assurance-goal-category/transparency",
    "assurance-goal-category/safety",
    "data-type/text",
    "data-requirements/training-data-required",
    "evidence-type/quantitative-metric",
    "evidence-type/test-results",
    "evidence-type/qualitative-assessment",
    "expertise-needed/ml-engineering",
    "expertise-needed/domain-expertise",
    "lifecycle-stage/model-evaluation",
    "lifecycle-stage/post-deployment",
    "technique-type/testing",
    "technique-type/analytical"
  ],
  "example_use_cases": [
    {
      "description": "Monitoring a medical information system to detect when it generates unsupported clinical claims or fabricates research citations, preventing patients from receiving incorrect health advice.",
      "goal": "Safety"
    },
    {
      "description": "Evaluating an AI journalism assistant to ensure generated article drafts don't fabricate quotes, misattribute sources, or create false claims that could damage credibility and public trust.",
      "goal": "Reliability"
    },
    {
      "description": "Implementing confidence scoring and uncertainty indicators in AI assistants that transparently signal when responses may contain hallucinated information versus verified facts.",
      "goal": "Transparency"
    },
    {
      "description": "Validating an AI legal research tool used by public defenders to ensure generated case law summaries don't fabricate judicial opinions, misstate holdings, or invent precedents that could undermine legal arguments and defendants' rights.",
      "goal": "Reliability"
    },
    {
      "description": "Monitoring an AI-powered financial reporting assistant to detect when it generates unsubstantiated market analysis, fabricates company earnings data, or creates false attributions to analysts, protecting investors from misleading information.",
      "goal": "Safety"
    }
  ],
  "limitations": [
    {
      "description": "Automated detection methods may miss subtle hallucinations or flag correct but unusual information as potentially fabricated."
    },
    {
      "description": "Requires access to reliable ground truth knowledge sources for fact-checking, which may not exist for many domains or recent events."
    },
    {
      "description": "Self-consistency methods assume inconsistency indicates hallucination, but models can consistently hallucinate the same false information."
    },
    {
      "description": "Human evaluation is expensive and subjective, with annotators potentially disagreeing about what constitutes hallucination versus interpretation."
    },
    {
      "description": "Detection effectiveness degrades for rapidly evolving domains or emerging topics where ground truth knowledge bases may be outdated, incomplete, or unavailable, making it difficult to distinguish hallucinations from genuinely novel or recent information."
    },
    {
      "description": "Particularly challenging to apply in creative writing, storytelling, or subjective analysis contexts where the boundary between acceptable creative license and problematic hallucination is domain-dependent and context-specific."
    }
  ],
  "resources": [
    {
      "title": "vectara/hallucination-leaderboard",
      "url": "https://github.com/vectara/hallucination-leaderboard",
      "source_type": "software_package"
    },
    {
      "title": "How to Perform Hallucination Detection for LLMs | Towards Data ...",
      "url": "https://towardsdatascience.com/how-to-perform-hallucination-detection-for-llms-b8cb8b72e697/",
      "source_type": "tutorial"
    },
    {
      "title": "SelfCheckAgent: Zero-Resource Hallucination Detection in Generative Large Language Models",
      "url": "https://www.semanticscholar.org/paper/e7cd95ec57302fc5897bed07bee87a388747f750",
      "source_type": "technical_paper",
      "authors": [
        "Diyana Muhammed",
        "Gollam Rabby",
        "Soren Auer"
      ]
    },
    {
      "title": "SINdex: Semantic INconsistency Index for Hallucination Detection in LLMs",
      "url": "https://www.semanticscholar.org/paper/06ef8d4343f35184b4dec004365dfe1a562e08fc",
      "source_type": "technical_paper",
      "authors": [
        "Samir Abdaljalil",
        "H. Kurban",
        "Parichit Sharma",
        "E. Serpedin",
        "Rachad Atat"
      ]
    },
    {
      "title": "Uncertainty Quantification for Hallucination Detection in Large Language Models: Foundations, Methodology, and Future Directions",
      "url": "https://www.semanticscholar.org/paper/76912e6ea42bdebb2795708dac381a9b268b391c",
      "source_type": "review_paper",
      "authors": [
        "Sungmin Kang",
        "Y. Bakman",
        "D. Yaldiz",
        "Baturalp Buyukates",
        "S. Avestimehr"
      ]
    }
  ],
  "related_techniques": [
    "retrieval-augmented-generation-evaluation",
    "chain-of-thought-faithfulness-evaluation",
    "epistemic-uncertainty-quantification",
    "confidence-thresholding"
  ]
}