{
  "slug": "machine-unlearning",
  "name": "Machine Unlearning",
  "description": "Machine unlearning enables removal of specific training data's influence from trained models without complete retraining. This technique addresses privacy rights like GDPR's right to be forgotten by selectively erasing learned patterns associated with particular data points, individuals, or sensitive attributes. Methods include exact unlearning (provably equivalent to retraining without the data), approximate unlearning (efficient algorithms that closely approximate retraining), and certified unlearning (providing formal guarantees about information removal).",
  "assurance_goals": [
    "Privacy",
    "Fairness",
    "Transparency"
  ],
  "tags": [
    "applicable-models/architecture/model-agnostic",
    "applicable-models/architecture/neural-networks",
    "applicable-models/requirements/white-box",
    "applicable-models/requirements/model-internals",
    "assurance-goal-category/privacy",
    "assurance-goal-category/fairness",
    "assurance-goal-category/transparency",
    "data-requirements/training-data-required",
    "data-type/any",
    "evidence-type/quantitative-metric",
    "evidence-type/documentation",
    "evidence-type/test-results",
    "expertise-needed/ml-engineering",
    "expertise-needed/statistical-knowledge",
    "lifecycle-stage/model-development",
    "lifecycle-stage/post-deployment",
    "technique-type/algorithmic"
  ],
  "example_use_cases": [
    {
      "description": "Responding to user deletion requests in a social media recommendation system by removing all influence of that user's historical interactions, ensuring GDPR compliance and verifiable data removal.",
      "goal": "Privacy"
    },
    {
      "description": "Removing specific patient records from a hospital's diagnostic model after a patient withdraws consent, ensuring the model no longer reflects patterns from that individual's medical history while maintaining clinical accuracy for other patients.",
      "goal": "Fairness"
    },
    {
      "description": "Enabling a financial institution to demonstrate regulatory compliance by providing cryptographic proof that a former customer's transaction history has been completely removed from credit risk assessment models following account closure.",
      "goal": "Transparency"
    }
  ],
  "limitations": [
    {
      "description": "Exact unlearning for complex models like deep neural networks is computationally expensive, often nearly as costly as full retraining."
    },
    {
      "description": "Approximate unlearning methods may not provide strong guarantees that information has been fully removed, potentially leaving residual influence."
    },
    {
      "description": "Difficult to verify unlearning effectiveness, as adversaries might extract information about supposedly removed data through membership inference or other attacks."
    },
    {
      "description": "Repeated unlearning requests can degrade model performance significantly, especially if many data points are removed from the training distribution."
    },
    {
      "description": "Requires access to model architecture and training process details (white-box access), making it difficult to apply to third-party models or models where internal structure is proprietary."
    },
    {
      "description": "Particularly challenging for very large foundation models where even storing checkpoints for potential retraining is infeasible, limiting practical applicability to smaller, domain-specific models."
    }
  ],
  "resources": [
    {
      "title": "tamlhp/awesome-machine-unlearning",
      "url": "https://github.com/tamlhp/awesome-machine-unlearning",
      "source_type": "software_package"
    },
    {
      "title": "jjbrophy47/machine_unlearning",
      "url": "https://github.com/jjbrophy47/machine_unlearning",
      "source_type": "software_package"
    },
    {
      "title": "Right to be forgotten in the era of large language models",
      "url": "https://link.springer.com/article/10.1007/s43681-024-00573-9",
      "source_type": "technical_paper"
    },
    {
      "title": "Digital forgetting in large language models: A survey of unlearning methods",
      "url": "https://link.springer.com/article/10.1007/s10462-024-11078-6",
      "source_type": "technical_paper"
    },
    {
      "title": "Machine unlearning for traditional models and large language models",
      "url": "https://arxiv.org/abs/2404.01206",
      "source_type": "technical_paper"
    }
  ],
  "related_techniques": [
    "federated-learning",
    "homomorphic-encryption",
    "membership-inference-attack-testing",
    "influence-functions"
  ]
}