{
  "slug": "jailbreak-resistance-testing",
  "name": "Jailbreak Resistance Testing",
  "description": "Jailbreak resistance testing evaluates LLM defences against techniques that bypass safety constraints. This involves testing role-playing attacks, hypothetical framing, encoded instructions, and multi-turn manipulation. Testing measures immediate resistance to jailbreaks and the system's ability to detect and block evolving attack patterns, producing reports on vulnerability severity and exploit paths.",
  "assurance_goals": [
    "Safety",
    "Security",
    "Reliability"
  ],
  "tags": [
    "applicable-models/architecture/neural-networks/transformer/llm",
    "applicable-models/requirements/black-box",
    "assurance-goal-category/safety",
    "assurance-goal-category/security",
    "assurance-goal-category/reliability",
    "data-type/text",
    "lifecycle-stage/model-evaluation",
    "lifecycle-stage/post-deployment",
    "technique-type/testing",
    "evidence-type/test-results",
    "evidence-type/quantitative-metric",
    "evidence-type/documentation",
    "expertise-needed/ml-engineering",
    "expertise-needed/domain-expertise",
    "data-requirements/no-special-requirements"
  ],
  "example_use_cases": [
    {
      "description": "Testing a mental health support chatbot to ensure it cannot be jailbroken into providing medical advice that contradicts established clinical guidelines or suggesting harmful interventions, even when users employ emotional manipulation or role-playing scenarios.",
      "goal": "Safety"
    },
    {
      "description": "Validating that a financial advisory AI cannot be manipulated through multi-turn conversations into revealing proprietary trading algorithms, internal risk assessment models, or client portfolio information through social engineering techniques.",
      "goal": "Security"
    },
    {
      "description": "Ensuring an educational assessment AI maintains reliable grading standards and cannot be convinced to inflate scores, provide test answers, or bypass academic integrity checks through creative prompt engineering or hypothetical framing.",
      "goal": "Reliability"
    },
    {
      "description": "Testing a legal research AI assistant to verify it cannot be jailbroken into generating legally problematic content, revealing confidential case strategies, or providing advice that contradicts professional ethics rules through iterative prompt refinement.",
      "goal": "Security"
    }
  ],
  "limitations": [
    {
      "description": "New jailbreak techniques emerge constantly as adversaries discover creative attack vectors, requiring continuous testing and defense updates."
    },
    {
      "description": "Overly restrictive defenses can cause false positive rates of 5-15%, blocking legitimate queries about sensitive topics for educational or research purposes."
    },
    {
      "description": "Testing coverage is inherently limited by the creativity of testers, potentially missing novel jailbreak approaches that real users might discover."
    },
    {
      "description": "Some jailbreaks work through subtle multi-turn interactions that are difficult to anticipate and test systematically."
    },
    {
      "description": "Defence mechanisms such as output filtering, multi-stage validation, and adversarial prompt detection add 100-300ms latency per response, which may impact user experience in real-time applications."
    },
    {
      "description": "Defining clear boundaries for what constitutes unacceptable behaviour versus legitimate edge case queries is context-dependent and culturally variable, making universal jailbreak resistance metrics difficult to establish."
    }
  ],
  "resources": [
    {
      "title": "LLAMATOR-Core/llamator",
      "url": "https://github.com/LLAMATOR-Core/llamator",
      "source_type": "software_package"
    },
    {
      "title": "walledai/walledeval",
      "url": "https://github.com/walledai/walledeval",
      "source_type": "software_package"
    },
    {
      "title": "Jailbroken: How does llm safety training fail?",
      "url": "https://arxiv.org/abs/2307.02483",
      "source_type": "technical_paper"
    },
    {
      "title": "GPTFuzzer: Red teaming large language models with auto-generated jailbreak prompts",
      "url": "https://arxiv.org/abs/2309.10253",
      "source_type": "technical_paper"
    },
    {
      "title": "Operationalizing a threat model for red-teaming large language models",
      "url": "https://arxiv.org/abs/2407.14937",
      "source_type": "technical_paper"
    }
  ],
  "related_techniques": [
    "prompt-injection-testing",
    "ai-agent-safety-testing",
    "prompt-sensitivity-analysis",
    "adversarial-robustness-testing"
  ]
}