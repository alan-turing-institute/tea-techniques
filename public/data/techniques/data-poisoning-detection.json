{
  "slug": "data-poisoning-detection",
  "name": "Data Poisoning Detection",
  "description": "Data poisoning detection identifies malicious training data designed to compromise model behaviour. This technique detects various poisoning attacks including backdoor injection (triggers causing specific behaviours), availability attacks (degrading overall performance), and targeted attacks (causing errors on specific inputs). Detection methods include statistical outlier analysis, influence function analysis to identify high-impact training points, and validation-based approaches that test for suspicious model behaviours indicative of poisoning.",
  "assurance_goals": [
    "Security",
    "Reliability",
    "Safety"
  ],
  "tags": [
    "applicable-models/architecture/model-agnostic",
    "applicable-models/requirements/white-box",
    "applicable-models/requirements/gradient-access",
    "assurance-goal-category/security",
    "assurance-goal-category/reliability",
    "assurance-goal-category/safety",
    "data-requirements/training-data-required",
    "data-type/any",
    "lifecycle-stage/data-collection",
    "lifecycle-stage/model-development",
    "lifecycle-stage/model-evaluation",
    "technique-type/analytical",
    "technique-type/testing",
    "evidence-type/quantitative-metric",
    "evidence-type/test-results",
    "expertise-needed/ml-engineering",
    "expertise-needed/domain-expertise"
  ],
  "example_use_cases": [
    {
      "description": "Scanning training data for an autonomous driving system to detect images that might contain backdoor triggers designed to cause unsafe behavior when specific objects appear in the environment.",
      "goal": "Safety"
    },
    {
      "description": "Protecting a federated learning system for hospital patient diagnosis models from malicious participants who might inject poisoned medical records designed to create backdoors that misclassify specific patient profiles or degrade overall diagnostic reliability.",
      "goal": "Security"
    },
    {
      "description": "Ensuring a content moderation model hasn't been poisoned with data designed to make it fail on specific types of harmful content, maintaining reliable safety filtering.",
      "goal": "Reliability"
    },
    {
      "description": "Detecting poisoned training data in recidivism prediction models used for sentencing recommendations, where malicious actors might inject manipulated records to systematically bias predictions for specific demographic groups.",
      "goal": "Safety"
    },
    {
      "description": "Scanning training data for algorithmic trading models to identify poisoned market data designed to create exploitable patterns, ensuring reliable trading decisions and preventing market manipulation vulnerabilities.",
      "goal": "Reliability"
    }
  ],
  "limitations": [
    {
      "description": "Sophisticated poisoning attacks can be designed to evade statistical detection by mimicking benign data distributions closely."
    },
    {
      "description": "High false positive rates may lead to incorrectly filtering legitimate but unusual training examples, reducing training data quality and diversity."
    },
    {
      "description": "Computationally expensive to apply influence function analysis or deep inspection to very large training datasets, potentially requiring hours to days for thorough analysis of datasets with millions of samples, particularly when using gradient-based detection methods."
    },
    {
      "description": "Difficult to detect poisoning in scenarios where attackers have knowledge of detection methods and can adapt attacks accordingly."
    },
    {
      "description": "Establishing ground truth for what constitutes 'poisoned' versus legitimate but unusual data is challenging, particularly when dealing with naturally occurring outliers or edge cases in the data distribution."
    }
  ],
  "resources": [
    {
      "title": "art.defences.detector.poison — Adversarial Robustness Toolbox ...",
      "url": "https://adversarial-robustness-toolbox.readthedocs.io/en/latest/modules/defences/detector_poisoning.html",
      "source_type": "documentation"
    },
    {
      "title": "Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT Systems using Large Language Models",
      "url": "http://arxiv.org/pdf/2511.02894v1",
      "source_type": "technical_paper",
      "authors": [
        "W. K. M Mithsara",
        "Ning Yang",
        "Ahmed Imteaj",
        "Hussein Zangoti",
        "Abdur R. Shahid"
      ],
      "publication_date": "2025-11-04"
    },
    {
      "title": "SAFELOC: Overcoming Data Poisoning Attacks in Heterogeneous Federated Machine Learning for Indoor Localization",
      "url": "http://arxiv.org/pdf/2411.09055v1",
      "source_type": "technical_paper",
      "authors": [
        "Akhil Singampalli",
        "Danish Gufran",
        "Sudeep Pasricha"
      ],
      "publication_date": "2024-11-13"
    },
    {
      "title": "Introduction — trojai 0.2.22 documentation",
      "url": "https://trojai.readthedocs.io/en/latest/intro.html",
      "source_type": "documentation"
    },
    {
      "title": "Understanding Influence Functions and Datamodels via Harmonic Analysis",
      "url": "http://arxiv.org/pdf/2210.01072v1",
      "source_type": "technical_paper",
      "authors": [
        "Nikunj Saunshi",
        "Arushi Gupta",
        "Mark Braverman",
        "Sanjeev Arora"
      ],
      "publication_date": "2022-10-03"
    }
  ],
  "related_techniques": [
    "anomaly-detection",
    "adversarial-robustness-testing",
    "influence-functions",
    "cross-validation"
  ]
}