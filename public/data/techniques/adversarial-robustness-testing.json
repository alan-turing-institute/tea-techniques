{
  "slug": "adversarial-robustness-testing",
  "name": "Adversarial Robustness Testing",
  "description": "Adversarial robustness testing evaluates model resilience against intentionally crafted inputs designed to cause misclassification or malfunction. This technique generates adversarial examples through methods like FGSM, PGD, Carlini & Wagner (C&W) attacks, and AutoAttack to measure model vulnerability. Testing assesses both white-box scenarios (where attackers have full model access) and black-box scenarios (where attackers only observe inputs and outputs), measuring metrics like robust accuracy, attack success rates, and perturbation budgets required for successful attacks.",
  "assurance_goals": [
    "Security",
    "Reliability",
    "Safety"
  ],
  "tags": [
    "applicable-models/architecture/model-agnostic",
    "applicable-models/architecture/neural-networks",
    "applicable-models/requirements/black-box",
    "applicable-models/requirements/white-box",
    "assurance-goal-category/security",
    "assurance-goal-category/reliability",
    "assurance-goal-category/safety",
    "data-requirements/no-special-requirements",
    "data-type/image",
    "data-type/text",
    "data-type/tabular",
    "evidence-type/quantitative-metric",
    "evidence-type/test-results",
    "expertise-needed/ml-engineering",
    "expertise-needed/domain-expertise",
    "lifecycle-stage/model-evaluation",
    "lifecycle-stage/post-deployment",
    "technique-type/testing"
  ],
  "example_use_cases": [
    {
      "description": "Testing an autonomous vehicle's perception system against adversarial perturbations to road signs or traffic signals, ensuring the vehicle cannot be fooled by maliciously modified visual inputs.",
      "goal": "Safety"
    },
    {
      "description": "Evaluating a spam filter's robustness to adversarial text manipulations such as character substitutions, synonym replacements, and formatting tricks that attackers use to craft phishing emails that evade detection whilst remaining readable to humans.",
      "goal": "Security"
    },
    {
      "description": "Assessing whether a medical imaging classifier maintains reliable performance when images are slightly corrupted or manipulated, preventing misdiagnosis from low-quality or tampered inputs.",
      "goal": "Reliability"
    },
    {
      "description": "Testing a credit risk assessment model against adversarial perturbations in loan application data, ensuring attackers cannot manipulate minor input features like stated income or employment history to obtain fraudulent credit approvals.",
      "goal": "Security"
    }
  ],
  "limitations": [
    {
      "description": "Adversarial examples often transfer poorly between different models or architectures, making it difficult to comprehensively test against all possible attacks."
    },
    {
      "description": "Focus on worst-case perturbations may not reflect realistic threat models, as some adversarial examples require unrealistic levels of control over inputs."
    },
    {
      "description": "Computationally expensive to generate strong adversarial examples, often requiring 10-100x more computation than standard inference, especially for large models or high-dimensional inputs."
    },
    {
      "description": "Trade-off between robustness and accuracy means improving adversarial robustness often reduces performance on clean, unperturbed data."
    },
    {
      "description": "Requires representative test data covering expected deployment scenarios, as adversarial testing on narrow datasets may miss vulnerabilities that emerge in real-world conditions."
    },
    {
      "description": "Requires expertise in both attack methodology and domain knowledge to design meaningful adversarial perturbations that reflect genuine threats rather than artificial edge cases."
    }
  ],
  "resources": [
    {
      "title": "Welcome to the Adversarial Robustness Toolbox — Adversarial ...",
      "url": "https://adversarial-robustness-toolbox.readthedocs.io/",
      "source_type": "documentation"
    },
    {
      "title": "SecML-Torch: A Library for Robustness Evaluation of Deep ...",
      "url": "https://secml-torch.readthedocs.io/",
      "source_type": "software_package"
    },
    {
      "title": "A Survey of Neural Network Robustness Assessment in Image Recognition",
      "url": "https://www.semanticscholar.org/paper/4ff905df12fe2e5d3ef9801f22cdf301124cb0cb",
      "source_type": "review_paper",
      "authors": [
        "Jie Wang",
        "Jun Ai",
        "Minyan Lu",
        "Haoran Su",
        "Dan Yu",
        "Yutao Zhang",
        "Junda Zhu",
        "Jingyu Liu"
      ]
    },
    {
      "title": "Adversarial Training: A Survey",
      "url": "https://www.semanticscholar.org/paper/3181007ef16737020b51d5f77ec2855bf2b82b0e",
      "source_type": "review_paper",
      "authors": [
        "Mengnan Zhao",
        "Lihe Zhang",
        "Jingwen Ye",
        "Huchuan Lu",
        "Baocai Yin",
        "Xinchao Wang"
      ]
    },
    {
      "title": "What is an adversarial attack in NLP? — TextAttack 0.3.10 ...",
      "url": "https://textattack.readthedocs.io/en/latest/1start/what_is_an_adversarial_attack.html",
      "source_type": "tutorial"
    }
  ],
  "related_techniques": [
    "data-poisoning-detection",
    "model-watermarking-and-theft-detection",
    "multi-agent-system-testing",
    "prompt-injection-testing"
  ]
}