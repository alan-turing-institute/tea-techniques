{
  "slug": "constitutional-ai-evaluation",
  "name": "Constitutional AI Evaluation",
  "description": "Constitutional AI evaluation assesses models trained to follow explicit behavioural principles or 'constitutions' that specify desired values and constraints. This technique verifies that models consistently adhere to specified principles across diverse scenarios, measuring both principle compliance and handling of principle conflicts. Evaluation includes testing boundary cases where principles might conflict, measuring robustness to adversarial attempts to violate principles, and assessing whether models can explain their reasoning in terms of constitutional principles.",
  "assurance_goals": [
    "Safety",
    "Transparency",
    "Reliability"
  ],
  "tags": [
    "applicable-models/architecture/neural-networks/transformer/llm",
    "applicable-models/requirements/white-box",
    "applicable-models/paradigm/supervised",
    "assurance-goal-category/safety",
    "assurance-goal-category/transparency",
    "assurance-goal-category/reliability",
    "data-type/text",
    "data-requirements/labeled-data-required",
    "data-requirements/training-data-required",
    "lifecycle-stage/model-evaluation",
    "lifecycle-stage/post-deployment",
    "technique-type/testing",
    "technique-type/analytical",
    "evidence-type/quantitative-metric",
    "evidence-type/qualitative-assessment",
    "evidence-type/test-results",
    "expertise-needed/ml-engineering",
    "expertise-needed/domain-expertise"
  ],
  "example_use_cases": [
    {
      "description": "Evaluating a general-purpose AI assistant with the principle 'provide helpful information while refusing requests for illegal activities' to ensure it consistently distinguishes between legitimate chemistry education queries and attempts to obtain instructions for synthesising controlled substances or explosives.",
      "goal": "Safety"
    },
    {
      "description": "Testing whether an AI assistant can transparently explain its decisions by referencing the specific constitutional principles that guided its responses, enabling users to understand value-based reasoning.",
      "goal": "Transparency"
    },
    {
      "description": "Verifying that an AI news aggregator consistently applies stated principles about neutrality versus editorial perspective across diverse political topics and international news sources.",
      "goal": "Reliability"
    },
    {
      "description": "Evaluating a mental health support chatbot designed with principles like 'be empathetic and supportive while never providing medical diagnoses or replacing professional care' to verify it consistently maintains this boundary across varied emotional crises and user requests.",
      "goal": "Safety"
    },
    {
      "description": "Testing an AI homework assistant built on principles of 'guide learning without providing direct answers' to ensure it maintains this educational philosophy across different subjects, student ages, and question complexities without reverting to simply solving problems.",
      "goal": "Reliability"
    }
  ],
  "limitations": [
    {
      "description": "Constitutional principles may be vague or subject to interpretation, making it difficult to objectively measure compliance."
    },
    {
      "description": "Principles can conflict in complex scenarios, and evaluation must assess whether the model's priority ordering matches intended values."
    },
    {
      "description": "Models may learn to superficially cite principles in explanations without genuinely using them in decision-making processes."
    },
    {
      "description": "Creating comprehensive test sets covering all relevant principle applications and edge cases requires significant domain expertise and resources."
    },
    {
      "description": "Constitutional principles that seem clear in one cultural or linguistic context may be interpreted differently across diverse user populations, making it challenging to evaluate whether the model appropriately adapts its principle application or inappropriately varies its values."
    },
    {
      "description": "As organisational values evolve or principles are refined based on deployment experience, re-evaluation becomes necessary, but comparing results across principle versions is challenging without confounding changes in the model itself versus changes in evaluation criteria."
    }
  ],
  "resources": [
    {
      "title": "chrbradley/constitutional-reasoning-engine",
      "url": "https://github.com/chrbradley/constitutional-reasoning-engine",
      "source_type": "software_package"
    },
    {
      "title": "C3ai: Crafting and evaluating constitutions for constitutional ai",
      "url": "https://dl.acm.org/doi/abs/10.1145/3696410.3714705",
      "source_type": "technical_paper",
      "authors": [
        "Y Kyrychenko",
        "K Zhou",
        "E Bogucka"
      ]
    },
    {
      "title": "Towards Principled AI Alignment: An Evaluation and Augmentation of Inverse Constitutional AI",
      "url": "https://dash.harvard.edu/items/54a0845c-a3a3-4c73-9278-e4c4927c2e1a",
      "source_type": "technical_paper",
      "authors": [
        "E An"
      ]
    },
    {
      "title": "Constitution or Collapse? Exploring Constitutional AI with Llama 3-8B",
      "url": "https://arxiv.org/abs/2504.04918",
      "source_type": "technical_paper",
      "authors": [
        "X Zhang"
      ]
    }
  ],
  "related_techniques": [
    "ai-agent-safety-testing",
    "hallucination-detection",
    "jailbreak-resistance-testing",
    "multimodal-alignment-evaluation"
  ]
}