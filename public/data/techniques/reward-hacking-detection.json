{
  "slug": "reward-hacking-detection",
  "name": "Reward Hacking Detection",
  "description": "Reward hacking detection identifies when AI systems achieve stated objectives through unintended shortcuts or loopholes rather than genuine task mastery. This technique analyses whether systems exploit reward specification ambiguities, reward function bugs, or simulator artifacts to maximise rewards without actually solving the intended problem. Detection involves adversarial specification testing, human evaluation of solution quality, transfer testing to slightly modified tasks, and analysis of unexpected strategy patterns that suggest reward hacking rather than genuine learning.",
  "assurance_goals": [
    "Reliability",
    "Safety",
    "Transparency"
  ],
  "tags": [
    "applicable-models/architecture/model-agnostic",
    "applicable-models/paradigm/reinforcement",
    "applicable-models/requirements/black-box",
    "assurance-goal-category/reliability",
    "assurance-goal-category/safety",
    "assurance-goal-category/transparency",
    "data-type/any",
    "data-requirements/no-special-requirements",
    "lifecycle-stage/model-evaluation",
    "lifecycle-stage/post-deployment",
    "technique-type/testing",
    "technique-type/analytical",
    "evidence-type/qualitative-assessment",
    "evidence-type/test-results",
    "expertise-needed/ml-engineering",
    "expertise-needed/domain-expertise"
  ],
  "example_use_cases": [
    {
      "description": "Detecting when a reinforcement learning agent for robot manipulation learns to exploit physics simulator quirks rather than developing real-world-transferable manipulation skills, preventing failures in physical deployment.",
      "goal": "Reliability"
    },
    {
      "description": "Identifying when an AI-based healthcare scheduling system appears to optimize patient wait times by gaming appointment classifications or encouraging cancellations rather than genuinely improving clinic efficiency, preventing patient care degradation.",
      "goal": "Safety"
    },
    {
      "description": "Detecting when a loan approval system achieves target approval rates by exploiting specification loopholes in creditworthiness definitions rather than accurately assessing borrower risk, ensuring reliable lending decisions.",
      "goal": "Reliability"
    },
    {
      "description": "Verifying that an educational content recommendation system optimizes for genuine learning outcomes rather than gaming engagement metrics through strategies like repeatedly presenting easier content that inflates measured progress.",
      "goal": "Reliability"
    }
  ],
  "limitations": [
    {
      "description": "Difficult to distinguish between clever problem-solving and specification gaming without deep understanding of task intent."
    },
    {
      "description": "Novel gaming strategies may not be detected until systems are deployed in real environments where gaming becomes apparent."
    },
    {
      "description": "Closing detected loopholes may simply push systems to discover new gaming strategies rather than solving the fundamental specification challenge."
    },
    {
      "description": "Perfect specifications that eliminate all gaming opportunities may be impossible for complex, open-ended tasks."
    },
    {
      "description": "Requires domain expertise and clear articulation of task intent to distinguish between legitimate optimization strategies and gaming behaviors, which can be subjective in complex domains."
    },
    {
      "description": "Detection often requires access to detailed behavioral logs and environment state information that may not be available in black-box deployment scenarios."
    },
    {
      "description": "Establishing ground truth for 'correct' task completion without gaming requires independent verification methods that may be as resource-intensive as the original task."
    }
  ],
  "resources": [
    {
      "title": "Detecting and Mitigating Reward Hacking in Reinforcement Learning Systems: A Comprehensive Empirical Study",
      "url": "https://www.semanticscholar.org/paper/f71379b765f01c333ebaab5736dbf7f1005b19c9",
      "source_type": "technical_paper",
      "authors": [
        "Ibne Farabi Shihab",
        "Sanjeda Akter",
        "Anuj Sharma"
      ]
    },
    {
      "title": "Preventing Reward Hacking with Occupancy Measure Regularization",
      "url": "https://www.semanticscholar.org/paper/878e2c80a9247b6106b479bf8d74c02427947176",
      "source_type": "technical_paper",
      "authors": [
        "Cassidy Laidlaw",
        "Shivam Singhal",
        "A. Dragan"
      ]
    },
    {
      "title": "Fine-tuning & RL for LLMs: Intro to Post-training - DeepLearning.AI",
      "url": "https://learn.deeplearning.ai/courses/fine-tuning-and-reinforcement-learning-for-llms-intro-to-post-training/lesson/wr7ye4/evals-for-post-training:-test-sets-and-metrics",
      "source_type": "tutorial"
    },
    {
      "title": "How to Make a Reward Function in Reinforcement Learning ...",
      "url": "https://www.geeksforgeeks.org/machine-learning/how-to-make-a-reward-function-in-reinforcement-learning/",
      "source_type": "tutorial"
    }
  ],
  "related_techniques": [
    "hallucination-detection",
    "adversarial-training-evaluation",
    "constitutional-ai-evaluation",
    "epistemic-uncertainty-quantification"
  ]
}