{
  "slug": "model-watermarking-and-theft-detection",
  "name": "Model Watermarking and Theft Detection",
  "description": "Model watermarking and theft detection techniques protect AI systems from unauthorised replication by embedding detectable signatures in model outputs and identifying suspiciously similar prediction patterns. This includes watermarking schemes that survive knowledge distillation, fingerprinting methods that create unique statistical signatures, and detection methods that identify when a model has been stolen or replicated through model extraction, distillation, or imitation. These techniques enable model owners to prove intellectual property theft and protect proprietary AI systems.",
  "assurance_goals": [
    "Security",
    "Transparency",
    "Fairness"
  ],
  "tags": [
    "applicable-models/architecture/model-agnostic",
    "applicable-models/requirements/black-box",
    "assurance-goal-category/security",
    "assurance-goal-category/transparency",
    "assurance-goal-category/fairness",
    "data-type/any",
    "data-requirements/no-special-requirements",
    "lifecycle-stage/model-development",
    "lifecycle-stage/deployment",
    "lifecycle-stage/post-deployment",
    "lifecycle-stage/system-deployment-and-use-monitoring",
    "technique-type/algorithmic",
    "technique-type/testing",
    "evidence-type/quantitative-metric",
    "evidence-type/test-results",
    "evidence-type/documentation",
    "expertise-needed/ml-engineering",
    "expertise-needed/domain-expertise"
  ],
  "example_use_cases": [
    {
      "description": "Protecting a proprietary medical imaging diagnostic model from theft by embedding watermarks that survive if competitors attempt to distill or extract the model, enabling hospitals to verify they're using legitimate licensed versions.",
      "goal": "Security"
    },
    {
      "description": "Providing forensic evidence in intellectual property litigation by demonstrating through watermark extraction and statistical fingerprinting that a competitor's fraud detection system was derived from a bank's proprietary model.",
      "goal": "Transparency"
    },
    {
      "description": "Protecting an autonomous vehicle perception model from unauthorised replication, ensuring that safety-critical models undergo proper validation rather than being deployed through model theft, maintaining fair safety standards across the industry.",
      "goal": "Fairness"
    }
  ],
  "limitations": [
    {
      "description": "Watermarks may be removed or degraded through post-processing, fine-tuning, or adversarial training by sophisticated attackers."
    },
    {
      "description": "Difficult to distinguish between independent development of similar capabilities and actual behavioral cloning, especially for simple tasks."
    },
    {
      "description": "Detection methods may produce false positives when models trained on similar data naturally develop comparable behaviors."
    },
    {
      "description": "Watermarking can slightly degrade model performance or be detectable by attackers, creating trade-offs between protection strength and model quality."
    },
    {
      "description": "Effectiveness varies significantly by model type and task, with some architectures (like transformers) and domains (like natural language) being more amenable to watermarking than others (like small computer vision models)."
    },
    {
      "description": "Legal frameworks for using watermarking evidence in intellectual property cases are still evolving, and successful theft claims may require complementary evidence beyond watermark detection alone."
    }
  ],
  "resources": [
    {
      "title": "A systematic review on model watermarking for neural networks",
      "url": "https://www.frontiersin.org/articles/10.3389/fdata.2021.729663/full",
      "source_type": "technical_paper",
      "authors": [
        "F. Boenisch"
      ]
    },
    {
      "title": "Watermark-Robustness-Toolbox",
      "url": "https://github.com/dnn-security/Watermark-Robustness-Toolbox",
      "source_type": "software_package"
    },
    {
      "title": "dnn-watermark: Embedding Watermarks into Deep Neural Networks",
      "url": "https://github.com/yu4u/dnn-watermark",
      "source_type": "software_package"
    },
    {
      "title": "Watermark and protect your Deep Neural Networks!",
      "url": "https://medium.com/@PrincyJ/watermark-and-protect-your-deep-neural-networks-c5d8e8824029",
      "source_type": "tutorial"
    },
    {
      "title": "Protecting intellectual property of deep neural networks with watermarking",
      "url": "https://dl.acm.org/doi/abs/10.1145/3196494.3196550",
      "source_type": "technical_paper",
      "authors": [
        "J. Zhang",
        "Z. Gu",
        "J. Jang",
        "H. Wu",
        "M.P. Stoecklin"
      ]
    }
  ],
  "related_techniques": [
    "adversarial-robustness-testing",
    "adversarial-training-evaluation",
    "data-poisoning-detection",
    "model-extraction-defence-testing"
  ]
}