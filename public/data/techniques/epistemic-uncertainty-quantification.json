{
  "slug": "epistemic-uncertainty-quantification",
  "name": "Epistemic Uncertainty Quantification",
  "description": "Epistemic uncertainty quantification systematically measures model uncertainty about what it knows, partially knows, and doesn't know across different domains and topics. This technique uses probing datasets, confidence calibration analysis, and consistency testing to quantify epistemic uncertainty (uncertainty due to lack of knowledge) as distinct from aleatoric uncertainty (inherent data uncertainty). Uncertainty quantification enables appropriate deployment scoping, communicating model limitations, and identifying knowledge gaps requiring additional training or human oversight.",
  "assurance_goals": [
    "Transparency",
    "Reliability",
    "Safety"
  ],
  "tags": [
    "applicable-models/architecture/model-agnostic",
    "applicable-models/requirements/black-box",
    "assurance-goal-category/transparency",
    "assurance-goal-category/reliability",
    "assurance-goal-category/safety",
    "data-requirements/labeled-data-required",
    "data-type/any",
    "evidence-type/quantitative-metric",
    "evidence-type/documentation",
    "evidence-type/visual-artifact",
    "expertise-needed/ml-engineering",
    "expertise-needed/domain-expertise",
    "expertise-needed/statistical-knowledge",
    "lifecycle-stage/model-evaluation",
    "lifecycle-stage/post-deployment",
    "technique-type/analytical",
    "technique-type/testing"
  ],
  "example_use_cases": [
    {
      "description": "Mapping a medical AI's knowledge boundaries to identify conditions it diagnoses reliably versus conditions requiring specialist referral, enabling safe deployment with appropriate scope limitations.",
      "goal": "Safety"
    },
    {
      "description": "Identifying knowledge gaps in a public policy analysis AI to ensure it provides reliable information about well-researched policy areas while disclaiming uncertainty about emerging policy domains or local contexts.",
      "goal": "Reliability"
    },
    {
      "description": "Transparently documenting model knowledge boundaries in user-facing applications, helping users understand when to trust AI outputs versus seek additional verification.",
      "goal": "Transparency"
    },
    {
      "description": "Quantifying uncertainty in an automated loan approval system to identify applications where the model's knowledge boundaries are exceeded (e.g., novel business types or unusual financial situations), triggering human expert review rather than automated rejection or approval.",
      "goal": "Reliability"
    },
    {
      "description": "Mapping knowledge boundaries in an educational AI tutor to distinguish between well-covered curriculum topics and emerging areas where the model lacks sufficient training data, ensuring students receive reliable guidance and appropriate referrals to human instructors.",
      "goal": "Safety"
    }
  ],
  "limitations": [
    {
      "description": "Comprehensive knowledge mapping across all possible domains and topics is infeasible, requiring prioritization of important knowledge areas."
    },
    {
      "description": "Knowledge boundaries may be fuzzy rather than discrete, making it difficult to establish clear cutoffs between known and unknown."
    },
    {
      "description": "Models may be confidently wrong in some areas, making calibration and confidence signals unreliable indicators of actual knowledge."
    },
    {
      "description": "Knowledge boundaries shift as models are updated or fine-tuned, requiring continuous remapping to maintain accuracy."
    },
    {
      "description": "Uncertainty quantification methods can add significant computational overhead (10-100x inference time for ensemble-based approaches), making real-time deployment challenging for latency-sensitive applications."
    },
    {
      "description": "Interpreting uncertainty estimates requires statistical expertise and domain knowledge to set appropriate thresholds for triggering human review or system warnings."
    }
  ],
  "resources": [
    {
      "title": "ZBox1005/CoT-UQ",
      "url": "https://github.com/ZBox1005/CoT-UQ",
      "source_type": "software_package"
    },
    {
      "title": "Knowledge boundary of large language models: A survey",
      "url": "https://aclanthology.org/2025.acl-long.256/",
      "source_type": "technical_paper"
    },
    {
      "title": "Teaching large language models to express knowledge boundary from their own signals",
      "url": "https://aclanthology.org/2025.knowllm-1.3/",
      "source_type": "technical_paper"
    },
    {
      "title": "Benchmarking knowledge boundary for large language models",
      "url": "https://arxiv.org/abs/2402.11493",
      "source_type": "technical_paper"
    },
    {
      "title": "Investigating the factual knowledge boundary of large language models with retrieval augmentation",
      "url": "https://aclanthology.org/2025.coling-main.250/",
      "source_type": "technical_paper"
    }
  ],
  "related_techniques": [
    "prediction-intervals",
    "jackknife-resampling",
    "hallucination-detection",
    "bootstrapping"
  ]
}