{
  "slug": "safety-guardrails",
  "name": "Safety Guardrails",
  "description": "Safety guardrails apply real-time content moderation and safety constraints during deployment, generating evidence of safety violations and protection effectiveness. This technique produces detailed logs of blocked content, safety incident reports, violation pattern analysis, and guardrail performance metrics. Evidence includes categorised threat taxonomies, false positive/negative rates, and temporal trends in attack patterns, supporting assurance claims about deployed system safety.",
  "assurance_goals": [
    "Safety",
    "Security",
    "Reliability"
  ],
  "tags": [
    "applicable-models/architecture/model-agnostic",
    "applicable-models/requirements/black-box",
    "assurance-goal-category/safety",
    "assurance-goal-category/security",
    "assurance-goal-category/reliability",
    "data-type/text",
    "data-requirements/no-special-requirements",
    "lifecycle-stage/deployment",
    "lifecycle-stage/post-deployment",
    "lifecycle-stage/system-deployment-and-use-monitoring",
    "technique-type/algorithmic",
    "technique-type/procedural",
    "evidence-type/quantitative-metric",
    "evidence-type/test-results",
    "evidence-type/documentation",
    "evidence-type/visual-artifact",
    "expertise-needed/ml-engineering",
    "expertise-needed/domain-expertise",
    "expertise-needed/basic-technical"
  ],
  "example_use_cases": [
    {
      "description": "Implementing real-time filtering on a chatbot to catch any harmful outputs that slip through training-time safety alignment, blocking toxic content before it reaches users.",
      "goal": "Safety"
    },
    {
      "description": "Protecting a code generation model from responding to prompts requesting malicious code by filtering both inputs and outputs for security vulnerabilities and attack patterns.",
      "goal": "Security"
    },
    {
      "description": "Ensuring reliable content safety in production by adding a filtering layer that catches edge cases missed during testing and provides immediate protection against newly discovered attack patterns.",
      "goal": "Reliability"
    },
    {
      "description": "Implementing guardrails on a medical information chatbot to filter queries requesting diagnosis or treatment recommendations that should only come from licensed professionals, and to block outputs containing specific dosage information without proper context and disclaimers.",
      "goal": "Safety"
    },
    {
      "description": "Protecting a financial advisory AI from generating outputs that could constitute unauthorised securities advice or recommendations violating regulatory requirements, filtering both prompts and responses for compliance violations.",
      "goal": "Security"
    },
    {
      "description": "Ensuring an educational AI tutor blocks inappropriate content and maintains age-appropriate interactions by filtering both student inputs (detecting potential self-harm signals) and system outputs (preventing exposure to mature content).",
      "goal": "Safety"
    }
  ],
  "limitations": [
    {
      "description": "Adds 50-200ms latency to inference depending on filter complexity, which may be unacceptable for real-time applications requiring sub-100ms response times."
    },
    {
      "description": "Safety classifiers may produce false positives that block legitimate content, degrading user experience and system utility."
    },
    {
      "description": "Sophisticated adversaries may craft outputs that evade filtering through obfuscation, encoding, or subtle harmful content."
    },
    {
      "description": "Requires continuous updates to filtering rules and classifiers as new attack patterns and harmful content types emerge."
    },
    {
      "description": "Running guardrail models alongside primary models increases infrastructure costs by 20-50% depending on guardrail complexity, which may be prohibitive for resource-constrained deployments."
    }
  ],
  "resources": [
    {
      "title": "Real-time Serving â€” Databricks SDK for Python beta documentation",
      "url": "https://databricks-sdk-py.readthedocs.io/en/latest/dbdataclasses/serving.html",
      "source_type": "documentation"
    },
    {
      "title": "NVIDIA-NeMo/Guardrails",
      "url": "https://github.com/NVIDIA-NeMo/Guardrails",
      "source_type": "software_package"
    },
    {
      "title": "Legilimens: Practical and Unified Content Moderation for Large Language Model Services",
      "url": "https://arxiv.org/abs/2408.15488",
      "source_type": "technical_paper"
    },
    {
      "title": "Aegis: Online adaptive ai content safety moderation with ensemble of llm experts",
      "url": "https://arxiv.org/abs/2404.05993",
      "source_type": "technical_paper"
    },
    {
      "title": "From Judgment to Interference: Early Stopping LLM Harmful Outputs via Streaming Content Monitoring",
      "url": "https://arxiv.org/abs/2506.09996",
      "source_type": "technical_paper"
    }
  ],
  "related_techniques": [
    "out-of-domain-detection",
    "hallucination-detection",
    "toxicity-and-bias-detection",
    "reward-hacking-detection"
  ]
}