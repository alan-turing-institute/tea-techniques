{
  "slug": "model-extraction-defence-testing",
  "name": "Model Extraction Defence Testing",
  "description": "Model extraction defence testing evaluates protections against attackers who attempt to steal model functionality by querying it and training surrogate models. This technique assesses defences like query limiting, output perturbation, watermarking, and fingerprinting by simulating extraction attacks and measuring how much model functionality can be replicated. Testing evaluates both the effectiveness of defences in preventing extraction and their impact on legitimate use cases, ensuring security measures don't excessively degrade user experience.",
  "assurance_goals": [
    "Security",
    "Privacy",
    "Transparency"
  ],
  "tags": [
    "applicable-models/architecture/model-agnostic",
    "applicable-models/requirements/black-box",
    "assurance-goal-category/security",
    "assurance-goal-category/privacy",
    "assurance-goal-category/transparency",
    "data-type/any",
    "data-requirements/training-data-required",
    "lifecycle-stage/model-evaluation",
    "lifecycle-stage/post-deployment",
    "technique-type/testing",
    "evidence-type/quantitative-metric",
    "evidence-type/test-results",
    "expertise-needed/ml-engineering",
    "expertise-needed/software-engineering"
  ],
  "example_use_cases": [
    {
      "description": "Testing protections for a proprietary fraud detection API to ensure competitors cannot recreate the model's decision boundaries through systematic querying, by simulating extraction attacks using query budgets, active learning strategies, and substitute model training.",
      "goal": "Security"
    },
    {
      "description": "Evaluating whether a medical diagnosis model's query limits and output perturbations prevent extraction while protecting patient privacy embedded in the model's learned patterns.",
      "goal": "Privacy"
    },
    {
      "description": "Assessing watermarking techniques that enable model owners to prove when competitors have extracted their model, providing transparent evidence for intellectual property claims.",
      "goal": "Transparency"
    },
    {
      "description": "Evaluating whether rate limiting and output obfuscation for an automated essay grading API prevent competitors from extracting the scoring model through systematic submission of probe essays designed to reverse-engineer grading criteria.",
      "goal": "Security"
    },
    {
      "description": "Testing whether a traffic prediction API's defensive perturbations prevent extraction of the underlying routing optimization model whilst maintaining sufficient accuracy for legitimate urban planning applications.",
      "goal": "Reliability"
    }
  ],
  "limitations": [
    {
      "description": "Sophisticated attackers may use transfer learning, active learning, or knowledge distillation to extract models with 10-50x fewer queries than static defences anticipate, and can adapt their strategies as they probe defences, requiring dynamic rather than static protection mechanisms."
    },
    {
      "description": "Defensive measures like output perturbation can degrade model utility for legitimate users, creating tension between security and usability."
    },
    {
      "description": "Difficult to distinguish between legitimate high-volume use and malicious extraction attempts, potentially blocking valid users."
    },
    {
      "description": "Watermarking and fingerprinting techniques may be removed or obscured by attackers who post-process extracted models."
    },
    {
      "description": "Difficult to validate defence effectiveness without exposing the model to actual extraction attempts, and limited public benchmarks make it challenging to compare defence strategies objectively across different model types and threat scenarios."
    },
    {
      "description": "Requires specialised expertise in adversarial machine learning and attack simulation to design realistic extraction scenarios, making it challenging for organisations without dedicated security teams to implement comprehensive testing."
    }
  ],
  "resources": [
    {
      "title": "Welcome to the Adversarial Robustness Toolbox â€” Adversarial ...",
      "url": "https://adversarial-robustness-toolbox.readthedocs.io/",
      "source_type": "software_package"
    },
    {
      "title": "Adversarial Machine Learning: Defense Strategies",
      "url": "https://neptune.ai/blog/adversarial-machine-learning-defense-strategies",
      "source_type": "tutorial"
    },
    {
      "title": "Hypothesis Testing and Beyond: a Mini Survey on Membership Inference Attacks",
      "url": "https://www.semanticscholar.org/paper/ac4dcda6b7490d525c24d27100b7f9428ee01265",
      "source_type": "review_paper",
      "authors": [
        "Jiajie Liu",
        "Zixuan Zhang",
        "Haonan Li",
        "Weijian Song",
        "Yiyang Lin",
        "Jinxin Zuo"
      ]
    }
  ],
  "related_techniques": [
    "adversarial-robustness-testing",
    "membership-inference-attack-testing",
    "model-watermarking-and-theft-detection",
    "adversarial-training-evaluation"
  ]
}