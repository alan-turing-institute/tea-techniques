{
  "slug": "membership-inference-attack-testing",
  "name": "Membership Inference Attack Testing",
  "description": "Membership inference attack testing evaluates whether adversaries can determine if specific data points were included in a model's training set. This technique simulates attacks where adversaries use model confidence scores, prediction patterns, or loss values to distinguish training data from non-training data. Testing measures privacy leakage by calculating attack success rates, precision-recall trade-offs, and advantage over random guessing. Results inform decisions about privacy-enhancing techniques like differential privacy or regularisation.",
  "assurance_goals": [
    "Privacy",
    "Security",
    "Transparency"
  ],
  "tags": [
    "applicable-models/architecture/model-agnostic",
    "applicable-models/requirements/black-box",
    "assurance-goal-category/privacy",
    "assurance-goal-category/security",
    "assurance-goal-category/transparency",
    "data-requirements/training-data-required",
    "data-type/any",
    "evidence-type/quantitative-metric",
    "evidence-type/test-results",
    "expertise-needed/ml-engineering",
    "expertise-needed/security-expertise",
    "lifecycle-stage/model-evaluation",
    "lifecycle-stage/post-deployment",
    "technique-type/testing",
    "technique-type/analytical"
  ],
  "example_use_cases": [
    {
      "description": "Testing a genomics research model to ensure attackers cannot determine which individuals' genetic data were used in training, protecting highly sensitive hereditary and health information from privacy breaches.",
      "goal": "Privacy"
    },
    {
      "description": "Evaluating whether a facial recognition system leaks information about whose faces were in the training set, preventing unauthorized identification of individuals in training data.",
      "goal": "Security"
    },
    {
      "description": "Auditing a credit scoring model used by multiple lenders to verify and transparently document that the model doesn't leak information about which specific customers' financial histories were used in training, supporting fair lending compliance reporting.",
      "goal": "Transparency"
    }
  ],
  "limitations": [
    {
      "description": "Attack success rates vary significantly depending on model architecture, training procedures, and data characteristics, making it difficult to establish universal thresholds for acceptable privacy."
    },
    {
      "description": "Sophisticated attackers with shadow models or auxiliary data may achieve attack success rates 2-3x higher than standard evaluation scenarios test."
    },
    {
      "description": "Trade-off between model utility and privacy protection means defending against membership inference often reduces model accuracy."
    },
    {
      "description": "Testing requires access to both training and non-training data from the same distribution, which may not always be available for realistic evaluation."
    }
  ],
  "resources": [
    {
      "title": "SoK: Comparing Different Membership Inference Attacks with a Comprehensive Benchmark",
      "url": "https://www.semanticscholar.org/paper/e5604a82a8a1deb71c46bc8fa4a54e742dc98305",
      "source_type": "review_paper",
      "authors": [
        "Jun Niu",
        "Xiaoyan Zhu",
        "Moxuan Zeng",
        "Ge-ming Zhang",
        "Qingyang Zhao",
        "Chu-Chun Huang",
        "Yang Zhang",
        "Suyu An",
        "Yangzhong Wang",
        "Xinghui Yue",
        "Zhipeng He",
        "Weihao Guo",
        "Kuo Shen",
        "Peng Liu",
        "Yulong Shen",
        "Xiaohong Jiang",
        "Jianfeng Ma",
        "Yuqing Zhang"
      ]
    },
    {
      "title": "SoK: Membership Inference Attacks on LLMs are Rushing Nowhere (and How to Fix It)",
      "url": "https://www.semanticscholar.org/paper/c438dcb2b3d7e1a9cee8d9b1035b96c39b53941c",
      "source_type": "review_paper",
      "authors": [
        "Matthieu Meeus",
        "Igor Shilov",
        "Shubham Jain",
        "Manuel Faysse",
        "Marek Rei",
        "Y. Montjoye"
      ]
    },
    {
      "title": "art.attacks.inference.membership_inference — Adversarial ...",
      "url": "https://adversarial-robustness-toolbox.readthedocs.io/en/latest/modules/attacks/inference/membership_inference.html",
      "source_type": "documentation"
    },
    {
      "title": "Library of Attacks — tapas 0.1 documentation",
      "url": "https://tapas-privacy.readthedocs.io/en/latest/library-of-attacks.html",
      "source_type": "documentation"
    },
    {
      "title": "Intro to Federated Learning - DeepLearning.AI",
      "url": "https://learn.deeplearning.ai/courses/intro-to-federated-learning/lesson/go7bi/data-privacy",
      "source_type": "tutorial"
    }
  ],
  "related_techniques": [
    "model-extraction-defence-testing",
    "adversarial-training-evaluation",
    "synthetic-data-evaluation",
    "adversarial-robustness-testing"
  ]
}